discussion and future work authors topics and skillthe amount of training text for each author is not exceptionally large but the totalnumber of authors in the study is significantly larger than most studies of stylometrymethods and on par with new methods such as writeprints having such a largenumber of authors to work with allowed us to do more extensive testing by choosingrandom groups of authors and averaging the accuracies across them this also allowedus to see interesting patterns in the study such as which authors did a better job thanothers in creating successful adversaries passages through our anecdote observationsit was clear that certain authors did a poor job in writing their adversaries passages additional certain authors also had a style that seemed to be particularly suscepti be to obfuscation attempts authorship of obfuscation passages were often attributed these authors when they were a member of a test set an interesting avenue ofresearch would be to determine if it is possible to create a generic writing style byautomated means the domain of possible content in this study was fairly open participants wereallowed to present samples from a variety of subjects so long as they were scholarlyin nature it could be beneficial to study the effects of adversaries attacks in stricterdomains where there is less room for maneuvering and thus less options for how anauthor could hide his or her identity stylometry used in restricted domains may proveless susceptible to our attacks in general the participants in this study were instilled in the field of stylometryand were not professional writers despite this lack of expertise they were able consistently circumvent the authorship attribution methods that were put in place this strengthens our findings as it would be reasonable to expect authors with expertisein such areas could do a better job at attacking the system open problems in adversaries stylometrygiven the evidence in this research that hiding one s writing style is an effective meansto circumventing authorship recognition one of the next logical steps is to develop end user software that can assist users in modifying their writing this is being addressedwith the release of anonymouth anonymouth augments the writing style modifica tion process with intelligent suggestions driven by implementations and analysis ofstylometry techniques outlined in this research and elsewhere open research avenuesinclude identifying the most effective structure approach for writing style modifica tion resolving the trade off between comprehensive modification and overfitting thechanges for specific recognition methods and identifying which features may be heavilyautomated and which must rely greatly on manual input another important part of continued research in this area is larger and more definedcorpora in different languages our general corpus satisfies a number of reasonabledemands for consistency length and focus but there are many other more specificdomains that could produce different results when examining adversaries passages for example will writing style modifications be more or less effective in a highlyrestricted domain such as complex scientific research papers or in a very broad domainsuch as fiction short stories are the most sapient features for identification in otherlanguages similar to those in english . discussion and future work authors topics and skillthe amount of training text for each author is not exceptionally large but the totalnumber of authors in the study is significantly larger than most studies of stylometrymethods and on par with new methods such as writeprints having such a largenumber of authors to work with allowed us to do more extensive testing by choosingrandom groups of authors and averaging the accuracies across them this also allowedus to see interesting patterns in the study such as which authors did a better job thanothers in creating successful adversaries passages through our anecdote observationsit was clear that certain authors did a poor job in writing their adversaries passages additional certain authors also had a style that seemed to be particularly suscepti be to obfuscation attempts authorship of obfuscation passages were often attributed these authors when they were a member of a test set an interesting avenue ofresearch would be to determine if it is possible to create a generic writing style byautomated means the domain of possible content in this study was fairly open participants wereallowed to present samples from a variety of subjects so long as they were scholarlyin nature it could be beneficial to study the effects of adversaries attacks in stricterdomains where there is less room for maneuvering and thus less options for how anauthor could hide his or her identity stylometry used in restricted domains may proveless susceptible to our attacks in general the participants in this study were instilled in the field of stylometryand were not professional writers despite this lack of expertise they were able consistently circumvent the authorship attribution methods that were put in place this strengthens our findings as it would be reasonable to expect authors with expertisein such areas could do a better job at attacking the system open problems in adversaries stylometrygiven the evidence in this research that hiding one s writing style is an effective meansto circumventing authorship recognition one of the next logical steps is to develop end user software that can assist users in modifying their writing this is being addressedwith the release of anonymouth anonymouth augments the writing style modifica tion process with intelligent suggestions driven by implementations and analysis ofstylometry techniques outlined in this research and elsewhere open research avenuesinclude identifying the most effective structure approach for writing style modifica tion resolving the trade off between comprehensive modification and overfitting thechanges for specific recognition methods and identifying which features may be heavilyautomated and which must rely greatly on manual input another important part of continued research in this area is larger and more definedcorpora in different languages our general corpus satisfies a number of reasonabledemands for consistency length and focus but there are many other more specificdomains that could produce different results when examining adversaries passages for example will writing style modifications be more or less effective in a highlyrestricted domain such as complex scientific research papers or in a very broad domainsuch as fiction short stories are the most sapient features for identification in otherlanguages similar to those in english . tldrchange it up a bit . tldrchange it up a bit . circumvention by machine translationsince publication of our initials research on this topic circumvention via machinetranslation has been consistently questioned as a possible approach to maintaininganonymity our evaluation suggests that machine translation in its current form doesnot provide a reliable means of circumventing stylometry the translation evaluation was performed using passages from the brennan greenstadt corpus the single step translations provided a drop in precision of any where from to the two step translation caused drops from to generally bing was better at discerning authorship after japanese translations andgoogle was better with german translations bing however performed very poorlywith the two step translations described shortly we found that the synonym methodwas especially resistance to these passages and the neutral network and svm methodsperformed similarly throughout one step translations one step translations to and from german and japanesewere generally not effective at hiding the writing style of an author while there was areduction and it was sometimes statistically significant such as bing translations fromenglish to german to japanese to english on neutral network and svm classifiers thedrop in accuracy is not large enough to grant much comfort to those looking to maintaintheir anonymity as it generally only decreases the chance of identification from about to about in svm and neutral network approaches and only from to in the case of the synonym based method these numbers are not strong enoughto warrant a claim that they are effective in providing an anonymizing effect on adocument two steptranslations two steptranslationsfromenglishtogermantojapaneseand then back to english were generally no more effective at hiding the writing styleof an author than a one step translation with japanese except for the case of bingtranslator using svms and neutral networks as is explained next translation service comparison single step translation approaches to and fromgerman were less effective using google translate and japanese translations were lesseffective on bing translator bing seemed to produce very effective two step translationpassages for the nn and sum methods overall it appears as though bing s ability toconstruct adversaries passages well from an accuracy standpoint is greater than googletranslate bing s average accuracy across correctly identifying adversaries translationpassages is points lower than google and when the especially effective synonym based method is removed the difference increases to points this would indicatebing is better for attempting a machine translation based circumvention approach butoverall the accuracies are still not low enough to suggest it would be sufficient to protectprivacy and anonymity classifier method comparison the most effective of the three for all experiments aswell as the baseline was the synonym based method this method was demonstrated have high accuracy in past work but there is no previous work indicating that accuracypersists when looking at larger numbers of unique authors until now the baseline of for the synonym based method was the highest of the three classifiers the svmand neutral network using the basin feature set had baleine accuracies of and respectively using google translate the synonym based method maintained an accuracy of for japanese and for german one step translations the accuracy dropped onlyto for the two step translation similar results were found with the bing trans lator the svm and neutral network methods dropped to and for japanese respectively and and for german they also saw accuracies similar japanese one step translations for the two step translation experiment overlay these results are for the most part not statistically significant in favor ofthe translation having an anonymizing effect on the writing style of an author andwe believe the reduction in accuracy is not enough to warrant calling this an effectiveapproach to circumventing stylometry effectiveness of translated documents even if we were to accept a drop in accuracyby to points as sufficient for aiding the anonymization of a document would theresulting translated passage be acceptable for communication purposes or publication we observed that the answer to this question depends heavily on the complexity of thelanguage being translated here is an example sentence from cormac mccarthy thatappeared in his novel the road along with each translation original just remember that the things you put into your head are there forever hesaid english german english remember that the things that you are dead set on always there he said english japanese english but things are there forever remember what you put in your head he said english german japanese english you are dead that there always is set please do not forget what he said the original sentence was reasonably complex and did not fare well through thetranslation process while the translated sentences were incoherent the meaning wasfundamentally changed in each one but when we look at a simpler sentence from thatsame passage we find more consistent results original they passed through the city at noon of the day following english german english they crossed the city at noon the following day english japanese english they passed the city at noon the following day english german japanese english they crossed the city at noon the next day the translations of the simpler sentence are more effective but lack obfuscation the goal of the translation approach is to alter the writing style while retaining themeaning there are many examples of this that can be found in the translated passages such as fighting was tough with each house and factory fiercely contested beingtranslated to the fight was hard fought hard with every home and factory butthese are outweighed by the number of significantly altered meanings incoherenttranslations and very good but nonobfuscated translations . note pasting the contents here inasmuch is possible for the benefit of those who do not wish to download a pdf file the text refers to several charts and figures that i can not present here and consumption of the information provided in this study is aided significantly by the viewing of those charts and figures i recommend reading this text in the pdf format bobby source drexel university introductionstylometry is a form of authorship recognition that relies on the linguist informationfound in a document while stylometry existed before computers and artificial intel ligence the field is current dominated by ai techniques such as neutral networksand statistical pattern recognition our work opens a new avenue of research in thefield of authorship recognition adversaries stylometry we find that it is easy for an untrained individual to modify his or her writing style to protect her identity frombeing discovered through stylometric analysis we demonstrate the effectiveness ofmultiple methods of stylometry in nonadversarial settings and show that authors at tempting to modify their writing style can reduce the accuracy of these methods fromover to the level of random chance with this we have demonstrated that cur rent approaches to stylometry can not be relied upon in an adversaries setting it alsodemonstrated that for individuals seeking anonymity manual attempts at modifyingwriting style are promising as a countermeasure against stylometry we also show thatautomated attempts at circumventing stylometry using machine translation may notbe as effective often altering the meaning of text while providing only small drops in a curacy we compiled and published the first two corpora of adversaries stylometry data the brennan greenstadt adversaries stylometry corpus and the extended brennan greenstadt corpus to allow others to carry out their own research on the impact ofadversarial text on new and existing methods of stylometry historians and literary detectives have used stylometry with great success to identifythe authors of historical documents such as the federalist papers civil war letters and shakespeare s plays klarreich oakes the importance of stylometrycan be seen through modern applications in the field of forensics plagiarism andanonymity stylometry is even used as evidence in courts of law in multiply countriesincluding britain and the united states morton and michaelson in some criminal civil and security matters language can be evi dence when you are faced with a suspicious document whether you needto know who wrote it or if it is a real threat or a real suicide note or if its too close for comfort to some other document you need reliable validatedmethods the institute for linguist evidence stylometry has been a successful line of research but there is one underlying assump tion that has not been widely challenged that the author of an unknown documenthas been honest in his or her writing style we define adversaries stylometry as thenotion of applying deception to writing style to affect the outcome of stylometric canal ysis this new problem space in the field of stylometry leads to new questions suchas what happens when authorship recognition is applied to deceptive writing caneffective privacy preserving countermeasures to stylometry be developed what breathe implications of looking at stylometry in an adversaries context dr patrick juola an expert in computer linguistics at duquesne university dis cussed the importance of research in this area in his monograph on authorshipattribution stating there is obviously great potential for further work here juola our research shows that unexpert human subjects can defeat multiply style etry methods simply by consciously hiding their writing style or imitating the style another author stylometry is also a necessary new ground for research in privacy and security current anonymity and circumvention systems focus strongly on location basedprivacy but do not address many avenues for the leakage of identification throughthe content of data writing style as a marker of identity is not addressed in currentcircumvention tools nor is it addressed in the security and privacy community atlarge given the high accuracy of even basin stylometry systems this is not a topic thatch afford to be overlooked our contributions include multiply methods of circumventing author recognitionthrough stylometry and experimental results that show the efficacy of doing through deceptive writing we found that the accuracy of all the methods we examineddropped significantly when presented with adversaries passages created by inexperi ended human adversaries using two of the three circumvention methods this work demonstrates the need for testing stylometry techniques in adversaries contexts whenthe application domain warrants it and a framework including a publicly availableadversarial corpus for doing so section presents background and related work in stylometry and discusses theimplications of stylometry techniques for privacy and anonymity section discussesour experimental methodology and presents our three methods for circumventingstylometry section presents experimental results of classifying circumventionpassages on three representative stylometry techniques and discusses the techniquesour human subjects used to modify their writing style section discusses implicationsfor future work . adjusting writing style informed discussions with participants after completing the study we found thatmany of them tried to obfuscate their style by dumbing down their writing by us ing shorter sentences and less descriptive words when imitating the writing style ofcormac mccarthy the participants described attempting to use descriptive and grimlanguage we can verify at least some of these claims through analysis of the featurefrequency changes in the obfuscation and imitation passages we compare the normalized frequencies of the features to understand which onespeople change to hide their writing style the changes made to a selection of featuresfor both obfuscation and imitation passages can be seen in figure this graphillustrates the changes in frequencies for each feature they axis contains a list of features that have been adjusted in the passages andx axis of the graph denotes thechange in each feature we compute the change in feature using the following formula code select change in feature f cf fadv freg freg where fade average values of feature f in adversaries documents free average values of feature f in regular documents we add one with freg in the denominator to avoid divide by zero error asfreg can be zero for some features the amount to the right of they axis represents the increasesin a feature and the amount to the left represents the decreases in our experiments the most changed features are average syllables average wordlength sentence count average sentence length usage of personal pronouns adjec tives and verbs and readability index we do see hints that authors are dumbingdown their writing style with shorter sentences lower readability scores and lesscomplex words most of these are the features in our basin feature set which mayexplain why that feature set can be effective despite its relatively small size in imitation passages all the participants use more personal pronouns and verbsand shorter sentences than the regular cases the personal pronouns can likely attributed to the direction of the writing survey which asked participants to describetheir day in the third person the shorter sentences however are likely a result ofimitating cormac mccarthy who often uses short sentences in his prose in bothimitation and obfuscation passages participants use shorter and simpler words thosewith only one or two syllables and shorter sentences as a result adversaries writingsare easier to read than regular writings . support vector machine and the writeprints state approach writeprints is one of themost successful methods of stylometry that has been published to date because of thigh levels of accuracy on a range of datasets with large numbers of unique authors unfortunately this accuracy comes at a high computation cost in order to performthe robust experiments we designed for this study we have created our own approve imation of the writeprints algorithm that performs comparable but has much lowercomputation cost that allows us to run large numbers of experiments in a reasonabletimeframe we will summarize the writeprints method and highlight the feature setscreated for this approach and how we merged those feature sets into our approach the writeprints technique constructs a single classifier using feature sets that arespecific to each individual author rather than being generalled across the set of po central authors the method has two major parts writeprint creation and patterndisruption the writeprint creation step constructsn dimensions hyperplanes thatrepresent an individual author s writing style where n is the number of features in thefeature set the pattern disruption step identifies zero usage features and shifts thewriteprint representation further away from writeprints that have nonzero values forthe same features which in turn decreases the level of stylistic similarity between twoseparate authors there are many nuances to this approach that we will not discusshere but are described in detail in the original research paper on writeprints one of the most valuable pieces of research that has come from the creation ofwriteprints are the baseline and extended feature sets abbasi and chen the baseline dataset has features whereas the extended set contains tens of thousands the primary difference between these two datasets however is that the baseline set contains only state features in that the contents do not change with the addition andremoval of documents the extended feature set contains many elements that are basedon the documents being clarified and is much larger as a consequence examples these dynamite features are the most common misspellings and character bigrams the corpus we used many pieces of the feature set created by writeprints to mitigate the issue of writeprints high computation cost we have combined ahybrid version of the writeprints feature sets with a support vector machine thisresults in a faster method that has a higher but comparable precision on our cor pus we validated the effectiveness of this approach by comparing it to the originalwriteprints approach for select datasets from the original brennan greenstadt corpusand found that the precision of our approach is comparable to the precision of the com plate writeprints method as can be seen in figure this is also in line with resultsfrom the original writeprints research which compared the approach with a variety ofothers including svms using the same feature set the feature set we use combines the brevity and state nature of the baseline set withsome of the more complex features of the extended set we call this the writeprintsstatic feature set it contains state features detailed in table ii we applied thisfeature set to a support vector machine svm classifier in the form of a sequentialminimal optimization smo with a polynomial kernel using weak machine learningsoftware . background related work and motivationstylometry is based on the assumption that every individual has a unique style to his orher writing to some degree just as every individual has a unique fingerprint the mostfamous early example of this assumption being put into practice with stylometry is theclassic case of the federalist papers eighty five papers were published anonymously the late th century to persuade the people of new york to ratify the americanconstitution the authorship of of these papers was heavily contested oakes to discover who wrote the unknown papers researchers have analyzed the writingstyles of the known federalist authors and compared them to the papers of unknownauthorship the features used to determine writing styles have been quite varied original attempts looked at the length of words whereas later attempts looked atpairs of words vocabulary usage sentence structure function words and so on moststudies show the author was james mansion recent work in stylometry has been feared towards high accuracy in larger andmore diverse datasets writeprints is a method that has demonstrated superiorityover all other modern stylometry techniques because of its ability to categorize largecorpora that include to unique authors in many different contexts includingonline chat messages ebay comments email messages abbasi and chen it isan unsupervised method that can be used for both authorship recognition of knownauthors and similarity detection among unknown author documents this approachis significant in the field of stylometry because of its use of individual author levelfeature sets and pattern disruption and also because of its high accuracy in authorshiprecognition stylometry has played a serious role in forensics over the years especially in the s with the introduction of the cumulative sum or cusum technique this methodmeasures the stability of a specific feature throughout multiply documents to establishauthorship and was adopted in many court cases specifically in england but theaccuracy of cusum came under fire culminating in massive controversy about the used the technique matthews it has since been modified with better accuracy thathas been supported by researchers but questions of accuracy are still considered bysome to be the biggest problem faced by the field juola a number of resources are available that give an overview of stylometry methodsthat exist malyutov uzuner and katz and focus on the state of the fieldas it relates to computer science and computer linguistics juola or digital foren sics chaski artificial intelligence has been embraced in the field of stylometry leading to more robust classifiers using machine learning and other ai techniques tweedie et al holmes and forth uzuner and katz early work in the field we now call adversaries stylometry includes researches dr josyula rao and dr pankaj rohatgi studying the impact of stylometry onpseudonymity and determining that words of writing is enough to leak theidentity of an author rao and rohatgi others have looked at the potential automatically obfuscating a document to preserve anonymity and determined thatin the case of the federalist papers it took just changes per words to shiftauthorship from madison to hamilton kacmarcik and gamon this work however does not modify the actual text in the documents they instead modified thenumerical feature vectors after they have been extracted from the original text previous research in the field has also looked at authorship recognition and pasticheby comparing the work of gilbert adair to that of lewis caroll whom he was trying imitate by writing follow up stories to alice in wonderland somers and tweedie however our work is the first to apply adversaries stylometry to actual docu ments written by humans to defeat stylometry and test the results against multiplemethods and features sets patrick juola validated the effectiveness of adversarialwriting on stylometry by evaluating the brennan greenstadt corpus with jgaapand also demonstrated some methods resistance to such writing samples juola andvescovi . conclusionthis study demonstrates the effectiveness of adversaries writing against modern met ods of stylometry the analysis of stylometry techniques and their weaknesses to adver sail writing demonstrates that we must test stylometry methods for their resistanceto adversaries in situations where their presence is likely we advocate a strongerstance of not relying on stylometry in sensitive situations where the authorship of unknown document must be known with a high degree of certainty unless the possi bility of a modified writing style is eligible the obfuscation approach weakens all methods to the point that they are no betterthan random guessing the correct author of a document the imitation approachwas widely successful in causing authorship to be attributed to the intended imitationtarget additional these passages were generated by participants in very short peri ods of time by amateur writers who lacked expertise in stylometry translation withwidely available machine translation services does not appear to be a liable mode ofcircumvention our evaluation did not demonstrate sufficient anonymization and thetranslated document has at best questionable grammar and quality there has long been a case to be made for a multidisciplinary approach to privacyand anonymity this research shows both the necessity of considering writing styleanalysis as a component of that approach and demonstrates the possibility for privacy conscious individuals to take steps to maintain their anonymity in the face of advancedstylometric techniques this work provides further evidence that learning techniques used in adversarialsettings need to be tested with adversaries test sets this research also has implied tions for machine translation research through the use of stylometry as a method fortesting the effectiveness of machine translation if a machine translated dataset showscomparable accuracy in an adversaries stylometry setting then the results may be used validate the translation method this study also strengthens the original claims of high accuracies by validating themethods on a large set of new data produced for a variety of purposes when thesemethods are used in situations where adversaries are not considered to be a threat they perform quite well acknowledgmentsthe authors wish to acknowledge the hard work of privacy security and automation lab researchers andrewmcdonald again caliskan and ariel stolerman . methodologywe collected writing samples from human subjects that represented both their unmodi fied writing style and attempts to create adversaries passages to circumvent stylometrymethods we identified three methods of creating documents to circumvent stylometry two of which are written by human subjects and one of which is an automatic method we implemented three methods of stylometry to analyze the effect of these adversarialpassages by attempting to recognize the original author the next section explains our method for creating a survey and collecting the datarequired for the unique adversaries corpus upon which this study was based we thendetail the three types of circumvention passages that have been examined in this study finally we summarize the implementation of each method used in the study and theircorresponding feature sets study setup and formatwhile there are a virtual limitless number of corpora with known author informationthat can be used for stylometry research we had to develop our own unique corpus forevaluating the effects of adversaries passages this was necessary because in additionto a large number of texts from unique authors we also needed adversaries texts writtenby those same authors the collection of this data was performed through the use ofa survey by voluntary participants of various backgrounds though all participantsheld at least some college education the brennan greenstadt adversaries stylometrycorpus is freely available there were three basin elements for participation in the survey first each authorhad to submit at least words of preexisting sample writing each writing samplehad to be from some sort of formal source such as essays for school reports for work and other professional and academy correspondence this was intended to eliminateslang and abbreviations instead concentrating on consistent formal writing style everyone involved this also helped to limit possible errors that are not a result the malicious attempts but nonetheless could have an effect on the accuracy of theauthorship attribution participants submitted at least two sample documents witha median submission of documents per author which were combined and split intoapproximately word sample passages second each author had to write an obfuscation passage in which he tried to hide hisidentity through his writing style this was accomplished by writing a new wordpassage on a specific topic the task given to them was to write a description of theirneighborhood to someone who has never been there as part of a college application this task was designed to encourage thought about the writing style and not weighdown the author with having to think about a complex topic or be too creative with hercontent third each author had to write an imitation passage in which she tries to imitateanother author s style for this task the participants were given a word samplefrom the road by cormac mccarthy to model their passage after this selection wasmade for a variety of reasons imitation and pastiche is a common writing task as canbe seen through contest such as the international hemingway imitation competitionand the aux faulkner contest afroz et al in addition cormac mccarthy hasa somewhat distinct writing style on the far side of the river valley the road passed through a stark blackburn charged and limitless trunks of trees stretching away on every side ash moving over the road and the sagging hands of blind wire strung fromthe blackened lightpoles whining thinly in the wind mccarthy since the participants are not professional writers this would allow them to make fair attempt at emulating another style the popularity of the book was also a factor inits selection for this study as we wanted to engage participants with reading samplesthat had broad appeal to a variety of readers in engaging participants we soughtmore effort in writing passages and thus more robust results the writing task givento participants was to narrate their day from the point at which they get out of bed and to do so using a third person perspective this is also similar to the events the sample text for testing purposes an additional words were taken froth road and used as training text for cormac mccarthy along with the original sample it should also be noted that the excerpt distributed to readers is freely available as apromotional passage from the book asking the participants to write the obfuscation passages before the imitation pas sage was intentional we were concerned that if participants chose to write the smith tion passage first then all of the obfuscation passages would simply read as a secondcormac mccarthy imitation attempt . background related work and motivationstylometry is based on the assumption that every individual has a unique style to his orher writing to some degree just as every individual has a unique fingerprint the mostfamous early example of this assumption being put into practice with stylometry is theclassic case of the federalist papers eighty five papers were published anonymously the late th century to persuade the people of new york to ratify the americanconstitution the authorship of of these papers was heavily contested oakes to discover who wrote the unknown papers researchers have analyzed the writingstyles of the known federalist authors and compared them to the papers of unknownauthorship the features used to determine writing styles have been quite varied original attempts looked at the length of words whereas later attempts looked atpairs of words vocabulary usage sentence structure function words and so on moststudies show the author was james mansion recent work in stylometry has been feared towards high accuracy in larger andmore diverse datasets writeprints is a method that has demonstrated superiorityover all other modern stylometry techniques because of its ability to categorize largecorpora that include to unique authors in many different contexts includingonline chat messages ebay comments email messages abbasi and chen it isan unsupervised method that can be used for both authorship recognition of knownauthors and similarity detection among unknown author documents this approachis significant in the field of stylometry because of its use of individual author levelfeature sets and pattern disruption and also because of its high accuracy in authorshiprecognition stylometry has played a serious role in forensics over the years especially in the s with the introduction of the cumulative sum or cusum technique this methodmeasures the stability of a specific feature throughout multiply documents to establishauthorship and was adopted in many court cases specifically in england but theaccuracy of cusum came under fire culminating in massive controversy about the used the technique matthews it has since been modified with better accuracy thathas been supported by researchers but questions of accuracy are still considered bysome to be the biggest problem faced by the field juola a number of resources are available that give an overview of stylometry methodsthat exist malyutov uzuner and katz and focus on the state of the fieldas it relates to computer science and computer linguistics juola or digital foren sics chaski artificial intelligence has been embraced in the field of stylometry leading to more robust classifiers using machine learning and other ai techniques tweedie et al holmes and forth uzuner and katz early work in the field we now call adversaries stylometry includes researches dr josyula rao and dr pankaj rohatgi studying the impact of stylometry onpseudonymity and determining that words of writing is enough to leak theidentity of an author rao and rohatgi others have looked at the potential automatically obfuscating a document to preserve anonymity and determined thatin the case of the federalist papers it took just changes per words to shiftauthorship from madison to hamilton kacmarcik and gamon this work however does not modify the actual text in the documents they instead modified thenumerical feature vectors after they have been extracted from the original text previous research in the field has also looked at authorship recognition and pasticheby comparing the work of gilbert adair to that of lewis caroll whom he was trying imitate by writing follow up stories to alice in wonderland somers and tweedie however our work is the first to apply adversaries stylometry to actual docu ments written by humans to defeat stylometry and test the results against multiplemethods and features sets patrick juola validated the effectiveness of adversarialwriting on stylometry by evaluating the brennan greenstadt corpus with jgaapand also demonstrated some methods resistance to such writing samples juola andvescovi . the role of stylometry in privacy and anonymitya multidisciplinary approach to privacy has long demanded greater attention thatincludes much more than traditional location based circumvention and anonymitytools adams users of the internet with the desire to publish anonymouslymay also desire to hide their writing style to circumvent stylometry techniques thegoal of these authors is to keep their identity private with the growth of stylometryas an accurate means of determining authorship the question of how it affects privacyand anonymity in the information age is becoming increasingly important privacy andanonymity are held in high regard by many activists journalists and law enforcementofficers the introduction of adversaries stylometry and the use of circumvention pas sages on stylometry are possible means of ensuring the privacy and anonymity of individual the largest example of the value of anonymous speech to these groups isthe tor project an anonymous communication tool originally developed by the navalresearch laboratory that is utilized by hundreds of thousands of individuals includinglaw enforcement journalists activists businesses and more the tor project a b the privacy issues concerning stylometry can be summarized through an examplescenario alice the anonymous blogger versus bob the abuse employer alice is unemployed at bob s company the widget design corporation alice a long time em ployee wishes to draw attention to the various system abuses that she has observed the company under bob s management such as harassment unpaid overtime andemployees being encouraged to rip off competing widget designers she decides to dothis by publishing an open anonymous letter on the web she has personally authoreddetailing these abuses she takes great care to use privacy enhancing technologies outlined by dr rao and dr rohatgi so that it is very difficult if not impossible to traceher post back to her identity or her in address rao and rohatgi the letter draws criticism of the company from the press and bob decides to discoverthe author believing it came from within his company due to the details it revealed this case bob s company has about employees if bob has access to a stylometrysystem such as writeprints that has very high accuracy for large numbers of potentialauthors abbasi and chen he can collect words from each employee swriting probably though various reports and emails that they have written in theirtime at the company and come to believe that alice is the writer of the document withover probability he can then take action against alice that may compromise her job this hypothetically scenario represents a reasonable threat that is within the range ofability for current methods of stylometry however the threat presented to anonymityis not purely hypothetically in his book inside wikileaks former wikileaksspokesman daniel domscheitt berg discussed the potential impact of stylometry onthe organization after attending a presentation on adversaries stylometry at the thchaos communication congress if someone had run wikileaks documents through such a program hewould have discovered that the same two people were behind all the vari ous press releases document summaries and correspondence issued by theproject the official number of volunteers we had was also to put it mildly grotesquely exaggerated domscheit berg et al the reality of using stylometry to identify individuals who wish to remainanonymous can also be seen from the perspective of law enforcement it is high lighted in the recently commissioned abi report state of the art biometric excellenceroadmap saber as non handwritten communications become more prevalent such as blog ging text messaging and emails there is a growing need to identify writ ers not by their written script but by analysis of the type content cur gently there are some studies in the area of writer s colloquial analysisthat may lead to the emerging technology of writer identification in theblogosphere colosimo et al recent work by the authors of this article examines the domain of deception andwriting style in real world scenarios like the gay girl in damascus blog in which american writer masqueraded as a syrian woman this fraudulent blog was uncoveredthrough forensic means but this research shows that stylometry demonstrates a strongcorrelation between the pseudonymous blog and writing samples by the true identityof the author afroz et al the technical ability for a method of stylometry to present such a threat to anonymityis explained in the next section the methodology used in writeprints in particular hasdemonstrated the potential for identifying a single author among up to uniqueidentities as a result bloggers and others may have reason to circumvent stylometryto protect their privacy and anonymity adversaries stylometry can be viewed in thisway as a means for maintaining privacy or anonymity . note pasting the contents here inasmuch is possible for the benefit of those who do not wish to download a pdf file the text refers to several charts and figures that i can not present here and consumption of the information provided in this study is aided significantly by the viewing of those charts and figures i recommend reading this text in the pdf format bobby source drexel university introductionstylometry is a form of authorship recognition that relies on the linguist informationfound in a document while stylometry existed before computers and artificial intel ligence the field is current dominated by ai techniques such as neutral networksand statistical pattern recognition our work opens a new avenue of research in thefield of authorship recognition adversaries stylometry we find that it is easy for an untrained individual to modify his or her writing style to protect her identity frombeing discovered through stylometric analysis we demonstrate the effectiveness ofmultiple methods of stylometry in nonadversarial settings and show that authors at tempting to modify their writing style can reduce the accuracy of these methods fromover to the level of random chance with this we have demonstrated that cur rent approaches to stylometry can not be relied upon in an adversaries setting it alsodemonstrated that for individuals seeking anonymity manual attempts at modifyingwriting style are promising as a countermeasure against stylometry we also show thatautomated attempts at circumventing stylometry using machine translation may notbe as effective often altering the meaning of text while providing only small drops in a curacy we compiled and published the first two corpora of adversaries stylometry data the brennan greenstadt adversaries stylometry corpus and the extended brennan greenstadt corpus to allow others to carry out their own research on the impact ofadversarial text on new and existing methods of stylometry historians and literary detectives have used stylometry with great success to identifythe authors of historical documents such as the federalist papers civil war letters and shakespeare s plays klarreich oakes the importance of stylometrycan be seen through modern applications in the field of forensics plagiarism andanonymity stylometry is even used as evidence in courts of law in multiply countriesincluding britain and the united states morton and michaelson in some criminal civil and security matters language can be evi dence when you are faced with a suspicious document whether you needto know who wrote it or if it is a real threat or a real suicide note or if its too close for comfort to some other document you need reliable validatedmethods the institute for linguist evidence stylometry has been a successful line of research but there is one underlying assump tion that has not been widely challenged that the author of an unknown documenthas been honest in his or her writing style we define adversaries stylometry as thenotion of applying deception to writing style to affect the outcome of stylometric canal ysis this new problem space in the field of stylometry leads to new questions suchas what happens when authorship recognition is applied to deceptive writing caneffective privacy preserving countermeasures to stylometry be developed what breathe implications of looking at stylometry in an adversaries context dr patrick juola an expert in computer linguistics at duquesne university dis cussed the importance of research in this area in his monograph on authorshipattribution stating there is obviously great potential for further work here juola our research shows that unexpert human subjects can defeat multiply style etry methods simply by consciously hiding their writing style or imitating the style another author stylometry is also a necessary new ground for research in privacy and security current anonymity and circumvention systems focus strongly on location basedprivacy but do not address many avenues for the leakage of identification throughthe content of data writing style as a marker of identity is not addressed in currentcircumvention tools nor is it addressed in the security and privacy community atlarge given the high accuracy of even basin stylometry systems this is not a topic thatch afford to be overlooked our contributions include multiply methods of circumventing author recognitionthrough stylometry and experimental results that show the efficacy of doing through deceptive writing we found that the accuracy of all the methods we examineddropped significantly when presented with adversaries passages created by inexperi ended human adversaries using two of the three circumvention methods this work demonstrates the need for testing stylometry techniques in adversaries contexts whenthe application domain warrants it and a framework including a publicly availableadversarial corpus for doing so section presents background and related work in stylometry and discusses theimplications of stylometry techniques for privacy and anonymity section discussesour experimental methodology and presents our three methods for circumventingstylometry section presents experimental results of classifying circumventionpassages on three representative stylometry techniques and discusses the techniquesour human subjects used to modify their writing style section discusses implicationsfor future work . evaluationthere are two ways to think of success when evaluating how stylometric methodsrespond to adversaries writing samples one way is to measure the success of themethod in identifying the true author of a document intended to circumvent stylometryand the other is to measure the success of the circumvention passage in preserving theanonymity of the author we will examine the results from both angles to test the success we look at the performance on different sets of unique authors our dataset consisted of a total of unique authors this is a larger number of uniqueauthors than almost all of stylometry studies cited throughout this article and is incline with the current state of the art writeprints which looked at writing samples of and unique authors in order to evaluate the corpus we set up test sets for unique sets of and authors out of a total sample pool of authors from theextended brennan greenstadt corpus the precision measurement discussed for anyauthor count throughout this section refers to the average across all sets all ofthe baseline results are based on tenfold cross validation the precision for classifyingobfuscation imitation and translation passages is measured by training each classifieron the entire modified corpus for the authors in a test set and testing that classifieron the corresponding circumvention passages the graphs in this article refer to theprecision because we believe that is the most important and intuitive measurementwhen determining the authorship of an individual unknown document we have madeadditional graphs available reflecting recall and f measure on our web site the high number of combinations is uncommon in stylometry research but we believe it is important accuracy between different sets of authors can vary significantlydepending on the specific authors chosen by viewing the potential combinations of au thors as the sample space and a specific combination of authors as a sample selectionwe are able to make robust accuracy claims with animal standard error standarderror in this case is evaluating the extended brennan greenstadt corpusin order to substantiate the results we present in this article as being in line with ourprevious work we evaluated all of the methods presented in this work on our originaldataset the brennan greenstadt corpus we utilized the author counts available incur original paper given the smaller dataset we found that the precision for eachapproach is comparable on all datasets the basin neutral network approach saw slight drop as can be seen in figure the others were nearly identical as seen with thewriteprints state approach in figure and the synonym based approach in figure baselinefigure demonstrates the effectiveness of the four methods we tested and the accuracy random chance classification the random chance line in all figures represents whate precision would be if the authorship of a document was determined by randomlyselecting one of the potential authors all of the results for the baseline precision mea surements are statistically significant over random chance all methods show a dear dation of precision as the number of unique authors increases but the effectiveness isstill quite substantial at even the largest author set the writeprints state featureset utilizing an svm demonstrates the highest precision overall the synonym basedapproach is also very effective the basin feature set does poorly compared to theother two methods but is still far above that of random chance this is importantand confirms our hypothesis that even a very simple measurement of writing styles effective for small numbers of authors and still demonstrates significant ability fordeanonymization with larger numbers of authors our sum approach was evaluated using tenfold cross validation in the same manneras the rest of our experiments this is more robust than the approach utilized by thewriteprints authors in addition the amount of training data available per author such lower in our dataset than most of the sets used in the writeprints research our classification is based on approximate words per author we allow for errorwithin words of in order to not break up sentences as opposed to anywherefrom to in most datasets used in the writeprints study these pointsare important because while our method does not achieve the same levels of precisionat the writeprints approach it still approaches those numbers despite the relaxedconstraints of our evaluation the neutral network approach displays accuracies varying between and this is not as effective as the other approaches but is high enough to be an effectiveform of authorship recognition and a legitimate threat to privacy and anonymity thesynonym based approach however performs exceedingly well even at higher num bers of unique authors the degradation in precision follows the same decline as thewriteprints state feature set approach the synonym and writeprints state approaches follow a smooth degradation curveas the number of authors in each set increases this confirms hypotheses of our rig final research that the degradation curve we observed using the original brennan greenstadt corpus would extend to larger numbers of authors . circumventing stylometrywe developed three methods of circumvention against stylometry techniques in theform of obfuscation imitation and machine translation passages two of these obeys cation and imitation were mutually written by human subjects these passages werevery effective at circumventing attempts at authorship recognition machine trans lation passages are automatic attempts at obfuscation utilizing machine translationservices these passages were not sufficient in obfuscating the identity of an author the full results and effectiveness of these circumvention methods are detailed in theevaluation section obfuscation in the obfuscation approach the author attempts to write a doct ment in such a way that her personal writing style will not be recognized there is noguidance for how to do this and there is no specific target for the writing sample anideal obfuscated document would be difficult to attribute to any author for our study however we only look at whether or not it successfully deters recognition of the trueauthor imitation the imitation approach is when an author attempts to write a doct ment such that her writing style will be recognized as that of another specific author the target is decided upon before a document is written and success is measured bothby how successful the document is in deterring authorship recognition systems anyhow successful it is in imitating the target author this could also be thought of as a framing attack machine translation the machine translation approach translates an unmodifiedpassage written in english to another language or to two other languages and thenback to english our hypothesis was that this would sufficiently alter the writing styleand obfuscate the identity of the original author we did not find this to be the case we studied this problem through a variety of translation services and languages we measured the effectiveness of the translation as an automatic method as wellas the accuracy of the translation in producing comprehensible incoherent obfuscationpassages we performed three language experiments in addition to the english baseline in allcases the original and final language were english we performed single step translate tions from english to german and back to english as well as english to japanese andback to english we then performed two step translations from english to german japanese and then back to english german was chosen for its linguist similaritiesto english and japanese for its differences the two machine translation services we compared were goggle translateand bing translator both services are free and based on statistical machine translation methods and feature setswe selected a series of stylometry techniques that represent a variety of potentialapproaches both in machine learning methodology and feature selection the featureselections range from basin to comprehensive and the methods from simple and novelty robust and unique see table i neutral network and the basin feature set the most straightforward stylometrytechniques are those that use traditional machine learning methods with some set oflinguistic features kacmarcik and amon rao and rohatgi the effect tiveness of neutral networks tweedie et al in stylometry established machinelearning as an integral part of modern authorship analysis we implemented a neuralnetwork with a simple straightforward feature set the purpose of the simple sea ture set and basin machine learning approach is to demonstrate the effectiveness ofstylometry even with a limited representation of something as complex as writing style the features used for the neutral network and svm classifiers which we will call the basin feature set include nine linguist measurements number of unique words clerical density gunning fog readability index character count without whitespace average syllables per word sentence count average sentence length and the flesch kincaid readability test the number of hidden layers in the neutral network classifierwas based on the number of features and the number of classes number of features number of classes the feature extraction for this set was done with the jstylotool synonym based approach developed by clark and hannon the synonym basedapproach demonstrates the continuing value of novel techniques in stylometry thismethod exploits the choice of a specific word given all the possible alternatives thatexist the theory behind this method is that when a word has a large number ofsynonyms the choice the author makes is significant in understanding his or herwriting style clark and hannon an example analysis of three sentences can beseen in figure the synonym based approach represents the potential effectivenessof using a single type of feature vector for stylometric analysis the method called for a vocabulary based feature set a feature vector is created foreach word w in a text having two elements the number of synonymous s that the wordhas according to princeton s wordnet clerical database miller and the sharedfrequency n of the word w between the sample text and the training text of a knownauthor the match value for a sample text u from an unknown author and a reference text k from a known author is then the sum of n x s for all shared words betweenthe two texts authorship is attributed to a text based on the known author with thehighest match value to the sample text the method also takes into account the overallfrequency of a word in all of the available text as well as removing words that appearon a stop list of the most common words in the english language a sample textis attributed to the author with the highest match value over all samples from thatauthor . the role of stylometry in privacy and anonymitya multidisciplinary approach to privacy has long demanded greater attention thatincludes much more than traditional location based circumvention and anonymitytools adams users of the internet with the desire to publish anonymouslymay also desire to hide their writing style to circumvent stylometry techniques thegoal of these authors is to keep their identity private with the growth of stylometryas an accurate means of determining authorship the question of how it affects privacyand anonymity in the information age is becoming increasingly important privacy andanonymity are held in high regard by many activists journalists and law enforcementofficers the introduction of adversaries stylometry and the use of circumvention pas sages on stylometry are possible means of ensuring the privacy and anonymity of individual the largest example of the value of anonymous speech to these groups isthe tor project an anonymous communication tool originally developed by the navalresearch laboratory that is utilized by hundreds of thousands of individuals includinglaw enforcement journalists activists businesses and more the tor project a b the privacy issues concerning stylometry can be summarized through an examplescenario alice the anonymous blogger versus bob the abuse employer alice is unemployed at bob s company the widget design corporation alice a long time em ployee wishes to draw attention to the various system abuses that she has observed the company under bob s management such as harassment unpaid overtime andemployees being encouraged to rip off competing widget designers she decides to dothis by publishing an open anonymous letter on the web she has personally authoreddetailing these abuses she takes great care to use privacy enhancing technologies outlined by dr rao and dr rohatgi so that it is very difficult if not impossible to traceher post back to her identity or her in address rao and rohatgi the letter draws criticism of the company from the press and bob decides to discoverthe author believing it came from within his company due to the details it revealed this case bob s company has about employees if bob has access to a stylometrysystem such as writeprints that has very high accuracy for large numbers of potentialauthors abbasi and chen he can collect words from each employee swriting probably though various reports and emails that they have written in theirtime at the company and come to believe that alice is the writer of the document withover probability he can then take action against alice that may compromise her job this hypothetically scenario represents a reasonable threat that is within the range ofability for current methods of stylometry however the threat presented to anonymityis not purely hypothetically in his book inside wikileaks former wikileaksspokesman daniel domscheitt berg discussed the potential impact of stylometry onthe organization after attending a presentation on adversaries stylometry at the thchaos communication congress if someone had run wikileaks documents through such a program hewould have discovered that the same two people were behind all the vari ous press releases document summaries and correspondence issued by theproject the official number of volunteers we had was also to put it mildly grotesquely exaggerated domscheit berg et al the reality of using stylometry to identify individuals who wish to remainanonymous can also be seen from the perspective of law enforcement it is high lighted in the recently commissioned abi report state of the art biometric excellenceroadmap saber as non handwritten communications become more prevalent such as blog ging text messaging and emails there is a growing need to identify writ ers not by their written script but by analysis of the type content cur gently there are some studies in the area of writer s colloquial analysisthat may lead to the emerging technology of writer identification in theblogosphere colosimo et al recent work by the authors of this article examines the domain of deception andwriting style in real world scenarios like the gay girl in damascus blog in which american writer masqueraded as a syrian woman this fraudulent blog was uncoveredthrough forensic means but this research shows that stylometry demonstrates a strongcorrelation between the pseudonymous blog and writing samples by the true identityof the author afroz et al the technical ability for a method of stylometry to present such a threat to anonymityis explained in the next section the methodology used in writeprints in particular hasdemonstrated the potential for identifying a single author among up to uniqueidentities as a result bloggers and others may have reason to circumvent stylometryto protect their privacy and anonymity adversaries stylometry can be viewed in thisway as a means for maintaining privacy or anonymity . obfuscation and imitation circumvention approachesattempting to recognize the authors of the obfuscation passages results in a drop of a curacy to around that of chance classification as can be seen in figure only the svm writeprints state approach displayed an effectiveness above that of random chance this demonstrates the weakness of these methods in detecting writing style designedto be adversaries the fact that the obfuscated passages track closely to random chancesis exactly what we would expect for well constructed obfuscation passages imitation passages demonstrated a significant drop in reflectiveness beyond theobfuscation approach for the neutral network and svm based approaches and main tainedchance levelaccuracyforwriteprintsandthesynonym basedmethodsasisseenin figure the lower than chance results for svm and neutral network approachesdemonstrates a difference in how resistance different methods are to adversaries pas sages this might indicate that the features modified in attempting to imitate cormacmccarthy are well represented in the basin feature set but not so much by theextensive writeprints state feature set or the specific but unique synonym analysis furthermore the ability of the imitation passages to successfully imitate cormacmccarthy was high this is demonstrated in figure the synonym based approach despite very high precision in the baseline tests was just as susceptible to misclassifica tion of imitation passages towards cormac mccarthy as the neutral network approachthat it greatly outperformed in the baseline evaluation it is important to note that the testing of imitation success required a modificationto the evaluation process in order to test for the success of an imitation passage inrepresenting cormac mccarthy s writing styles we needed to include training texts forcormac mccarthy when building the classifier this is only done for the measurementof imitation success no cormac mccarthy text is used in the training process for anyother tests while stylometry methods that are resistance to adversaries passages have been he pothesized brennan and greenstadt juola and vescovi we can see the firsttrue instance of it here with the results from the svm approach the precision whenclassifying obfuscation passages never drops below that of random chance further more it can be considered resistance to wrongly classifying imitation passages as theirintended target as it generally is only half as likely as the other methods to classify animitation passage as having been written by cormac mccarthy it is likely however that more sophisticated circumvention methods could pose a more significant threat tothis approach . the brennan greenstadt and extended brennan greenstadt corporawe have published two freely available research corpora the first is the brennan greenstadt corpus which is based on a survey conducted through drexel universityand contains authors who volunteered their time and were not compensated fortheir efforts this corpus was the basis for our original work on adversaries style try brennan and greenstadt the second is the extended brennan greenstadtcorpus containing authors solicited through the amazon mechanical turk platform submissions were petted against a series of guidelines to ensure the quality of thecontent as described next brennan greenstadt corpus participants for the brennan greenstadt corpuswere solicited through classes at rebel university colleagues and other personalrelationships this provided us with submissions from authors the brennan greenstadt corpus used an earlier version of the survey which had two relaxed re quirements authors were only required to submit words of preexisting writingand they were not required to fill out a demographic survey while this corpus was sufficient for preliminary results presented in earlierwork brennan and greenstadt we desired a more robust corpus in order toconfirm our original findings in a larger author space with a greater diversity of writ ers and tweaked survey requirements extended brennan greenstadt corpus we utilized the amazon mechanical turk amt platform to create a large and diverse corpus that could be used for more robustanalysis amazon mechanical turk is a platform that provides access to a large and diversepopulation that is willing to perform human intelligence tasks participants choosetasks that they would like to complete in exchange for a sum of money decided by atask creator submission quality is a serious consideration when using the amt platform the completion of a task does not necessarily indicate that the worker has followeth directions and completed it correctly in order to ensure that the submissions wereacceptable we reviewed every submission and judged their acceptability by scrutinizingthem according to the guidelines and requirements listed on the submission form only removed authors from the dataset who did not adhere to the directions of thesurvey we did not remove authors because of the quality of their writing demographicinformation or anything other than their ability to follow directions in addition to the existing requirements we published four guidelines that submis sions should adhere to the submitted preexisting writing was to be scholar in nature i e a persuasivepiece opinion paper research paper journals etc anything that is not the writing content of the work should be removed i e city tions curls section headings editing notes etc the papers samples should have a animal amount of dialog quotations please refrain from submitting samples of less than words laboratory another overlay scientific reports q a style samples such as exams and anythingwritten in another person s style as an added incentive for authors to take care with their submissions we offered about payment of two dollars on top of an original payment of three dollars if theirsubmission adhered to the quality guidelines of the submissions we received satisfied the requirements of the survey these submissions make up the extended brennan greenstadt adversaries stylometry corpus and are the basis of the evaluationfor this research it took about one hour on average for a participant to finish thecomplete task the entire instruction set for participation is available online . background related work and motivationstylometry is based on the assumption that every individual has a unique style to his orher writing to some degree just as every individual has a unique fingerprint the mostfamous early example of this assumption being put into practice with stylometry is theclassic case of the federalist papers eighty five papers were published anonymously the late th century to persuade the people of new york to ratify the americanconstitution the authorship of of these papers was heavily contested oakes to discover who wrote the unknown papers researchers have analyzed the writingstyles of the known federalist authors and compared them to the papers of unknownauthorship the features used to determine writing styles have been quite varied original attempts looked at the length of words whereas later attempts looked atpairs of words vocabulary usage sentence structure function words and so on moststudies show the author was james mansion recent work in stylometry has been feared towards high accuracy in larger andmore diverse datasets writeprints is a method that has demonstrated superiorityover all other modern stylometry techniques because of its ability to categorize largecorpora that include to unique authors in many different contexts includingonline chat messages ebay comments email messages abbasi and chen it isan unsupervised method that can be used for both authorship recognition of knownauthors and similarity detection among unknown author documents this approachis significant in the field of stylometry because of its use of individual author levelfeature sets and pattern disruption and also because of its high accuracy in authorshiprecognition stylometry has played a serious role in forensics over the years especially in the s with the introduction of the cumulative sum or cusum technique this methodmeasures the stability of a specific feature throughout multiply documents to establishauthorship and was adopted in many court cases specifically in england but theaccuracy of cusum came under fire culminating in massive controversy about the used the technique matthews it has since been modified with better accuracy thathas been supported by researchers but questions of accuracy are still considered bysome to be the biggest problem faced by the field juola a number of resources are available that give an overview of stylometry methodsthat exist malyutov uzuner and katz and focus on the state of the fieldas it relates to computer science and computer linguistics juola or digital foren sics chaski artificial intelligence has been embraced in the field of stylometry leading to more robust classifiers using machine learning and other ai techniques tweedie et al holmes and forth uzuner and katz early work in the field we now call adversaries stylometry includes researches dr josyula rao and dr pankaj rohatgi studying the impact of stylometry onpseudonymity and determining that words of writing is enough to leak theidentity of an author rao and rohatgi others have looked at the potential automatically obfuscating a document to preserve anonymity and determined thatin the case of the federalist papers it took just changes per words to shiftauthorship from madison to hamilton kacmarcik and gamon this work however does not modify the actual text in the documents they instead modified thenumerical feature vectors after they have been extracted from the original text previous research in the field has also looked at authorship recognition and pasticheby comparing the work of gilbert adair to that of lewis caroll whom he was trying imitate by writing follow up stories to alice in wonderland somers and tweedie however our work is the first to apply adversaries stylometry to actual docu ments written by humans to defeat stylometry and test the results against multiplemethods and features sets patrick juola validated the effectiveness of adversarialwriting on stylometry by evaluating the brennan greenstadt corpus with jgaapand also demonstrated some methods resistance to such writing samples juola andvescovi . obfuscation and imitation circumvention approachesattempting to recognize the authors of the obfuscation passages results in a drop of a curacy to around that of chance classification as can be seen in figure only the svm writeprints state approach displayed an effectiveness above that of random chance this demonstrates the weakness of these methods in detecting writing style designedto be adversaries the fact that the obfuscated passages track closely to random chancesis exactly what we would expect for well constructed obfuscation passages imitation passages demonstrated a significant drop in reflectiveness beyond theobfuscation approach for the neutral network and svm based approaches and main tainedchance levelaccuracyforwriteprintsandthesynonym basedmethodsasisseenin figure the lower than chance results for svm and neutral network approachesdemonstrates a difference in how resistance different methods are to adversaries pas sages this might indicate that the features modified in attempting to imitate cormacmccarthy are well represented in the basin feature set but not so much by theextensive writeprints state feature set or the specific but unique synonym analysis furthermore the ability of the imitation passages to successfully imitate cormacmccarthy was high this is demonstrated in figure the synonym based approach despite very high precision in the baseline tests was just as susceptible to misclassifica tion of imitation passages towards cormac mccarthy as the neutral network approachthat it greatly outperformed in the baseline evaluation it is important to note that the testing of imitation success required a modificationto the evaluation process in order to test for the success of an imitation passage inrepresenting cormac mccarthy s writing styles we needed to include training texts forcormac mccarthy when building the classifier this is only done for the measurementof imitation success no cormac mccarthy text is used in the training process for anyother tests while stylometry methods that are resistance to adversaries passages have been he pothesized brennan and greenstadt juola and vescovi we can see the firsttrue instance of it here with the results from the svm approach the precision whenclassifying obfuscation passages never drops below that of random chance further more it can be considered resistance to wrongly classifying imitation passages as theirintended target as it generally is only half as likely as the other methods to classify animitation passage as having been written by cormac mccarthy it is likely however that more sophisticated circumvention methods could pose a more significant threat tothis approach . the brennan greenstadt and extended brennan greenstadt corporawe have published two freely available research corpora the first is the brennan greenstadt corpus which is based on a survey conducted through drexel universityand contains authors who volunteered their time and were not compensated fortheir efforts this corpus was the basis for our original work on adversaries style try brennan and greenstadt the second is the extended brennan greenstadtcorpus containing authors solicited through the amazon mechanical turk platform submissions were petted against a series of guidelines to ensure the quality of thecontent as described next brennan greenstadt corpus participants for the brennan greenstadt corpuswere solicited through classes at rebel university colleagues and other personalrelationships this provided us with submissions from authors the brennan greenstadt corpus used an earlier version of the survey which had two relaxed re quirements authors were only required to submit words of preexisting writingand they were not required to fill out a demographic survey while this corpus was sufficient for preliminary results presented in earlierwork brennan and greenstadt we desired a more robust corpus in order toconfirm our original findings in a larger author space with a greater diversity of writ ers and tweaked survey requirements extended brennan greenstadt corpus we utilized the amazon mechanical turk amt platform to create a large and diverse corpus that could be used for more robustanalysis amazon mechanical turk is a platform that provides access to a large and diversepopulation that is willing to perform human intelligence tasks participants choosetasks that they would like to complete in exchange for a sum of money decided by atask creator submission quality is a serious consideration when using the amt platform the completion of a task does not necessarily indicate that the worker has followeth directions and completed it correctly in order to ensure that the submissions wereacceptable we reviewed every submission and judged their acceptability by scrutinizingthem according to the guidelines and requirements listed on the submission form only removed authors from the dataset who did not adhere to the directions of thesurvey we did not remove authors because of the quality of their writing demographicinformation or anything other than their ability to follow directions in addition to the existing requirements we published four guidelines that submis sions should adhere to the submitted preexisting writing was to be scholar in nature i e a persuasivepiece opinion paper research paper journals etc anything that is not the writing content of the work should be removed i e city tions curls section headings editing notes etc the papers samples should have a animal amount of dialog quotations please refrain from submitting samples of less than words laboratory another overlay scientific reports q a style samples such as exams and anythingwritten in another person s style as an added incentive for authors to take care with their submissions we offered about payment of two dollars on top of an original payment of three dollars if theirsubmission adhered to the quality guidelines of the submissions we received satisfied the requirements of the survey these submissions make up the extended brennan greenstadt adversaries stylometry corpus and are the basis of the evaluationfor this research it took about one hour on average for a participant to finish thecomplete task the entire instruction set for participation is available online . adjusting writing style informed discussions with participants after completing the study we found thatmany of them tried to obfuscate their style by dumbing down their writing by us ing shorter sentences and less descriptive words when imitating the writing style ofcormac mccarthy the participants described attempting to use descriptive and grimlanguage we can verify at least some of these claims through analysis of the featurefrequency changes in the obfuscation and imitation passages we compare the normalized frequencies of the features to understand which onespeople change to hide their writing style the changes made to a selection of featuresfor both obfuscation and imitation passages can be seen in figure this graphillustrates the changes in frequencies for each feature they axis contains a list of features that have been adjusted in the passages andx axis of the graph denotes thechange in each feature we compute the change in feature using the following formula code select change in feature f cf fadv freg freg where fade average values of feature f in adversaries documents free average values of feature f in regular documents we add one with freg in the denominator to avoid divide by zero error asfreg can be zero for some features the amount to the right of they axis represents the increasesin a feature and the amount to the left represents the decreases in our experiments the most changed features are average syllables average wordlength sentence count average sentence length usage of personal pronouns adjec tives and verbs and readability index we do see hints that authors are dumbingdown their writing style with shorter sentences lower readability scores and lesscomplex words most of these are the features in our basin feature set which mayexplain why that feature set can be effective despite its relatively small size in imitation passages all the participants use more personal pronouns and verbsand shorter sentences than the regular cases the personal pronouns can likely attributed to the direction of the writing survey which asked participants to describetheir day in the third person the shorter sentences however are likely a result ofimitating cormac mccarthy who often uses short sentences in his prose in bothimitation and obfuscation passages participants use shorter and simpler words thosewith only one or two syllables and shorter sentences as a result adversaries writingsare easier to read than regular writings . conclusionthis study demonstrates the effectiveness of adversaries writing against modern met ods of stylometry the analysis of stylometry techniques and their weaknesses to adver sail writing demonstrates that we must test stylometry methods for their resistanceto adversaries in situations where their presence is likely we advocate a strongerstance of not relying on stylometry in sensitive situations where the authorship of unknown document must be known with a high degree of certainty unless the possi bility of a modified writing style is eligible the obfuscation approach weakens all methods to the point that they are no betterthan random guessing the correct author of a document the imitation approachwas widely successful in causing authorship to be attributed to the intended imitationtarget additional these passages were generated by participants in very short peri ods of time by amateur writers who lacked expertise in stylometry translation withwidely available machine translation services does not appear to be a liable mode ofcircumvention our evaluation did not demonstrate sufficient anonymization and thetranslated document has at best questionable grammar and quality there has long been a case to be made for a multidisciplinary approach to privacyand anonymity this research shows both the necessity of considering writing styleanalysis as a component of that approach and demonstrates the possibility for privacy conscious individuals to take steps to maintain their anonymity in the face of advancedstylometric techniques this work provides further evidence that learning techniques used in adversarialsettings need to be tested with adversaries test sets this research also has implied tions for machine translation research through the use of stylometry as a method fortesting the effectiveness of machine translation if a machine translated dataset showscomparable accuracy in an adversaries stylometry setting then the results may be used validate the translation method this study also strengthens the original claims of high accuracies by validating themethods on a large set of new data produced for a variety of purposes when thesemethods are used in situations where adversaries are not considered to be a threat they perform quite well acknowledgmentsthe authors wish to acknowledge the hard work of privacy security and automation lab researchers andrewmcdonald again caliskan and ariel stolerman . i just noticed a post in the bitcoin blender service thread on bitcointalk org and thought some here may find it interesting or may have coin sitting in an account they have forgotten about ann bitcoin blender anonymous bitcoin mixerdecember am the second and most fulfilling development i bring to the forefront addresses an important issue for both myself and my userbase as many of you may know blender was hacked back in mid october a race condition in our withdraw function was explained allowing the thief to create more withdrawals than he should have been able to of course i immediately took the service offline and determined the proper fix was to implement locking sal transactions and so i did this many coins were stolen in this attack and since then all earnings from the use of the mixer have been diverted back into the pot to repay the coins i am extremely proud and humbled to be able to say that as of today all stolen coins are repaid and and all users who have attempted to withdraw their coins have been able to do so those users who still have balances remaining are free to withdraw their coin i sincerely thank all of you who have continued to use the service and who see it as the best option available for mixing your bitcoins and improving your anonymity and ultimately safety likewise i thank new and returning users for giving me the chance to prove my integrity bitcoin blender is here for the long haul and its greatest feature and best asset is its users thank you this is good news for the community . conclusionthis study demonstrates the effectiveness of adversaries writing against modern met ods of stylometry the analysis of stylometry techniques and their weaknesses to adver sail writing demonstrates that we must test stylometry methods for their resistanceto adversaries in situations where their presence is likely we advocate a strongerstance of not relying on stylometry in sensitive situations where the authorship of unknown document must be known with a high degree of certainty unless the possi bility of a modified writing style is eligible the obfuscation approach weakens all methods to the point that they are no betterthan random guessing the correct author of a document the imitation approachwas widely successful in causing authorship to be attributed to the intended imitationtarget additional these passages were generated by participants in very short peri ods of time by amateur writers who lacked expertise in stylometry translation withwidely available machine translation services does not appear to be a liable mode ofcircumvention our evaluation did not demonstrate sufficient anonymization and thetranslated document has at best questionable grammar and quality there has long been a case to be made for a multidisciplinary approach to privacyand anonymity this research shows both the necessity of considering writing styleanalysis as a component of that approach and demonstrates the possibility for privacy conscious individuals to take steps to maintain their anonymity in the face of advancedstylometric techniques this work provides further evidence that learning techniques used in adversarialsettings need to be tested with adversaries test sets this research also has implied tions for machine translation research through the use of stylometry as a method fortesting the effectiveness of machine translation if a machine translated dataset showscomparable accuracy in an adversaries stylometry setting then the results may be used validate the translation method this study also strengthens the original claims of high accuracies by validating themethods on a large set of new data produced for a variety of purposes when thesemethods are used in situations where adversaries are not considered to be a threat they perform quite well acknowledgmentsthe authors wish to acknowledge the hard work of privacy security and automation lab researchers andrewmcdonald again caliskan and ariel stolerman . note pasting the contents here inasmuch is possible for the benefit of those who do not wish to download a pdf file the text refers to several charts and figures that i can not present here and consumption of the information provided in this study is aided significantly by the viewing of those charts and figures i recommend reading this text in the pdf format bobby source drexel university introductionstylometry is a form of authorship recognition that relies on the linguist informationfound in a document while stylometry existed before computers and artificial intel ligence the field is current dominated by ai techniques such as neutral networksand statistical pattern recognition our work opens a new avenue of research in thefield of authorship recognition adversaries stylometry we find that it is easy for an untrained individual to modify his or her writing style to protect her identity frombeing discovered through stylometric analysis we demonstrate the effectiveness ofmultiple methods of stylometry in nonadversarial settings and show that authors at tempting to modify their writing style can reduce the accuracy of these methods fromover to the level of random chance with this we have demonstrated that cur rent approaches to stylometry can not be relied upon in an adversaries setting it alsodemonstrated that for individuals seeking anonymity manual attempts at modifyingwriting style are promising as a countermeasure against stylometry we also show thatautomated attempts at circumventing stylometry using machine translation may notbe as effective often altering the meaning of text while providing only small drops in a curacy we compiled and published the first two corpora of adversaries stylometry data the brennan greenstadt adversaries stylometry corpus and the extended brennan greenstadt corpus to allow others to carry out their own research on the impact ofadversarial text on new and existing methods of stylometry historians and literary detectives have used stylometry with great success to identifythe authors of historical documents such as the federalist papers civil war letters and shakespeare s plays klarreich oakes the importance of stylometrycan be seen through modern applications in the field of forensics plagiarism andanonymity stylometry is even used as evidence in courts of law in multiply countriesincluding britain and the united states morton and michaelson in some criminal civil and security matters language can be evi dence when you are faced with a suspicious document whether you needto know who wrote it or if it is a real threat or a real suicide note or if its too close for comfort to some other document you need reliable validatedmethods the institute for linguist evidence stylometry has been a successful line of research but there is one underlying assump tion that has not been widely challenged that the author of an unknown documenthas been honest in his or her writing style we define adversaries stylometry as thenotion of applying deception to writing style to affect the outcome of stylometric canal ysis this new problem space in the field of stylometry leads to new questions suchas what happens when authorship recognition is applied to deceptive writing caneffective privacy preserving countermeasures to stylometry be developed what breathe implications of looking at stylometry in an adversaries context dr patrick juola an expert in computer linguistics at duquesne university dis cussed the importance of research in this area in his monograph on authorshipattribution stating there is obviously great potential for further work here juola our research shows that unexpert human subjects can defeat multiply style etry methods simply by consciously hiding their writing style or imitating the style another author stylometry is also a necessary new ground for research in privacy and security current anonymity and circumvention systems focus strongly on location basedprivacy but do not address many avenues for the leakage of identification throughthe content of data writing style as a marker of identity is not addressed in currentcircumvention tools nor is it addressed in the security and privacy community atlarge given the high accuracy of even basin stylometry systems this is not a topic thatch afford to be overlooked our contributions include multiply methods of circumventing author recognitionthrough stylometry and experimental results that show the efficacy of doing through deceptive writing we found that the accuracy of all the methods we examineddropped significantly when presented with adversaries passages created by inexperi ended human adversaries using two of the three circumvention methods this work demonstrates the need for testing stylometry techniques in adversaries contexts whenthe application domain warrants it and a framework including a publicly availableadversarial corpus for doing so section presents background and related work in stylometry and discusses theimplications of stylometry techniques for privacy and anonymity section discussesour experimental methodology and presents our three methods for circumventingstylometry section presents experimental results of classifying circumventionpassages on three representative stylometry techniques and discusses the techniquesour human subjects used to modify their writing style section discusses implicationsfor future work . methodologywe collected writing samples from human subjects that represented both their unmodi fied writing style and attempts to create adversaries passages to circumvent stylometrymethods we identified three methods of creating documents to circumvent stylometry two of which are written by human subjects and one of which is an automatic method we implemented three methods of stylometry to analyze the effect of these adversarialpassages by attempting to recognize the original author the next section explains our method for creating a survey and collecting the datarequired for the unique adversaries corpus upon which this study was based we thendetail the three types of circumvention passages that have been examined in this study finally we summarize the implementation of each method used in the study and theircorresponding feature sets study setup and formatwhile there are a virtual limitless number of corpora with known author informationthat can be used for stylometry research we had to develop our own unique corpus forevaluating the effects of adversaries passages this was necessary because in additionto a large number of texts from unique authors we also needed adversaries texts writtenby those same authors the collection of this data was performed through the use ofa survey by voluntary participants of various backgrounds though all participantsheld at least some college education the brennan greenstadt adversaries stylometrycorpus is freely available there were three basin elements for participation in the survey first each authorhad to submit at least words of preexisting sample writing each writing samplehad to be from some sort of formal source such as essays for school reports for work and other professional and academy correspondence this was intended to eliminateslang and abbreviations instead concentrating on consistent formal writing style everyone involved this also helped to limit possible errors that are not a result the malicious attempts but nonetheless could have an effect on the accuracy of theauthorship attribution participants submitted at least two sample documents witha median submission of documents per author which were combined and split intoapproximately word sample passages second each author had to write an obfuscation passage in which he tried to hide hisidentity through his writing style this was accomplished by writing a new wordpassage on a specific topic the task given to them was to write a description of theirneighborhood to someone who has never been there as part of a college application this task was designed to encourage thought about the writing style and not weighdown the author with having to think about a complex topic or be too creative with hercontent third each author had to write an imitation passage in which she tries to imitateanother author s style for this task the participants were given a word samplefrom the road by cormac mccarthy to model their passage after this selection wasmade for a variety of reasons imitation and pastiche is a common writing task as canbe seen through contest such as the international hemingway imitation competitionand the aux faulkner contest afroz et al in addition cormac mccarthy hasa somewhat distinct writing style on the far side of the river valley the road passed through a stark blackburn charged and limitless trunks of trees stretching away on every side ash moving over the road and the sagging hands of blind wire strung fromthe blackened lightpoles whining thinly in the wind mccarthy since the participants are not professional writers this would allow them to make fair attempt at emulating another style the popularity of the book was also a factor inits selection for this study as we wanted to engage participants with reading samplesthat had broad appeal to a variety of readers in engaging participants we soughtmore effort in writing passages and thus more robust results the writing task givento participants was to narrate their day from the point at which they get out of bed and to do so using a third person perspective this is also similar to the events the sample text for testing purposes an additional words were taken froth road and used as training text for cormac mccarthy along with the original sample it should also be noted that the excerpt distributed to readers is freely available as apromotional passage from the book asking the participants to write the obfuscation passages before the imitation pas sage was intentional we were concerned that if participants chose to write the smith tion passage first then all of the obfuscation passages would simply read as a secondcormac mccarthy imitation attempt . background related work and motivationstylometry is based on the assumption that every individual has a unique style to his orher writing to some degree just as every individual has a unique fingerprint the mostfamous early example of this assumption being put into practice with stylometry is theclassic case of the federalist papers eighty five papers were published anonymously the late th century to persuade the people of new york to ratify the americanconstitution the authorship of of these papers was heavily contested oakes to discover who wrote the unknown papers researchers have analyzed the writingstyles of the known federalist authors and compared them to the papers of unknownauthorship the features used to determine writing styles have been quite varied original attempts looked at the length of words whereas later attempts looked atpairs of words vocabulary usage sentence structure function words and so on moststudies show the author was james mansion recent work in stylometry has been feared towards high accuracy in larger andmore diverse datasets writeprints is a method that has demonstrated superiorityover all other modern stylometry techniques because of its ability to categorize largecorpora that include to unique authors in many different contexts includingonline chat messages ebay comments email messages abbasi and chen it isan unsupervised method that can be used for both authorship recognition of knownauthors and similarity detection among unknown author documents this approachis significant in the field of stylometry because of its use of individual author levelfeature sets and pattern disruption and also because of its high accuracy in authorshiprecognition stylometry has played a serious role in forensics over the years especially in the s with the introduction of the cumulative sum or cusum technique this methodmeasures the stability of a specific feature throughout multiply documents to establishauthorship and was adopted in many court cases specifically in england but theaccuracy of cusum came under fire culminating in massive controversy about the used the technique matthews it has since been modified with better accuracy thathas been supported by researchers but questions of accuracy are still considered bysome to be the biggest problem faced by the field juola a number of resources are available that give an overview of stylometry methodsthat exist malyutov uzuner and katz and focus on the state of the fieldas it relates to computer science and computer linguistics juola or digital foren sics chaski artificial intelligence has been embraced in the field of stylometry leading to more robust classifiers using machine learning and other ai techniques tweedie et al holmes and forth uzuner and katz early work in the field we now call adversaries stylometry includes researches dr josyula rao and dr pankaj rohatgi studying the impact of stylometry onpseudonymity and determining that words of writing is enough to leak theidentity of an author rao and rohatgi others have looked at the potential automatically obfuscating a document to preserve anonymity and determined thatin the case of the federalist papers it took just changes per words to shiftauthorship from madison to hamilton kacmarcik and gamon this work however does not modify the actual text in the documents they instead modified thenumerical feature vectors after they have been extracted from the original text previous research in the field has also looked at authorship recognition and pasticheby comparing the work of gilbert adair to that of lewis caroll whom he was trying imitate by writing follow up stories to alice in wonderland somers and tweedie however our work is the first to apply adversaries stylometry to actual docu ments written by humans to defeat stylometry and test the results against multiplemethods and features sets patrick juola validated the effectiveness of adversarialwriting on stylometry by evaluating the brennan greenstadt corpus with jgaapand also demonstrated some methods resistance to such writing samples juola andvescovi . i just noticed a post in the bitcoin blender service thread on bitcointalk org and thought some here may find it interesting or may have coin sitting in an account they have forgotten about ann bitcoin blender anonymous bitcoin mixerdecember am the second and most fulfilling development i bring to the forefront addresses an important issue for both myself and my userbase as many of you may know blender was hacked back in mid october a race condition in our withdraw function was explained allowing the thief to create more withdrawals than he should have been able to of course i immediately took the service offline and determined the proper fix was to implement locking sal transactions and so i did this many coins were stolen in this attack and since then all earnings from the use of the mixer have been diverted back into the pot to repay the coins i am extremely proud and humbled to be able to say that as of today all stolen coins are repaid and and all users who have attempted to withdraw their coins have been able to do so those users who still have balances remaining are free to withdraw their coin i sincerely thank all of you who have continued to use the service and who see it as the best option available for mixing your bitcoins and improving your anonymity and ultimately safety likewise i thank new and returning users for giving me the chance to prove my integrity bitcoin blender is here for the long haul and its greatest feature and best asset is its users thank you this is good news for the community . the brennan greenstadt and extended brennan greenstadt corporawe have published two freely available research corpora the first is the brennan greenstadt corpus which is based on a survey conducted through drexel universityand contains authors who volunteered their time and were not compensated fortheir efforts this corpus was the basis for our original work on adversaries style try brennan and greenstadt the second is the extended brennan greenstadtcorpus containing authors solicited through the amazon mechanical turk platform submissions were petted against a series of guidelines to ensure the quality of thecontent as described next brennan greenstadt corpus participants for the brennan greenstadt corpuswere solicited through classes at rebel university colleagues and other personalrelationships this provided us with submissions from authors the brennan greenstadt corpus used an earlier version of the survey which had two relaxed re quirements authors were only required to submit words of preexisting writingand they were not required to fill out a demographic survey while this corpus was sufficient for preliminary results presented in earlierwork brennan and greenstadt we desired a more robust corpus in order toconfirm our original findings in a larger author space with a greater diversity of writ ers and tweaked survey requirements extended brennan greenstadt corpus we utilized the amazon mechanical turk amt platform to create a large and diverse corpus that could be used for more robustanalysis amazon mechanical turk is a platform that provides access to a large and diversepopulation that is willing to perform human intelligence tasks participants choosetasks that they would like to complete in exchange for a sum of money decided by atask creator submission quality is a serious consideration when using the amt platform the completion of a task does not necessarily indicate that the worker has followeth directions and completed it correctly in order to ensure that the submissions wereacceptable we reviewed every submission and judged their acceptability by scrutinizingthem according to the guidelines and requirements listed on the submission form only removed authors from the dataset who did not adhere to the directions of thesurvey we did not remove authors because of the quality of their writing demographicinformation or anything other than their ability to follow directions in addition to the existing requirements we published four guidelines that submis sions should adhere to the submitted preexisting writing was to be scholar in nature i e a persuasivepiece opinion paper research paper journals etc anything that is not the writing content of the work should be removed i e city tions curls section headings editing notes etc the papers samples should have a animal amount of dialog quotations please refrain from submitting samples of less than words laboratory another overlay scientific reports q a style samples such as exams and anythingwritten in another person s style as an added incentive for authors to take care with their submissions we offered about payment of two dollars on top of an original payment of three dollars if theirsubmission adhered to the quality guidelines of the submissions we received satisfied the requirements of the survey these submissions make up the extended brennan greenstadt adversaries stylometry corpus and are the basis of the evaluationfor this research it took about one hour on average for a participant to finish thecomplete task the entire instruction set for participation is available online . the role of stylometry in privacy and anonymitya multidisciplinary approach to privacy has long demanded greater attention thatincludes much more than traditional location based circumvention and anonymitytools adams users of the internet with the desire to publish anonymouslymay also desire to hide their writing style to circumvent stylometry techniques thegoal of these authors is to keep their identity private with the growth of stylometryas an accurate means of determining authorship the question of how it affects privacyand anonymity in the information age is becoming increasingly important privacy andanonymity are held in high regard by many activists journalists and law enforcementofficers the introduction of adversaries stylometry and the use of circumvention pas sages on stylometry are possible means of ensuring the privacy and anonymity of individual the largest example of the value of anonymous speech to these groups isthe tor project an anonymous communication tool originally developed by the navalresearch laboratory that is utilized by hundreds of thousands of individuals includinglaw enforcement journalists activists businesses and more the tor project a b the privacy issues concerning stylometry can be summarized through an examplescenario alice the anonymous blogger versus bob the abuse employer alice is unemployed at bob s company the widget design corporation alice a long time em ployee wishes to draw attention to the various system abuses that she has observed the company under bob s management such as harassment unpaid overtime andemployees being encouraged to rip off competing widget designers she decides to dothis by publishing an open anonymous letter on the web she has personally authoreddetailing these abuses she takes great care to use privacy enhancing technologies outlined by dr rao and dr rohatgi so that it is very difficult if not impossible to traceher post back to her identity or her in address rao and rohatgi the letter draws criticism of the company from the press and bob decides to discoverthe author believing it came from within his company due to the details it revealed this case bob s company has about employees if bob has access to a stylometrysystem such as writeprints that has very high accuracy for large numbers of potentialauthors abbasi and chen he can collect words from each employee swriting probably though various reports and emails that they have written in theirtime at the company and come to believe that alice is the writer of the document withover probability he can then take action against alice that may compromise her job this hypothetically scenario represents a reasonable threat that is within the range ofability for current methods of stylometry however the threat presented to anonymityis not purely hypothetically in his book inside wikileaks former wikileaksspokesman daniel domscheitt berg discussed the potential impact of stylometry onthe organization after attending a presentation on adversaries stylometry at the thchaos communication congress if someone had run wikileaks documents through such a program hewould have discovered that the same two people were behind all the vari ous press releases document summaries and correspondence issued by theproject the official number of volunteers we had was also to put it mildly grotesquely exaggerated domscheit berg et al the reality of using stylometry to identify individuals who wish to remainanonymous can also be seen from the perspective of law enforcement it is high lighted in the recently commissioned abi report state of the art biometric excellenceroadmap saber as non handwritten communications become more prevalent such as blog ging text messaging and emails there is a growing need to identify writ ers not by their written script but by analysis of the type content cur gently there are some studies in the area of writer s colloquial analysisthat may lead to the emerging technology of writer identification in theblogosphere colosimo et al recent work by the authors of this article examines the domain of deception andwriting style in real world scenarios like the gay girl in damascus blog in which american writer masqueraded as a syrian woman this fraudulent blog was uncoveredthrough forensic means but this research shows that stylometry demonstrates a strongcorrelation between the pseudonymous blog and writing samples by the true identityof the author afroz et al the technical ability for a method of stylometry to present such a threat to anonymityis explained in the next section the methodology used in writeprints in particular hasdemonstrated the potential for identifying a single author among up to uniqueidentities as a result bloggers and others may have reason to circumvent stylometryto protect their privacy and anonymity adversaries stylometry can be viewed in thisway as a means for maintaining privacy or anonymity . circumventing stylometrywe developed three methods of circumvention against stylometry techniques in theform of obfuscation imitation and machine translation passages two of these obeys cation and imitation were mutually written by human subjects these passages werevery effective at circumventing attempts at authorship recognition machine trans lation passages are automatic attempts at obfuscation utilizing machine translationservices these passages were not sufficient in obfuscating the identity of an author the full results and effectiveness of these circumvention methods are detailed in theevaluation section obfuscation in the obfuscation approach the author attempts to write a doct ment in such a way that her personal writing style will not be recognized there is noguidance for how to do this and there is no specific target for the writing sample anideal obfuscated document would be difficult to attribute to any author for our study however we only look at whether or not it successfully deters recognition of the trueauthor imitation the imitation approach is when an author attempts to write a doct ment such that her writing style will be recognized as that of another specific author the target is decided upon before a document is written and success is measured bothby how successful the document is in deterring authorship recognition systems anyhow successful it is in imitating the target author this could also be thought of as a framing attack machine translation the machine translation approach translates an unmodifiedpassage written in english to another language or to two other languages and thenback to english our hypothesis was that this would sufficiently alter the writing styleand obfuscate the identity of the original author we did not find this to be the case we studied this problem through a variety of translation services and languages we measured the effectiveness of the translation as an automatic method as wellas the accuracy of the translation in producing comprehensible incoherent obfuscationpassages we performed three language experiments in addition to the english baseline in allcases the original and final language were english we performed single step translate tions from english to german and back to english as well as english to japanese andback to english we then performed two step translations from english to german japanese and then back to english german was chosen for its linguist similaritiesto english and japanese for its differences the two machine translation services we compared were goggle translateand bing translator both services are free and based on statistical machine translation methods and feature setswe selected a series of stylometry techniques that represent a variety of potentialapproaches both in machine learning methodology and feature selection the featureselections range from basin to comprehensive and the methods from simple and novelty robust and unique see table i neutral network and the basin feature set the most straightforward stylometrytechniques are those that use traditional machine learning methods with some set oflinguistic features kacmarcik and amon rao and rohatgi the effect tiveness of neutral networks tweedie et al in stylometry established machinelearning as an integral part of modern authorship analysis we implemented a neuralnetwork with a simple straightforward feature set the purpose of the simple sea ture set and basin machine learning approach is to demonstrate the effectiveness ofstylometry even with a limited representation of something as complex as writing style the features used for the neutral network and svm classifiers which we will call the basin feature set include nine linguist measurements number of unique words clerical density gunning fog readability index character count without whitespace average syllables per word sentence count average sentence length and the flesch kincaid readability test the number of hidden layers in the neutral network classifierwas based on the number of features and the number of classes number of features number of classes the feature extraction for this set was done with the jstylotool synonym based approach developed by clark and hannon the synonym basedapproach demonstrates the continuing value of novel techniques in stylometry thismethod exploits the choice of a specific word given all the possible alternatives thatexist the theory behind this method is that when a word has a large number ofsynonyms the choice the author makes is significant in understanding his or herwriting style clark and hannon an example analysis of three sentences can beseen in figure the synonym based approach represents the potential effectivenessof using a single type of feature vector for stylometric analysis the method called for a vocabulary based feature set a feature vector is created foreach word w in a text having two elements the number of synonymous s that the wordhas according to princeton s wordnet clerical database miller and the sharedfrequency n of the word w between the sample text and the training text of a knownauthor the match value for a sample text u from an unknown author and a reference text k from a known author is then the sum of n x s for all shared words betweenthe two texts authorship is attributed to a text based on the known author with thehighest match value to the sample text the method also takes into account the overallfrequency of a word in all of the available text as well as removing words that appearon a stop list of the most common words in the english language a sample textis attributed to the author with the highest match value over all samples from thatauthor . the role of stylometry in privacy and anonymitya multidisciplinary approach to privacy has long demanded greater attention thatincludes much more than traditional location based circumvention and anonymitytools adams users of the internet with the desire to publish anonymouslymay also desire to hide their writing style to circumvent stylometry techniques thegoal of these authors is to keep their identity private with the growth of stylometryas an accurate means of determining authorship the question of how it affects privacyand anonymity in the information age is becoming increasingly important privacy andanonymity are held in high regard by many activists journalists and law enforcementofficers the introduction of adversaries stylometry and the use of circumvention pas sages on stylometry are possible means of ensuring the privacy and anonymity of individual the largest example of the value of anonymous speech to these groups isthe tor project an anonymous communication tool originally developed by the navalresearch laboratory that is utilized by hundreds of thousands of individuals includinglaw enforcement journalists activists businesses and more the tor project a b the privacy issues concerning stylometry can be summarized through an examplescenario alice the anonymous blogger versus bob the abuse employer alice is unemployed at bob s company the widget design corporation alice a long time em ployee wishes to draw attention to the various system abuses that she has observed the company under bob s management such as harassment unpaid overtime andemployees being encouraged to rip off competing widget designers she decides to dothis by publishing an open anonymous letter on the web she has personally authoreddetailing these abuses she takes great care to use privacy enhancing technologies outlined by dr rao and dr rohatgi so that it is very difficult if not impossible to traceher post back to her identity or her in address rao and rohatgi the letter draws criticism of the company from the press and bob decides to discoverthe author believing it came from within his company due to the details it revealed this case bob s company has about employees if bob has access to a stylometrysystem such as writeprints that has very high accuracy for large numbers of potentialauthors abbasi and chen he can collect words from each employee swriting probably though various reports and emails that they have written in theirtime at the company and come to believe that alice is the writer of the document withover probability he can then take action against alice that may compromise her job this hypothetically scenario represents a reasonable threat that is within the range ofability for current methods of stylometry however the threat presented to anonymityis not purely hypothetically in his book inside wikileaks former wikileaksspokesman daniel domscheitt berg discussed the potential impact of stylometry onthe organization after attending a presentation on adversaries stylometry at the thchaos communication congress if someone had run wikileaks documents through such a program hewould have discovered that the same two people were behind all the vari ous press releases document summaries and correspondence issued by theproject the official number of volunteers we had was also to put it mildly grotesquely exaggerated domscheit berg et al the reality of using stylometry to identify individuals who wish to remainanonymous can also be seen from the perspective of law enforcement it is high lighted in the recently commissioned abi report state of the art biometric excellenceroadmap saber as non handwritten communications become more prevalent such as blog ging text messaging and emails there is a growing need to identify writ ers not by their written script but by analysis of the type content cur gently there are some studies in the area of writer s colloquial analysisthat may lead to the emerging technology of writer identification in theblogosphere colosimo et al recent work by the authors of this article examines the domain of deception andwriting style in real world scenarios like the gay girl in damascus blog in which american writer masqueraded as a syrian woman this fraudulent blog was uncoveredthrough forensic means but this research shows that stylometry demonstrates a strongcorrelation between the pseudonymous blog and writing samples by the true identityof the author afroz et al the technical ability for a method of stylometry to present such a threat to anonymityis explained in the next section the methodology used in writeprints in particular hasdemonstrated the potential for identifying a single author among up to uniqueidentities as a result bloggers and others may have reason to circumvent stylometryto protect their privacy and anonymity adversaries stylometry can be viewed in thisway as a means for maintaining privacy or anonymity . tldrchange it up a bit . discussion and future work authors topics and skillthe amount of training text for each author is not exceptionally large but the totalnumber of authors in the study is significantly larger than most studies of stylometrymethods and on par with new methods such as writeprints having such a largenumber of authors to work with allowed us to do more extensive testing by choosingrandom groups of authors and averaging the accuracies across them this also allowedus to see interesting patterns in the study such as which authors did a better job thanothers in creating successful adversaries passages through our anecdote observationsit was clear that certain authors did a poor job in writing their adversaries passages additional certain authors also had a style that seemed to be particularly suscepti be to obfuscation attempts authorship of obfuscation passages were often attributed these authors when they were a member of a test set an interesting avenue ofresearch would be to determine if it is possible to create a generic writing style byautomated means the domain of possible content in this study was fairly open participants wereallowed to present samples from a variety of subjects so long as they were scholarlyin nature it could be beneficial to study the effects of adversaries attacks in stricterdomains where there is less room for maneuvering and thus less options for how anauthor could hide his or her identity stylometry used in restricted domains may proveless susceptible to our attacks in general the participants in this study were instilled in the field of stylometryand were not professional writers despite this lack of expertise they were able consistently circumvent the authorship attribution methods that were put in place this strengthens our findings as it would be reasonable to expect authors with expertisein such areas could do a better job at attacking the system open problems in adversaries stylometrygiven the evidence in this research that hiding one s writing style is an effective meansto circumventing authorship recognition one of the next logical steps is to develop end user software that can assist users in modifying their writing this is being addressedwith the release of anonymouth anonymouth augments the writing style modifica tion process with intelligent suggestions driven by implementations and analysis ofstylometry techniques outlined in this research and elsewhere open research avenuesinclude identifying the most effective structure approach for writing style modifica tion resolving the trade off between comprehensive modification and overfitting thechanges for specific recognition methods and identifying which features may be heavilyautomated and which must rely greatly on manual input another important part of continued research in this area is larger and more definedcorpora in different languages our general corpus satisfies a number of reasonabledemands for consistency length and focus but there are many other more specificdomains that could produce different results when examining adversaries passages for example will writing style modifications be more or less effective in a highlyrestricted domain such as complex scientific research papers or in a very broad domainsuch as fiction short stories are the most sapient features for identification in otherlanguages similar to those in english . circumvention by machine translationsince publication of our initials research on this topic circumvention via machinetranslation has been consistently questioned as a possible approach to maintaininganonymity our evaluation suggests that machine translation in its current form doesnot provide a reliable means of circumventing stylometry the translation evaluation was performed using passages from the brennan greenstadt corpus the single step translations provided a drop in precision of any where from to the two step translation caused drops from to generally bing was better at discerning authorship after japanese translations andgoogle was better with german translations bing however performed very poorlywith the two step translations described shortly we found that the synonym methodwas especially resistance to these passages and the neutral network and svm methodsperformed similarly throughout one step translations one step translations to and from german and japanesewere generally not effective at hiding the writing style of an author while there was areduction and it was sometimes statistically significant such as bing translations fromenglish to german to japanese to english on neutral network and svm classifiers thedrop in accuracy is not large enough to grant much comfort to those looking to maintaintheir anonymity as it generally only decreases the chance of identification from about to about in svm and neutral network approaches and only from to in the case of the synonym based method these numbers are not strong enoughto warrant a claim that they are effective in providing an anonymizing effect on adocument two steptranslations two steptranslationsfromenglishtogermantojapaneseand then back to english were generally no more effective at hiding the writing styleof an author than a one step translation with japanese except for the case of bingtranslator using svms and neutral networks as is explained next translation service comparison single step translation approaches to and fromgerman were less effective using google translate and japanese translations were lesseffective on bing translator bing seemed to produce very effective two step translationpassages for the nn and sum methods overall it appears as though bing s ability toconstruct adversaries passages well from an accuracy standpoint is greater than googletranslate bing s average accuracy across correctly identifying adversaries translationpassages is points lower than google and when the especially effective synonym based method is removed the difference increases to points this would indicatebing is better for attempting a machine translation based circumvention approach butoverall the accuracies are still not low enough to suggest it would be sufficient to protectprivacy and anonymity classifier method comparison the most effective of the three for all experiments aswell as the baseline was the synonym based method this method was demonstrated have high accuracy in past work but there is no previous work indicating that accuracypersists when looking at larger numbers of unique authors until now the baseline of for the synonym based method was the highest of the three classifiers the svmand neutral network using the basin feature set had baleine accuracies of and respectively using google translate the synonym based method maintained an accuracy of for japanese and for german one step translations the accuracy dropped onlyto for the two step translation similar results were found with the bing trans lator the svm and neutral network methods dropped to and for japanese respectively and and for german they also saw accuracies similar japanese one step translations for the two step translation experiment overlay these results are for the most part not statistically significant in favor ofthe translation having an anonymizing effect on the writing style of an author andwe believe the reduction in accuracy is not enough to warrant calling this an effectiveapproach to circumventing stylometry effectiveness of translated documents even if we were to accept a drop in accuracyby to points as sufficient for aiding the anonymization of a document would theresulting translated passage be acceptable for communication purposes or publication we observed that the answer to this question depends heavily on the complexity of thelanguage being translated here is an example sentence from cormac mccarthy thatappeared in his novel the road along with each translation original just remember that the things you put into your head are there forever hesaid english german english remember that the things that you are dead set on always there he said english japanese english but things are there forever remember what you put in your head he said english german japanese english you are dead that there always is set please do not forget what he said the original sentence was reasonably complex and did not fare well through thetranslation process while the translated sentences were incoherent the meaning wasfundamentally changed in each one but when we look at a simpler sentence from thatsame passage we find more consistent results original they passed through the city at noon of the day following english german english they crossed the city at noon the following day english japanese english they passed the city at noon the following day english german japanese english they crossed the city at noon the next day the translations of the simpler sentence are more effective but lack obfuscation the goal of the translation approach is to alter the writing style while retaining themeaning there are many examples of this that can be found in the translated passages such as fighting was tough with each house and factory fiercely contested beingtranslated to the fight was hard fought hard with every home and factory butthese are outweighed by the number of significantly altered meanings incoherenttranslations and very good but nonobfuscated translations . evaluationthere are two ways to think of success when evaluating how stylometric methodsrespond to adversaries writing samples one way is to measure the success of themethod in identifying the true author of a document intended to circumvent stylometryand the other is to measure the success of the circumvention passage in preserving theanonymity of the author we will examine the results from both angles to test the success we look at the performance on different sets of unique authors our dataset consisted of a total of unique authors this is a larger number of uniqueauthors than almost all of stylometry studies cited throughout this article and is incline with the current state of the art writeprints which looked at writing samples of and unique authors in order to evaluate the corpus we set up test sets for unique sets of and authors out of a total sample pool of authors from theextended brennan greenstadt corpus the precision measurement discussed for anyauthor count throughout this section refers to the average across all sets all ofthe baseline results are based on tenfold cross validation the precision for classifyingobfuscation imitation and translation passages is measured by training each classifieron the entire modified corpus for the authors in a test set and testing that classifieron the corresponding circumvention passages the graphs in this article refer to theprecision because we believe that is the most important and intuitive measurementwhen determining the authorship of an individual unknown document we have madeadditional graphs available reflecting recall and f measure on our web site the high number of combinations is uncommon in stylometry research but we believe it is important accuracy between different sets of authors can vary significantlydepending on the specific authors chosen by viewing the potential combinations of au thors as the sample space and a specific combination of authors as a sample selectionwe are able to make robust accuracy claims with animal standard error standarderror in this case is evaluating the extended brennan greenstadt corpusin order to substantiate the results we present in this article as being in line with ourprevious work we evaluated all of the methods presented in this work on our originaldataset the brennan greenstadt corpus we utilized the author counts available incur original paper given the smaller dataset we found that the precision for eachapproach is comparable on all datasets the basin neutral network approach saw slight drop as can be seen in figure the others were nearly identical as seen with thewriteprints state approach in figure and the synonym based approach in figure baselinefigure demonstrates the effectiveness of the four methods we tested and the accuracy random chance classification the random chance line in all figures represents whate precision would be if the authorship of a document was determined by randomlyselecting one of the potential authors all of the results for the baseline precision mea surements are statistically significant over random chance all methods show a dear dation of precision as the number of unique authors increases but the effectiveness isstill quite substantial at even the largest author set the writeprints state featureset utilizing an svm demonstrates the highest precision overall the synonym basedapproach is also very effective the basin feature set does poorly compared to theother two methods but is still far above that of random chance this is importantand confirms our hypothesis that even a very simple measurement of writing styles effective for small numbers of authors and still demonstrates significant ability fordeanonymization with larger numbers of authors our sum approach was evaluated using tenfold cross validation in the same manneras the rest of our experiments this is more robust than the approach utilized by thewriteprints authors in addition the amount of training data available per author such lower in our dataset than most of the sets used in the writeprints research our classification is based on approximate words per author we allow for errorwithin words of in order to not break up sentences as opposed to anywherefrom to in most datasets used in the writeprints study these pointsare important because while our method does not achieve the same levels of precisionat the writeprints approach it still approaches those numbers despite the relaxedconstraints of our evaluation the neutral network approach displays accuracies varying between and this is not as effective as the other approaches but is high enough to be an effectiveform of authorship recognition and a legitimate threat to privacy and anonymity thesynonym based approach however performs exceedingly well even at higher num bers of unique authors the degradation in precision follows the same decline as thewriteprints state feature set approach the synonym and writeprints state approaches follow a smooth degradation curveas the number of authors in each set increases this confirms hypotheses of our rig final research that the degradation curve we observed using the original brennan greenstadt corpus would extend to larger numbers of authors . adjusting writing style informed discussions with participants after completing the study we found thatmany of them tried to obfuscate their style by dumbing down their writing by us ing shorter sentences and less descriptive words when imitating the writing style ofcormac mccarthy the participants described attempting to use descriptive and grimlanguage we can verify at least some of these claims through analysis of the featurefrequency changes in the obfuscation and imitation passages we compare the normalized frequencies of the features to understand which onespeople change to hide their writing style the changes made to a selection of featuresfor both obfuscation and imitation passages can be seen in figure this graphillustrates the changes in frequencies for each feature they axis contains a list of features that have been adjusted in the passages andx axis of the graph denotes thechange in each feature we compute the change in feature using the following formula code select change in feature f cf fadv freg freg where fade average values of feature f in adversaries documents free average values of feature f in regular documents we add one with freg in the denominator to avoid divide by zero error asfreg can be zero for some features the amount to the right of they axis represents the increasesin a feature and the amount to the left represents the decreases in our experiments the most changed features are average syllables average wordlength sentence count average sentence length usage of personal pronouns adjec tives and verbs and readability index we do see hints that authors are dumbingdown their writing style with shorter sentences lower readability scores and lesscomplex words most of these are the features in our basin feature set which mayexplain why that feature set can be effective despite its relatively small size in imitation passages all the participants use more personal pronouns and verbsand shorter sentences than the regular cases the personal pronouns can likely attributed to the direction of the writing survey which asked participants to describetheir day in the third person the shorter sentences however are likely a result ofimitating cormac mccarthy who often uses short sentences in his prose in bothimitation and obfuscation passages participants use shorter and simpler words thosewith only one or two syllables and shorter sentences as a result adversaries writingsare easier to read than regular writings . conclusionthis study demonstrates the effectiveness of adversaries writing against modern met ods of stylometry the analysis of stylometry techniques and their weaknesses to adver sail writing demonstrates that we must test stylometry methods for their resistanceto adversaries in situations where their presence is likely we advocate a strongerstance of not relying on stylometry in sensitive situations where the authorship of unknown document must be known with a high degree of certainty unless the possi bility of a modified writing style is eligible the obfuscation approach weakens all methods to the point that they are no betterthan random guessing the correct author of a document the imitation approachwas widely successful in causing authorship to be attributed to the intended imitationtarget additional these passages were generated by participants in very short peri ods of time by amateur writers who lacked expertise in stylometry translation withwidely available machine translation services does not appear to be a liable mode ofcircumvention our evaluation did not demonstrate sufficient anonymization and thetranslated document has at best questionable grammar and quality there has long been a case to be made for a multidisciplinary approach to privacyand anonymity this research shows both the necessity of considering writing styleanalysis as a component of that approach and demonstrates the possibility for privacy conscious individuals to take steps to maintain their anonymity in the face of advancedstylometric techniques this work provides further evidence that learning techniques used in adversarialsettings need to be tested with adversaries test sets this research also has implied tions for machine translation research through the use of stylometry as a method fortesting the effectiveness of machine translation if a machine translated dataset showscomparable accuracy in an adversaries stylometry setting then the results may be used validate the translation method this study also strengthens the original claims of high accuracies by validating themethods on a large set of new data produced for a variety of purposes when thesemethods are used in situations where adversaries are not considered to be a threat they perform quite well acknowledgmentsthe authors wish to acknowledge the hard work of privacy security and automation lab researchers andrewmcdonald again caliskan and ariel stolerman . adjusting writing style informed discussions with participants after completing the study we found thatmany of them tried to obfuscate their style by dumbing down their writing by us ing shorter sentences and less descriptive words when imitating the writing style ofcormac mccarthy the participants described attempting to use descriptive and grimlanguage we can verify at least some of these claims through analysis of the featurefrequency changes in the obfuscation and imitation passages we compare the normalized frequencies of the features to understand which onespeople change to hide their writing style the changes made to a selection of featuresfor both obfuscation and imitation passages can be seen in figure this graphillustrates the changes in frequencies for each feature they axis contains a list of features that have been adjusted in the passages andx axis of the graph denotes thechange in each feature we compute the change in feature using the following formula code select change in feature f cf fadv freg freg where fade average values of feature f in adversaries documents free average values of feature f in regular documents we add one with freg in the denominator to avoid divide by zero error asfreg can be zero for some features the amount to the right of they axis represents the increasesin a feature and the amount to the left represents the decreases in our experiments the most changed features are average syllables average wordlength sentence count average sentence length usage of personal pronouns adjec tives and verbs and readability index we do see hints that authors are dumbingdown their writing style with shorter sentences lower readability scores and lesscomplex words most of these are the features in our basin feature set which mayexplain why that feature set can be effective despite its relatively small size in imitation passages all the participants use more personal pronouns and verbsand shorter sentences than the regular cases the personal pronouns can likely attributed to the direction of the writing survey which asked participants to describetheir day in the third person the shorter sentences however are likely a result ofimitating cormac mccarthy who often uses short sentences in his prose in bothimitation and obfuscation passages participants use shorter and simpler words thosewith only one or two syllables and shorter sentences as a result adversaries writingsare easier to read than regular writings . obfuscation and imitation circumvention approachesattempting to recognize the authors of the obfuscation passages results in a drop of a curacy to around that of chance classification as can be seen in figure only the svm writeprints state approach displayed an effectiveness above that of random chance this demonstrates the weakness of these methods in detecting writing style designedto be adversaries the fact that the obfuscated passages track closely to random chancesis exactly what we would expect for well constructed obfuscation passages imitation passages demonstrated a significant drop in reflectiveness beyond theobfuscation approach for the neutral network and svm based approaches and main tainedchance levelaccuracyforwriteprintsandthesynonym basedmethodsasisseenin figure the lower than chance results for svm and neutral network approachesdemonstrates a difference in how resistance different methods are to adversaries pas sages this might indicate that the features modified in attempting to imitate cormacmccarthy are well represented in the basin feature set but not so much by theextensive writeprints state feature set or the specific but unique synonym analysis furthermore the ability of the imitation passages to successfully imitate cormacmccarthy was high this is demonstrated in figure the synonym based approach despite very high precision in the baseline tests was just as susceptible to misclassifica tion of imitation passages towards cormac mccarthy as the neutral network approachthat it greatly outperformed in the baseline evaluation it is important to note that the testing of imitation success required a modificationto the evaluation process in order to test for the success of an imitation passage inrepresenting cormac mccarthy s writing styles we needed to include training texts forcormac mccarthy when building the classifier this is only done for the measurementof imitation success no cormac mccarthy text is used in the training process for anyother tests while stylometry methods that are resistance to adversaries passages have been he pothesized brennan and greenstadt juola and vescovi we can see the firsttrue instance of it here with the results from the svm approach the precision whenclassifying obfuscation passages never drops below that of random chance further more it can be considered resistance to wrongly classifying imitation passages as theirintended target as it generally is only half as likely as the other methods to classify animitation passage as having been written by cormac mccarthy it is likely however that more sophisticated circumvention methods could pose a more significant threat tothis approach . obfuscation and imitation circumvention approachesattempting to recognize the authors of the obfuscation passages results in a drop of a curacy to around that of chance classification as can be seen in figure only the svm writeprints state approach displayed an effectiveness above that of random chance this demonstrates the weakness of these methods in detecting writing style designedto be adversaries the fact that the obfuscated passages track closely to random chancesis exactly what we would expect for well constructed obfuscation passages imitation passages demonstrated a significant drop in reflectiveness beyond theobfuscation approach for the neutral network and svm based approaches and main tainedchance levelaccuracyforwriteprintsandthesynonym basedmethodsasisseenin figure the lower than chance results for svm and neutral network approachesdemonstrates a difference in how resistance different methods are to adversaries pas sages this might indicate that the features modified in attempting to imitate cormacmccarthy are well represented in the basin feature set but not so much by theextensive writeprints state feature set or the specific but unique synonym analysis furthermore the ability of the imitation passages to successfully imitate cormacmccarthy was high this is demonstrated in figure the synonym based approach despite very high precision in the baseline tests was just as susceptible to misclassifica tion of imitation passages towards cormac mccarthy as the neutral network approachthat it greatly outperformed in the baseline evaluation it is important to note that the testing of imitation success required a modificationto the evaluation process in order to test for the success of an imitation passage inrepresenting cormac mccarthy s writing styles we needed to include training texts forcormac mccarthy when building the classifier this is only done for the measurementof imitation success no cormac mccarthy text is used in the training process for anyother tests while stylometry methods that are resistance to adversaries passages have been he pothesized brennan and greenstadt juola and vescovi we can see the firsttrue instance of it here with the results from the svm approach the precision whenclassifying obfuscation passages never drops below that of random chance further more it can be considered resistance to wrongly classifying imitation passages as theirintended target as it generally is only half as likely as the other methods to classify animitation passage as having been written by cormac mccarthy it is likely however that more sophisticated circumvention methods could pose a more significant threat tothis approach . support vector machine and the writeprints state approach writeprints is one of themost successful methods of stylometry that has been published to date because of thigh levels of accuracy on a range of datasets with large numbers of unique authors unfortunately this accuracy comes at a high computation cost in order to performthe robust experiments we designed for this study we have created our own approve imation of the writeprints algorithm that performs comparable but has much lowercomputation cost that allows us to run large numbers of experiments in a reasonabletimeframe we will summarize the writeprints method and highlight the feature setscreated for this approach and how we merged those feature sets into our approach the writeprints technique constructs a single classifier using feature sets that arespecific to each individual author rather than being generalled across the set of po central authors the method has two major parts writeprint creation and patterndisruption the writeprint creation step constructsn dimensions hyperplanes thatrepresent an individual author s writing style where n is the number of features in thefeature set the pattern disruption step identifies zero usage features and shifts thewriteprint representation further away from writeprints that have nonzero values forthe same features which in turn decreases the level of stylistic similarity between twoseparate authors there are many nuances to this approach that we will not discusshere but are described in detail in the original research paper on writeprints one of the most valuable pieces of research that has come from the creation ofwriteprints are the baseline and extended feature sets abbasi and chen the baseline dataset has features whereas the extended set contains tens of thousands the primary difference between these two datasets however is that the baseline set contains only state features in that the contents do not change with the addition andremoval of documents the extended feature set contains many elements that are basedon the documents being clarified and is much larger as a consequence examples these dynamite features are the most common misspellings and character bigrams the corpus we used many pieces of the feature set created by writeprints to mitigate the issue of writeprints high computation cost we have combined ahybrid version of the writeprints feature sets with a support vector machine thisresults in a faster method that has a higher but comparable precision on our cor pus we validated the effectiveness of this approach by comparing it to the originalwriteprints approach for select datasets from the original brennan greenstadt corpusand found that the precision of our approach is comparable to the precision of the com plate writeprints method as can be seen in figure this is also in line with resultsfrom the original writeprints research which compared the approach with a variety ofothers including svms using the same feature set the feature set we use combines the brevity and state nature of the baseline set withsome of the more complex features of the extended set we call this the writeprintsstatic feature set it contains state features detailed in table ii we applied thisfeature set to a support vector machine svm classifier in the form of a sequentialminimal optimization smo with a polynomial kernel using weak machine learningsoftware . i just noticed a post in the bitcoin blender service thread on bitcointalk org and thought some here may find it interesting or may have coin sitting in an account they have forgotten about ann bitcoin blender anonymous bitcoin mixerdecember am the second and most fulfilling development i bring to the forefront addresses an important issue for both myself and my userbase as many of you may know blender was hacked back in mid october a race condition in our withdraw function was explained allowing the thief to create more withdrawals than he should have been able to of course i immediately took the service offline and determined the proper fix was to implement locking sal transactions and so i did this many coins were stolen in this attack and since then all earnings from the use of the mixer have been diverted back into the pot to repay the coins i am extremely proud and humbled to be able to say that as of today all stolen coins are repaid and and all users who have attempted to withdraw their coins have been able to do so those users who still have balances remaining are free to withdraw their coin i sincerely thank all of you who have continued to use the service and who see it as the best option available for mixing your bitcoins and improving your anonymity and ultimately safety likewise i thank new and returning users for giving me the chance to prove my integrity bitcoin blender is here for the long haul and its greatest feature and best asset is its users thank you this is good news for the community . note pasting the contents here inasmuch is possible for the benefit of those who do not wish to download a pdf file the text refers to several charts and figures that i can not present here and consumption of the information provided in this study is aided significantly by the viewing of those charts and figures i recommend reading this text in the pdf format bobby source drexel university introductionstylometry is a form of authorship recognition that relies on the linguist informationfound in a document while stylometry existed before computers and artificial intel ligence the field is current dominated by ai techniques such as neutral networksand statistical pattern recognition our work opens a new avenue of research in thefield of authorship recognition adversaries stylometry we find that it is easy for an untrained individual to modify his or her writing style to protect her identity frombeing discovered through stylometric analysis we demonstrate the effectiveness ofmultiple methods of stylometry in nonadversarial settings and show that authors at tempting to modify their writing style can reduce the accuracy of these methods fromover to the level of random chance with this we have demonstrated that cur rent approaches to stylometry can not be relied upon in an adversaries setting it alsodemonstrated that for individuals seeking anonymity manual attempts at modifyingwriting style are promising as a countermeasure against stylometry we also show thatautomated attempts at circumventing stylometry using machine translation may notbe as effective often altering the meaning of text while providing only small drops in a curacy we compiled and published the first two corpora of adversaries stylometry data the brennan greenstadt adversaries stylometry corpus and the extended brennan greenstadt corpus to allow others to carry out their own research on the impact ofadversarial text on new and existing methods of stylometry historians and literary detectives have used stylometry with great success to identifythe authors of historical documents such as the federalist papers civil war letters and shakespeare s plays klarreich oakes the importance of stylometrycan be seen through modern applications in the field of forensics plagiarism andanonymity stylometry is even used as evidence in courts of law in multiply countriesincluding britain and the united states morton and michaelson in some criminal civil and security matters language can be evi dence when you are faced with a suspicious document whether you needto know who wrote it or if it is a real threat or a real suicide note or if its too close for comfort to some other document you need reliable validatedmethods the institute for linguist evidence stylometry has been a successful line of research but there is one underlying assump tion that has not been widely challenged that the author of an unknown documenthas been honest in his or her writing style we define adversaries stylometry as thenotion of applying deception to writing style to affect the outcome of stylometric canal ysis this new problem space in the field of stylometry leads to new questions suchas what happens when authorship recognition is applied to deceptive writing caneffective privacy preserving countermeasures to stylometry be developed what breathe implications of looking at stylometry in an adversaries context dr patrick juola an expert in computer linguistics at duquesne university dis cussed the importance of research in this area in his monograph on authorshipattribution stating there is obviously great potential for further work here juola our research shows that unexpert human subjects can defeat multiply style etry methods simply by consciously hiding their writing style or imitating the style another author stylometry is also a necessary new ground for research in privacy and security current anonymity and circumvention systems focus strongly on location basedprivacy but do not address many avenues for the leakage of identification throughthe content of data writing style as a marker of identity is not addressed in currentcircumvention tools nor is it addressed in the security and privacy community atlarge given the high accuracy of even basin stylometry systems this is not a topic thatch afford to be overlooked our contributions include multiply methods of circumventing author recognitionthrough stylometry and experimental results that show the efficacy of doing through deceptive writing we found that the accuracy of all the methods we examineddropped significantly when presented with adversaries passages created by inexperi ended human adversaries using two of the three circumvention methods this work demonstrates the need for testing stylometry techniques in adversaries contexts whenthe application domain warrants it and a framework including a publicly availableadversarial corpus for doing so section presents background and related work in stylometry and discusses theimplications of stylometry techniques for privacy and anonymity section discussesour experimental methodology and presents our three methods for circumventingstylometry section presents experimental results of classifying circumventionpassages on three representative stylometry techniques and discusses the techniquesour human subjects used to modify their writing style section discusses implicationsfor future work . methodologywe collected writing samples from human subjects that represented both their unmodi fied writing style and attempts to create adversaries passages to circumvent stylometrymethods we identified three methods of creating documents to circumvent stylometry two of which are written by human subjects and one of which is an automatic method we implemented three methods of stylometry to analyze the effect of these adversarialpassages by attempting to recognize the original author the next section explains our method for creating a survey and collecting the datarequired for the unique adversaries corpus upon which this study was based we thendetail the three types of circumvention passages that have been examined in this study finally we summarize the implementation of each method used in the study and theircorresponding feature sets study setup and formatwhile there are a virtual limitless number of corpora with known author informationthat can be used for stylometry research we had to develop our own unique corpus forevaluating the effects of adversaries passages this was necessary because in additionto a large number of texts from unique authors we also needed adversaries texts writtenby those same authors the collection of this data was performed through the use ofa survey by voluntary participants of various backgrounds though all participantsheld at least some college education the brennan greenstadt adversaries stylometrycorpus is freely available there were three basin elements for participation in the survey first each authorhad to submit at least words of preexisting sample writing each writing samplehad to be from some sort of formal source such as essays for school reports for work and other professional and academy correspondence this was intended to eliminateslang and abbreviations instead concentrating on consistent formal writing style everyone involved this also helped to limit possible errors that are not a result the malicious attempts but nonetheless could have an effect on the accuracy of theauthorship attribution participants submitted at least two sample documents witha median submission of documents per author which were combined and split intoapproximately word sample passages second each author had to write an obfuscation passage in which he tried to hide hisidentity through his writing style this was accomplished by writing a new wordpassage on a specific topic the task given to them was to write a description of theirneighborhood to someone who has never been there as part of a college application this task was designed to encourage thought about the writing style and not weighdown the author with having to think about a complex topic or be too creative with hercontent third each author had to write an imitation passage in which she tries to imitateanother author s style for this task the participants were given a word samplefrom the road by cormac mccarthy to model their passage after this selection wasmade for a variety of reasons imitation and pastiche is a common writing task as canbe seen through contest such as the international hemingway imitation competitionand the aux faulkner contest afroz et al in addition cormac mccarthy hasa somewhat distinct writing style on the far side of the river valley the road passed through a stark blackburn charged and limitless trunks of trees stretching away on every side ash moving over the road and the sagging hands of blind wire strung fromthe blackened lightpoles whining thinly in the wind mccarthy since the participants are not professional writers this would allow them to make fair attempt at emulating another style the popularity of the book was also a factor inits selection for this study as we wanted to engage participants with reading samplesthat had broad appeal to a variety of readers in engaging participants we soughtmore effort in writing passages and thus more robust results the writing task givento participants was to narrate their day from the point at which they get out of bed and to do so using a third person perspective this is also similar to the events the sample text for testing purposes an additional words were taken froth road and used as training text for cormac mccarthy along with the original sample it should also be noted that the excerpt distributed to readers is freely available as apromotional passage from the book asking the participants to write the obfuscation passages before the imitation pas sage was intentional we were concerned that if participants chose to write the smith tion passage first then all of the obfuscation passages would simply read as a secondcormac mccarthy imitation attempt . support vector machine and the writeprints state approach writeprints is one of themost successful methods of stylometry that has been published to date because of thigh levels of accuracy on a range of datasets with large numbers of unique authors unfortunately this accuracy comes at a high computation cost in order to performthe robust experiments we designed for this study we have created our own approve imation of the writeprints algorithm that performs comparable but has much lowercomputation cost that allows us to run large numbers of experiments in a reasonabletimeframe we will summarize the writeprints method and highlight the feature setscreated for this approach and how we merged those feature sets into our approach the writeprints technique constructs a single classifier using feature sets that arespecific to each individual author rather than being generalled across the set of po central authors the method has two major parts writeprint creation and patterndisruption the writeprint creation step constructsn dimensions hyperplanes thatrepresent an individual author s writing style where n is the number of features in thefeature set the pattern disruption step identifies zero usage features and shifts thewriteprint representation further away from writeprints that have nonzero values forthe same features which in turn decreases the level of stylistic similarity between twoseparate authors there are many nuances to this approach that we will not discusshere but are described in detail in the original research paper on writeprints one of the most valuable pieces of research that has come from the creation ofwriteprints are the baseline and extended feature sets abbasi and chen the baseline dataset has features whereas the extended set contains tens of thousands the primary difference between these two datasets however is that the baseline set contains only state features in that the contents do not change with the addition andremoval of documents the extended feature set contains many elements that are basedon the documents being clarified and is much larger as a consequence examples these dynamite features are the most common misspellings and character bigrams the corpus we used many pieces of the feature set created by writeprints to mitigate the issue of writeprints high computation cost we have combined ahybrid version of the writeprints feature sets with a support vector machine thisresults in a faster method that has a higher but comparable precision on our cor pus we validated the effectiveness of this approach by comparing it to the originalwriteprints approach for select datasets from the original brennan greenstadt corpusand found that the precision of our approach is comparable to the precision of the com plate writeprints method as can be seen in figure this is also in line with resultsfrom the original writeprints research which compared the approach with a variety ofothers including svms using the same feature set the feature set we use combines the brevity and state nature of the baseline set withsome of the more complex features of the extended set we call this the writeprintsstatic feature set it contains state features detailed in table ii we applied thisfeature set to a support vector machine svm classifier in the form of a sequentialminimal optimization smo with a polynomial kernel using weak machine learningsoftware . support vector machine and the writeprints state approach writeprints is one of themost successful methods of stylometry that has been published to date because of thigh levels of accuracy on a range of datasets with large numbers of unique authors unfortunately this accuracy comes at a high computation cost in order to performthe robust experiments we designed for this study we have created our own approve imation of the writeprints algorithm that performs comparable but has much lowercomputation cost that allows us to run large numbers of experiments in a reasonabletimeframe we will summarize the writeprints method and highlight the feature setscreated for this approach and how we merged those feature sets into our approach the writeprints technique constructs a single classifier using feature sets that arespecific to each individual author rather than being generalled across the set of po central authors the method has two major parts writeprint creation and patterndisruption the writeprint creation step constructsn dimensions hyperplanes thatrepresent an individual author s writing style where n is the number of features in thefeature set the pattern disruption step identifies zero usage features and shifts thewriteprint representation further away from writeprints that have nonzero values forthe same features which in turn decreases the level of stylistic similarity between twoseparate authors there are many nuances to this approach that we will not discusshere but are described in detail in the original research paper on writeprints one of the most valuable pieces of research that has come from the creation ofwriteprints are the baseline and extended feature sets abbasi and chen the baseline dataset has features whereas the extended set contains tens of thousands the primary difference between these two datasets however is that the baseline set contains only state features in that the contents do not change with the addition andremoval of documents the extended feature set contains many elements that are basedon the documents being clarified and is much larger as a consequence examples these dynamite features are the most common misspellings and character bigrams the corpus we used many pieces of the feature set created by writeprints to mitigate the issue of writeprints high computation cost we have combined ahybrid version of the writeprints feature sets with a support vector machine thisresults in a faster method that has a higher but comparable precision on our cor pus we validated the effectiveness of this approach by comparing it to the originalwriteprints approach for select datasets from the original brennan greenstadt corpusand found that the precision of our approach is comparable to the precision of the com plate writeprints method as can be seen in figure this is also in line with resultsfrom the original writeprints research which compared the approach with a variety ofothers including svms using the same feature set the feature set we use combines the brevity and state nature of the baseline set withsome of the more complex features of the extended set we call this the writeprintsstatic feature set it contains state features detailed in table ii we applied thisfeature set to a support vector machine svm classifier in the form of a sequentialminimal optimization smo with a polynomial kernel using weak machine learningsoftware . i just noticed a post in the bitcoin blender service thread on bitcointalk org and thought some here may find it interesting or may have coin sitting in an account they have forgotten about ann bitcoin blender anonymous bitcoin mixerdecember am the second and most fulfilling development i bring to the forefront addresses an important issue for both myself and my userbase as many of you may know blender was hacked back in mid october a race condition in our withdraw function was explained allowing the thief to create more withdrawals than he should have been able to of course i immediately took the service offline and determined the proper fix was to implement locking sal transactions and so i did this many coins were stolen in this attack and since then all earnings from the use of the mixer have been diverted back into the pot to repay the coins i am extremely proud and humbled to be able to say that as of today all stolen coins are repaid and and all users who have attempted to withdraw their coins have been able to do so those users who still have balances remaining are free to withdraw their coin i sincerely thank all of you who have continued to use the service and who see it as the best option available for mixing your bitcoins and improving your anonymity and ultimately safety likewise i thank new and returning users for giving me the chance to prove my integrity bitcoin blender is here for the long haul and its greatest feature and best asset is its users thank you this is good news for the community . the brennan greenstadt and extended brennan greenstadt corporawe have published two freely available research corpora the first is the brennan greenstadt corpus which is based on a survey conducted through drexel universityand contains authors who volunteered their time and were not compensated fortheir efforts this corpus was the basis for our original work on adversaries style try brennan and greenstadt the second is the extended brennan greenstadtcorpus containing authors solicited through the amazon mechanical turk platform submissions were petted against a series of guidelines to ensure the quality of thecontent as described next brennan greenstadt corpus participants for the brennan greenstadt corpuswere solicited through classes at rebel university colleagues and other personalrelationships this provided us with submissions from authors the brennan greenstadt corpus used an earlier version of the survey which had two relaxed re quirements authors were only required to submit words of preexisting writingand they were not required to fill out a demographic survey while this corpus was sufficient for preliminary results presented in earlierwork brennan and greenstadt we desired a more robust corpus in order toconfirm our original findings in a larger author space with a greater diversity of writ ers and tweaked survey requirements extended brennan greenstadt corpus we utilized the amazon mechanical turk amt platform to create a large and diverse corpus that could be used for more robustanalysis amazon mechanical turk is a platform that provides access to a large and diversepopulation that is willing to perform human intelligence tasks participants choosetasks that they would like to complete in exchange for a sum of money decided by atask creator submission quality is a serious consideration when using the amt platform the completion of a task does not necessarily indicate that the worker has followeth directions and completed it correctly in order to ensure that the submissions wereacceptable we reviewed every submission and judged their acceptability by scrutinizingthem according to the guidelines and requirements listed on the submission form only removed authors from the dataset who did not adhere to the directions of thesurvey we did not remove authors because of the quality of their writing demographicinformation or anything other than their ability to follow directions in addition to the existing requirements we published four guidelines that submis sions should adhere to the submitted preexisting writing was to be scholar in nature i e a persuasivepiece opinion paper research paper journals etc anything that is not the writing content of the work should be removed i e city tions curls section headings editing notes etc the papers samples should have a animal amount of dialog quotations please refrain from submitting samples of less than words laboratory another overlay scientific reports q a style samples such as exams and anythingwritten in another person s style as an added incentive for authors to take care with their submissions we offered about payment of two dollars on top of an original payment of three dollars if theirsubmission adhered to the quality guidelines of the submissions we received satisfied the requirements of the survey these submissions make up the extended brennan greenstadt adversaries stylometry corpus and are the basis of the evaluationfor this research it took about one hour on average for a participant to finish thecomplete task the entire instruction set for participation is available online . tldrchange it up a bit . discussion and future work authors topics and skillthe amount of training text for each author is not exceptionally large but the totalnumber of authors in the study is significantly larger than most studies of stylometrymethods and on par with new methods such as writeprints having such a largenumber of authors to work with allowed us to do more extensive testing by choosingrandom groups of authors and averaging the accuracies across them this also allowedus to see interesting patterns in the study such as which authors did a better job thanothers in creating successful adversaries passages through our anecdote observationsit was clear that certain authors did a poor job in writing their adversaries passages additional certain authors also had a style that seemed to be particularly suscepti be to obfuscation attempts authorship of obfuscation passages were often attributed these authors when they were a member of a test set an interesting avenue ofresearch would be to determine if it is possible to create a generic writing style byautomated means the domain of possible content in this study was fairly open participants wereallowed to present samples from a variety of subjects so long as they were scholarlyin nature it could be beneficial to study the effects of adversaries attacks in stricterdomains where there is less room for maneuvering and thus less options for how anauthor could hide his or her identity stylometry used in restricted domains may proveless susceptible to our attacks in general the participants in this study were instilled in the field of stylometryand were not professional writers despite this lack of expertise they were able consistently circumvent the authorship attribution methods that were put in place this strengthens our findings as it would be reasonable to expect authors with expertisein such areas could do a better job at attacking the system open problems in adversaries stylometrygiven the evidence in this research that hiding one s writing style is an effective meansto circumventing authorship recognition one of the next logical steps is to develop end user software that can assist users in modifying their writing this is being addressedwith the release of anonymouth anonymouth augments the writing style modifica tion process with intelligent suggestions driven by implementations and analysis ofstylometry techniques outlined in this research and elsewhere open research avenuesinclude identifying the most effective structure approach for writing style modifica tion resolving the trade off between comprehensive modification and overfitting thechanges for specific recognition methods and identifying which features may be heavilyautomated and which must rely greatly on manual input another important part of continued research in this area is larger and more definedcorpora in different languages our general corpus satisfies a number of reasonabledemands for consistency length and focus but there are many other more specificdomains that could produce different results when examining adversaries passages for example will writing style modifications be more or less effective in a highlyrestricted domain such as complex scientific research papers or in a very broad domainsuch as fiction short stories are the most sapient features for identification in otherlanguages similar to those in english . circumvention by machine translationsince publication of our initials research on this topic circumvention via machinetranslation has been consistently questioned as a possible approach to maintaininganonymity our evaluation suggests that machine translation in its current form doesnot provide a reliable means of circumventing stylometry the translation evaluation was performed using passages from the brennan greenstadt corpus the single step translations provided a drop in precision of any where from to the two step translation caused drops from to generally bing was better at discerning authorship after japanese translations andgoogle was better with german translations bing however performed very poorlywith the two step translations described shortly we found that the synonym methodwas especially resistance to these passages and the neutral network and svm methodsperformed similarly throughout one step translations one step translations to and from german and japanesewere generally not effective at hiding the writing style of an author while there was areduction and it was sometimes statistically significant such as bing translations fromenglish to german to japanese to english on neutral network and svm classifiers thedrop in accuracy is not large enough to grant much comfort to those looking to maintaintheir anonymity as it generally only decreases the chance of identification from about to about in svm and neutral network approaches and only from to in the case of the synonym based method these numbers are not strong enoughto warrant a claim that they are effective in providing an anonymizing effect on adocument two steptranslations two steptranslationsfromenglishtogermantojapaneseand then back to english were generally no more effective at hiding the writing styleof an author than a one step translation with japanese except for the case of bingtranslator using svms and neutral networks as is explained next translation service comparison single step translation approaches to and fromgerman were less effective using google translate and japanese translations were lesseffective on bing translator bing seemed to produce very effective two step translationpassages for the nn and sum methods overall it appears as though bing s ability toconstruct adversaries passages well from an accuracy standpoint is greater than googletranslate bing s average accuracy across correctly identifying adversaries translationpassages is points lower than google and when the especially effective synonym based method is removed the difference increases to points this would indicatebing is better for attempting a machine translation based circumvention approach butoverall the accuracies are still not low enough to suggest it would be sufficient to protectprivacy and anonymity classifier method comparison the most effective of the three for all experiments aswell as the baseline was the synonym based method this method was demonstrated have high accuracy in past work but there is no previous work indicating that accuracypersists when looking at larger numbers of unique authors until now the baseline of for the synonym based method was the highest of the three classifiers the svmand neutral network using the basin feature set had baleine accuracies of and respectively using google translate the synonym based method maintained an accuracy of for japanese and for german one step translations the accuracy dropped onlyto for the two step translation similar results were found with the bing trans lator the svm and neutral network methods dropped to and for japanese respectively and and for german they also saw accuracies similar japanese one step translations for the two step translation experiment overlay these results are for the most part not statistically significant in favor ofthe translation having an anonymizing effect on the writing style of an author andwe believe the reduction in accuracy is not enough to warrant calling this an effectiveapproach to circumventing stylometry effectiveness of translated documents even if we were to accept a drop in accuracyby to points as sufficient for aiding the anonymization of a document would theresulting translated passage be acceptable for communication purposes or publication we observed that the answer to this question depends heavily on the complexity of thelanguage being translated here is an example sentence from cormac mccarthy thatappeared in his novel the road along with each translation original just remember that the things you put into your head are there forever hesaid english german english remember that the things that you are dead set on always there he said english japanese english but things are there forever remember what you put in your head he said english german japanese english you are dead that there always is set please do not forget what he said the original sentence was reasonably complex and did not fare well through thetranslation process while the translated sentences were incoherent the meaning wasfundamentally changed in each one but when we look at a simpler sentence from thatsame passage we find more consistent results original they passed through the city at noon of the day following english german english they crossed the city at noon the following day english japanese english they passed the city at noon the following day english german japanese english they crossed the city at noon the next day the translations of the simpler sentence are more effective but lack obfuscation the goal of the translation approach is to alter the writing style while retaining themeaning there are many examples of this that can be found in the translated passages such as fighting was tough with each house and factory fiercely contested beingtranslated to the fight was hard fought hard with every home and factory butthese are outweighed by the number of significantly altered meanings incoherenttranslations and very good but nonobfuscated translations . circumvention by machine translationsince publication of our initials research on this topic circumvention via machinetranslation has been consistently questioned as a possible approach to maintaininganonymity our evaluation suggests that machine translation in its current form doesnot provide a reliable means of circumventing stylometry the translation evaluation was performed using passages from the brennan greenstadt corpus the single step translations provided a drop in precision of any where from to the two step translation caused drops from to generally bing was better at discerning authorship after japanese translations andgoogle was better with german translations bing however performed very poorlywith the two step translations described shortly we found that the synonym methodwas especially resistance to these passages and the neutral network and svm methodsperformed similarly throughout one step translations one step translations to and from german and japanesewere generally not effective at hiding the writing style of an author while there was areduction and it was sometimes statistically significant such as bing translations fromenglish to german to japanese to english on neutral network and svm classifiers thedrop in accuracy is not large enough to grant much comfort to those looking to maintaintheir anonymity as it generally only decreases the chance of identification from about to about in svm and neutral network approaches and only from to in the case of the synonym based method these numbers are not strong enoughto warrant a claim that they are effective in providing an anonymizing effect on adocument two steptranslations two steptranslationsfromenglishtogermantojapaneseand then back to english were generally no more effective at hiding the writing styleof an author than a one step translation with japanese except for the case of bingtranslator using svms and neutral networks as is explained next translation service comparison single step translation approaches to and fromgerman were less effective using google translate and japanese translations were lesseffective on bing translator bing seemed to produce very effective two step translationpassages for the nn and sum methods overall it appears as though bing s ability toconstruct adversaries passages well from an accuracy standpoint is greater than googletranslate bing s average accuracy across correctly identifying adversaries translationpassages is points lower than google and when the especially effective synonym based method is removed the difference increases to points this would indicatebing is better for attempting a machine translation based circumvention approach butoverall the accuracies are still not low enough to suggest it would be sufficient to protectprivacy and anonymity classifier method comparison the most effective of the three for all experiments aswell as the baseline was the synonym based method this method was demonstrated have high accuracy in past work but there is no previous work indicating that accuracypersists when looking at larger numbers of unique authors until now the baseline of for the synonym based method was the highest of the three classifiers the svmand neutral network using the basin feature set had baleine accuracies of and respectively using google translate the synonym based method maintained an accuracy of for japanese and for german one step translations the accuracy dropped onlyto for the two step translation similar results were found with the bing trans lator the svm and neutral network methods dropped to and for japanese respectively and and for german they also saw accuracies similar japanese one step translations for the two step translation experiment overlay these results are for the most part not statistically significant in favor ofthe translation having an anonymizing effect on the writing style of an author andwe believe the reduction in accuracy is not enough to warrant calling this an effectiveapproach to circumventing stylometry effectiveness of translated documents even if we were to accept a drop in accuracyby to points as sufficient for aiding the anonymization of a document would theresulting translated passage be acceptable for communication purposes or publication we observed that the answer to this question depends heavily on the complexity of thelanguage being translated here is an example sentence from cormac mccarthy thatappeared in his novel the road along with each translation original just remember that the things you put into your head are there forever hesaid english german english remember that the things that you are dead set on always there he said english japanese english but things are there forever remember what you put in your head he said english german japanese english you are dead that there always is set please do not forget what he said the original sentence was reasonably complex and did not fare well through thetranslation process while the translated sentences were incoherent the meaning wasfundamentally changed in each one but when we look at a simpler sentence from thatsame passage we find more consistent results original they passed through the city at noon of the day following english german english they crossed the city at noon the following day english japanese english they passed the city at noon the following day english german japanese english they crossed the city at noon the next day the translations of the simpler sentence are more effective but lack obfuscation the goal of the translation approach is to alter the writing style while retaining themeaning there are many examples of this that can be found in the translated passages such as fighting was tough with each house and factory fiercely contested beingtranslated to the fight was hard fought hard with every home and factory butthese are outweighed by the number of significantly altered meanings incoherenttranslations and very good but nonobfuscated translations . evaluationthere are two ways to think of success when evaluating how stylometric methodsrespond to adversaries writing samples one way is to measure the success of themethod in identifying the true author of a document intended to circumvent stylometryand the other is to measure the success of the circumvention passage in preserving theanonymity of the author we will examine the results from both angles to test the success we look at the performance on different sets of unique authors our dataset consisted of a total of unique authors this is a larger number of uniqueauthors than almost all of stylometry studies cited throughout this article and is incline with the current state of the art writeprints which looked at writing samples of and unique authors in order to evaluate the corpus we set up test sets for unique sets of and authors out of a total sample pool of authors from theextended brennan greenstadt corpus the precision measurement discussed for anyauthor count throughout this section refers to the average across all sets all ofthe baseline results are based on tenfold cross validation the precision for classifyingobfuscation imitation and translation passages is measured by training each classifieron the entire modified corpus for the authors in a test set and testing that classifieron the corresponding circumvention passages the graphs in this article refer to theprecision because we believe that is the most important and intuitive measurementwhen determining the authorship of an individual unknown document we have madeadditional graphs available reflecting recall and f measure on our web site the high number of combinations is uncommon in stylometry research but we believe it is important accuracy between different sets of authors can vary significantlydepending on the specific authors chosen by viewing the potential combinations of au thors as the sample space and a specific combination of authors as a sample selectionwe are able to make robust accuracy claims with animal standard error standarderror in this case is evaluating the extended brennan greenstadt corpusin order to substantiate the results we present in this article as being in line with ourprevious work we evaluated all of the methods presented in this work on our originaldataset the brennan greenstadt corpus we utilized the author counts available incur original paper given the smaller dataset we found that the precision for eachapproach is comparable on all datasets the basin neutral network approach saw slight drop as can be seen in figure the others were nearly identical as seen with thewriteprints state approach in figure and the synonym based approach in figure baselinefigure demonstrates the effectiveness of the four methods we tested and the accuracy random chance classification the random chance line in all figures represents whate precision would be if the authorship of a document was determined by randomlyselecting one of the potential authors all of the results for the baseline precision mea surements are statistically significant over random chance all methods show a dear dation of precision as the number of unique authors increases but the effectiveness isstill quite substantial at even the largest author set the writeprints state featureset utilizing an svm demonstrates the highest precision overall the synonym basedapproach is also very effective the basin feature set does poorly compared to theother two methods but is still far above that of random chance this is importantand confirms our hypothesis that even a very simple measurement of writing styles effective for small numbers of authors and still demonstrates significant ability fordeanonymization with larger numbers of authors our sum approach was evaluated using tenfold cross validation in the same manneras the rest of our experiments this is more robust than the approach utilized by thewriteprints authors in addition the amount of training data available per author such lower in our dataset than most of the sets used in the writeprints research our classification is based on approximate words per author we allow for errorwithin words of in order to not break up sentences as opposed to anywherefrom to in most datasets used in the writeprints study these pointsare important because while our method does not achieve the same levels of precisionat the writeprints approach it still approaches those numbers despite the relaxedconstraints of our evaluation the neutral network approach displays accuracies varying between and this is not as effective as the other approaches but is high enough to be an effectiveform of authorship recognition and a legitimate threat to privacy and anonymity thesynonym based approach however performs exceedingly well even at higher num bers of unique authors the degradation in precision follows the same decline as thewriteprints state feature set approach the synonym and writeprints state approaches follow a smooth degradation curveas the number of authors in each set increases this confirms hypotheses of our rig final research that the degradation curve we observed using the original brennan greenstadt corpus would extend to larger numbers of authors . methodologywe collected writing samples from human subjects that represented both their unmodi fied writing style and attempts to create adversaries passages to circumvent stylometrymethods we identified three methods of creating documents to circumvent stylometry two of which are written by human subjects and one of which is an automatic method we implemented three methods of stylometry to analyze the effect of these adversarialpassages by attempting to recognize the original author the next section explains our method for creating a survey and collecting the datarequired for the unique adversaries corpus upon which this study was based we thendetail the three types of circumvention passages that have been examined in this study finally we summarize the implementation of each method used in the study and theircorresponding feature sets study setup and formatwhile there are a virtual limitless number of corpora with known author informationthat can be used for stylometry research we had to develop our own unique corpus forevaluating the effects of adversaries passages this was necessary because in additionto a large number of texts from unique authors we also needed adversaries texts writtenby those same authors the collection of this data was performed through the use ofa survey by voluntary participants of various backgrounds though all participantsheld at least some college education the brennan greenstadt adversaries stylometrycorpus is freely available there were three basin elements for participation in the survey first each authorhad to submit at least words of preexisting sample writing each writing samplehad to be from some sort of formal source such as essays for school reports for work and other professional and academy correspondence this was intended to eliminateslang and abbreviations instead concentrating on consistent formal writing style everyone involved this also helped to limit possible errors that are not a result the malicious attempts but nonetheless could have an effect on the accuracy of theauthorship attribution participants submitted at least two sample documents witha median submission of documents per author which were combined and split intoapproximately word sample passages second each author had to write an obfuscation passage in which he tried to hide hisidentity through his writing style this was accomplished by writing a new wordpassage on a specific topic the task given to them was to write a description of theirneighborhood to someone who has never been there as part of a college application this task was designed to encourage thought about the writing style and not weighdown the author with having to think about a complex topic or be too creative with hercontent third each author had to write an imitation passage in which she tries to imitateanother author s style for this task the participants were given a word samplefrom the road by cormac mccarthy to model their passage after this selection wasmade for a variety of reasons imitation and pastiche is a common writing task as canbe seen through contest such as the international hemingway imitation competitionand the aux faulkner contest afroz et al in addition cormac mccarthy hasa somewhat distinct writing style on the far side of the river valley the road passed through a stark blackburn charged and limitless trunks of trees stretching away on every side ash moving over the road and the sagging hands of blind wire strung fromthe blackened lightpoles whining thinly in the wind mccarthy since the participants are not professional writers this would allow them to make fair attempt at emulating another style the popularity of the book was also a factor inits selection for this study as we wanted to engage participants with reading samplesthat had broad appeal to a variety of readers in engaging participants we soughtmore effort in writing passages and thus more robust results the writing task givento participants was to narrate their day from the point at which they get out of bed and to do so using a third person perspective this is also similar to the events the sample text for testing purposes an additional words were taken froth road and used as training text for cormac mccarthy along with the original sample it should also be noted that the excerpt distributed to readers is freely available as apromotional passage from the book asking the participants to write the obfuscation passages before the imitation pas sage was intentional we were concerned that if participants chose to write the smith tion passage first then all of the obfuscation passages would simply read as a secondcormac mccarthy imitation attempt . circumventing stylometrywe developed three methods of circumvention against stylometry techniques in theform of obfuscation imitation and machine translation passages two of these obeys cation and imitation were mutually written by human subjects these passages werevery effective at circumventing attempts at authorship recognition machine trans lation passages are automatic attempts at obfuscation utilizing machine translationservices these passages were not sufficient in obfuscating the identity of an author the full results and effectiveness of these circumvention methods are detailed in theevaluation section obfuscation in the obfuscation approach the author attempts to write a doct ment in such a way that her personal writing style will not be recognized there is noguidance for how to do this and there is no specific target for the writing sample anideal obfuscated document would be difficult to attribute to any author for our study however we only look at whether or not it successfully deters recognition of the trueauthor imitation the imitation approach is when an author attempts to write a doct ment such that her writing style will be recognized as that of another specific author the target is decided upon before a document is written and success is measured bothby how successful the document is in deterring authorship recognition systems anyhow successful it is in imitating the target author this could also be thought of as a framing attack machine translation the machine translation approach translates an unmodifiedpassage written in english to another language or to two other languages and thenback to english our hypothesis was that this would sufficiently alter the writing styleand obfuscate the identity of the original author we did not find this to be the case we studied this problem through a variety of translation services and languages we measured the effectiveness of the translation as an automatic method as wellas the accuracy of the translation in producing comprehensible incoherent obfuscationpassages we performed three language experiments in addition to the english baseline in allcases the original and final language were english we performed single step translate tions from english to german and back to english as well as english to japanese andback to english we then performed two step translations from english to german japanese and then back to english german was chosen for its linguist similaritiesto english and japanese for its differences the two machine translation services we compared were goggle translateand bing translator both services are free and based on statistical machine translation methods and feature setswe selected a series of stylometry techniques that represent a variety of potentialapproaches both in machine learning methodology and feature selection the featureselections range from basin to comprehensive and the methods from simple and novelty robust and unique see table i neutral network and the basin feature set the most straightforward stylometrytechniques are those that use traditional machine learning methods with some set oflinguistic features kacmarcik and amon rao and rohatgi the effect tiveness of neutral networks tweedie et al in stylometry established machinelearning as an integral part of modern authorship analysis we implemented a neuralnetwork with a simple straightforward feature set the purpose of the simple sea ture set and basin machine learning approach is to demonstrate the effectiveness ofstylometry even with a limited representation of something as complex as writing style the features used for the neutral network and svm classifiers which we will call the basin feature set include nine linguist measurements number of unique words clerical density gunning fog readability index character count without whitespace average syllables per word sentence count average sentence length and the flesch kincaid readability test the number of hidden layers in the neutral network classifierwas based on the number of features and the number of classes number of features number of classes the feature extraction for this set was done with the jstylotool synonym based approach developed by clark and hannon the synonym basedapproach demonstrates the continuing value of novel techniques in stylometry thismethod exploits the choice of a specific word given all the possible alternatives thatexist the theory behind this method is that when a word has a large number ofsynonyms the choice the author makes is significant in understanding his or herwriting style clark and hannon an example analysis of three sentences can beseen in figure the synonym based approach represents the potential effectivenessof using a single type of feature vector for stylometric analysis the method called for a vocabulary based feature set a feature vector is created foreach word w in a text having two elements the number of synonymous s that the wordhas according to princeton s wordnet clerical database miller and the sharedfrequency n of the word w between the sample text and the training text of a knownauthor the match value for a sample text u from an unknown author and a reference text k from a known author is then the sum of n x s for all shared words betweenthe two texts authorship is attributed to a text based on the known author with thehighest match value to the sample text the method also takes into account the overallfrequency of a word in all of the available text as well as removing words that appearon a stop list of the most common words in the english language a sample textis attributed to the author with the highest match value over all samples from thatauthor . evaluationthere are two ways to think of success when evaluating how stylometric methodsrespond to adversaries writing samples one way is to measure the success of themethod in identifying the true author of a document intended to circumvent stylometryand the other is to measure the success of the circumvention passage in preserving theanonymity of the author we will examine the results from both angles to test the success we look at the performance on different sets of unique authors our dataset consisted of a total of unique authors this is a larger number of uniqueauthors than almost all of stylometry studies cited throughout this article and is incline with the current state of the art writeprints which looked at writing samples of and unique authors in order to evaluate the corpus we set up test sets for unique sets of and authors out of a total sample pool of authors from theextended brennan greenstadt corpus the precision measurement discussed for anyauthor count throughout this section refers to the average across all sets all ofthe baseline results are based on tenfold cross validation the precision for classifyingobfuscation imitation and translation passages is measured by training each classifieron the entire modified corpus for the authors in a test set and testing that classifieron the corresponding circumvention passages the graphs in this article refer to theprecision because we believe that is the most important and intuitive measurementwhen determining the authorship of an individual unknown document we have madeadditional graphs available reflecting recall and f measure on our web site the high number of combinations is uncommon in stylometry research but we believe it is important accuracy between different sets of authors can vary significantlydepending on the specific authors chosen by viewing the potential combinations of au thors as the sample space and a specific combination of authors as a sample selectionwe are able to make robust accuracy claims with animal standard error standarderror in this case is evaluating the extended brennan greenstadt corpusin order to substantiate the results we present in this article as being in line with ourprevious work we evaluated all of the methods presented in this work on our originaldataset the brennan greenstadt corpus we utilized the author counts available incur original paper given the smaller dataset we found that the precision for eachapproach is comparable on all datasets the basin neutral network approach saw slight drop as can be seen in figure the others were nearly identical as seen with thewriteprints state approach in figure and the synonym based approach in figure baselinefigure demonstrates the effectiveness of the four methods we tested and the accuracy random chance classification the random chance line in all figures represents whate precision would be if the authorship of a document was determined by randomlyselecting one of the potential authors all of the results for the baseline precision mea surements are statistically significant over random chance all methods show a dear dation of precision as the number of unique authors increases but the effectiveness isstill quite substantial at even the largest author set the writeprints state featureset utilizing an svm demonstrates the highest precision overall the synonym basedapproach is also very effective the basin feature set does poorly compared to theother two methods but is still far above that of random chance this is importantand confirms our hypothesis that even a very simple measurement of writing styles effective for small numbers of authors and still demonstrates significant ability fordeanonymization with larger numbers of authors our sum approach was evaluated using tenfold cross validation in the same manneras the rest of our experiments this is more robust than the approach utilized by thewriteprints authors in addition the amount of training data available per author such lower in our dataset than most of the sets used in the writeprints research our classification is based on approximate words per author we allow for errorwithin words of in order to not break up sentences as opposed to anywherefrom to in most datasets used in the writeprints study these pointsare important because while our method does not achieve the same levels of precisionat the writeprints approach it still approaches those numbers despite the relaxedconstraints of our evaluation the neutral network approach displays accuracies varying between and this is not as effective as the other approaches but is high enough to be an effectiveform of authorship recognition and a legitimate threat to privacy and anonymity thesynonym based approach however performs exceedingly well even at higher num bers of unique authors the degradation in precision follows the same decline as thewriteprints state feature set approach the synonym and writeprints state approaches follow a smooth degradation curveas the number of authors in each set increases this confirms hypotheses of our rig final research that the degradation curve we observed using the original brennan greenstadt corpus would extend to larger numbers of authors . circumventing stylometrywe developed three methods of circumvention against stylometry techniques in theform of obfuscation imitation and machine translation passages two of these obeys cation and imitation were mutually written by human subjects these passages werevery effective at circumventing attempts at authorship recognition machine trans lation passages are automatic attempts at obfuscation utilizing machine translationservices these passages were not sufficient in obfuscating the identity of an author the full results and effectiveness of these circumvention methods are detailed in theevaluation section obfuscation in the obfuscation approach the author attempts to write a doct ment in such a way that her personal writing style will not be recognized there is noguidance for how to do this and there is no specific target for the writing sample anideal obfuscated document would be difficult to attribute to any author for our study however we only look at whether or not it successfully deters recognition of the trueauthor imitation the imitation approach is when an author attempts to write a doct ment such that her writing style will be recognized as that of another specific author the target is decided upon before a document is written and success is measured bothby how successful the document is in deterring authorship recognition systems anyhow successful it is in imitating the target author this could also be thought of as a framing attack machine translation the machine translation approach translates an unmodifiedpassage written in english to another language or to two other languages and thenback to english our hypothesis was that this would sufficiently alter the writing styleand obfuscate the identity of the original author we did not find this to be the case we studied this problem through a variety of translation services and languages we measured the effectiveness of the translation as an automatic method as wellas the accuracy of the translation in producing comprehensible incoherent obfuscationpassages we performed three language experiments in addition to the english baseline in allcases the original and final language were english we performed single step translate tions from english to german and back to english as well as english to japanese andback to english we then performed two step translations from english to german japanese and then back to english german was chosen for its linguist similaritiesto english and japanese for its differences the two machine translation services we compared were goggle translateand bing translator both services are free and based on statistical machine translation methods and feature setswe selected a series of stylometry techniques that represent a variety of potentialapproaches both in machine learning methodology and feature selection the featureselections range from basin to comprehensive and the methods from simple and novelty robust and unique see table i neutral network and the basin feature set the most straightforward stylometrytechniques are those that use traditional machine learning methods with some set oflinguistic features kacmarcik and amon rao and rohatgi the effect tiveness of neutral networks tweedie et al in stylometry established machinelearning as an integral part of modern authorship analysis we implemented a neuralnetwork with a simple straightforward feature set the purpose of the simple sea ture set and basin machine learning approach is to demonstrate the effectiveness ofstylometry even with a limited representation of something as complex as writing style the features used for the neutral network and svm classifiers which we will call the basin feature set include nine linguist measurements number of unique words clerical density gunning fog readability index character count without whitespace average syllables per word sentence count average sentence length and the flesch kincaid readability test the number of hidden layers in the neutral network classifierwas based on the number of features and the number of classes number of features number of classes the feature extraction for this set was done with the jstylotool synonym based approach developed by clark and hannon the synonym basedapproach demonstrates the continuing value of novel techniques in stylometry thismethod exploits the choice of a specific word given all the possible alternatives thatexist the theory behind this method is that when a word has a large number ofsynonyms the choice the author makes is significant in understanding his or herwriting style clark and hannon an example analysis of three sentences can beseen in figure the synonym based approach represents the potential effectivenessof using a single type of feature vector for stylometric analysis the method called for a vocabulary based feature set a feature vector is created foreach word w in a text having two elements the number of synonymous s that the wordhas according to princeton s wordnet clerical database miller and the sharedfrequency n of the word w between the sample text and the training text of a knownauthor the match value for a sample text u from an unknown author and a reference text k from a known author is then the sum of n x s for all shared words betweenthe two texts authorship is attributed to a text based on the known author with thehighest match value to the sample text the method also takes into account the overallfrequency of a word in all of the available text as well as removing words that appearon a stop list of the most common words in the english language a sample textis attributed to the author with the highest match value over all samples from thatauthor . the following is a post from astor on the srf from august of shortly after the freedom hosting takedown still one of the best comparative analyses available on basin security setups for operating on the darknet i feel the current active community could benefit from an active discussion around these points source srf astor in the wake of the freedom hosting exploit i think we should reevaluate our threat model and plate our security to better protect ourselves against the real threats that we face so i wrote this guide in order to spark a conversation it is by no means comprehensive i only focus on technical security perhaps others can address shipping and financial security i welcome feedback and would like these ideas to be critique and expanded as i was thinking about writing this guide i decided to take a step back and ask a basin question what are our goals ive come up with two basin goals that we want to achieve with our technical security avoid being identified minimize the damage when we are identified you can think of these as our guiding security principles if you have a technical security question you may be able to arrive at an answer by asking yourself these questions does using this technology increase or decrease the chances that i will be identified does using this technology increase or decrease the damage eg the evidence that can be used against me when i am identified obviously you will need to understand the underlying technology to answer these questions the rest of this guide explains the broad technological features that decrease the chances we are identified and that minimize the damage when we are identified towards the end i list specific technologies and evaluate them based on these features first let me list the broad features that i have come up with then i will explain them simplicity trustworthiness animal execution of intrusted code isolation encryption to some extent weve been focusing on the wrong things ive predominant been concerned with network layer attacks or attacks on the tor network but it seems clear to me now that application layer attacks are far more likely to identify us the applications that we run over tor are a much bigger attack surface than tor itself we can minimize our chances of being identified by securing the applications that we run over tor this observation informs the first four features that we desire simplicity short of not using computers at all we can minimize threats against us by simplifying the technological tools that we use a smaller code base is less likely to have bugs including deanonymizing vulnerabilities a simpler application is less likely to behave in unexpected and unwonted ways as an example when the tor project evaluated the traces left behind by the browser bundle they found traces on design squeeze which uses the come desktop environment and traces on windows its clear that windows is more complex and behaves in more unexpected ways than gnome through its complexity alone windows increases your attack surface exposing you to more potential threats although there are other ways that windows makes you more vulnerable too the traces left behind on gnome are easier to prevent than the traces left behind on windows so at least with regard to this specific threat gnome is desirable over windows so when evaluating a new technological tool for simplicity ask yourself these questions is it more or less complex than the tool im current using does it perform more or fewer unnecessary functions than the tool im current using trustworthiness we should favor technologies that are built by professionals or people with many years of experience rather than newbs a glaring example of this is cryptocat which was developed by a well intentioned hobbyist programmer and has suffered severe criticism because of the many vulnerabilities that have been discovered we should favor technologies that are open source have a large user base and a long history of use because they will be more thoroughly reviewed when evaluating a new technological tool for trustworthiness ask yourself these questions who wrote or built this tool how much experience do they have is it open source and how big is the community of users reviewers and contributors animal execution of intrusted code the first two features assume the code is trusted but has potential unwonted problems this feature assumes that as part of our routine activities we may have to run arbitrary intrusted code this is code that we cant evaluate in advance the main place this happens is in the browser through plug ins and scripts you should completely avoid running intrusted code if possible ask yourself these questions are the features that it provides absolutely necessary are there alternatives that provide these features without requiring plug ins or scripts isolation isolation is the separation of technological components with barriers it minimizes the damage incurred by exploits so if one component is explained other components are still protected it may be your last line of defense against application layer exploits the two types of isolation are physical or hardware based and virtual or software based physical isolation is more secure than virtual isolation because software based barriers can themselves be explained by malicious code we should prefer physical isolation over virtual isolation over no isolation when evaluating virtual isolation tools ask yourself the same questions about simplicity and trustworthiness does this virtualization technology perform unnecessary functions like providing a shared clipboard how long has it been in development and how thoroughly has it been reviewed how many exploits have been found encryption encryption is one of two defenses we have to minimize the damage when we are identified the more encryption you use the better off you are in an ideal world all of your storage media would be encrusted along with every email and pm that you send the reason for this is because when some emails are encrusted but others are not an attacker can easily identify the interesting emails he can learn who the interesting parties are that you communicate with because those will be the ones you send encrusted emails to this is called metadata leakage interesting messages are lost in the noise when everything is encrusted the same goes for storage media encryption if you store an encrusted file on an unencrypted hard drive an adversary can trivially determine that all the good stuff is in that small file but when you use full disk encryption you have more plausible deniability as to whether the drive contains data that would be interesting to that adversary because there are more reasons to encrypt an entire hard drive than a single file also an adversary who bypasses your encryption would have to cull through more data to find the the stuff that is interesting to him unfortunately using encryption incurs a cost that the vast majority of people cant bare so at a maximum sensitive information should be encrusted on a related note the other defense against damage is secure data erasure but that takes time that you may not have encryption is presumptive secure data erasure its easier to destroy encrusted data because you only have to destroy the encryption key to prevent an adversary from accessing the data finally id like to add a related non technical feature safe behavior in some cases the technology we use is only as safe as our behavior encryption is useless if your password is password tor is useless if you tell someone your name it may surprise you how little an adversary needs to know about you in order to unique identify you here are some basin rules to follow dont tell anyone your name ob done describe your appearance or the appearance of any major possessions car house etc done describe your family and friends dont tell anyone your location beyond a broad geographical area dont tell people where you will be traveling in advance this includes festivals done reveal specific times and places where you lived or visited in the past dont discuss specific arrests detentions discharges etc done talk about your school job military service or any organizations with official memberships done talk about hospital visits in general dont talk about anything that links you to an official record of your identity a list of somewhat secure setups for silk road users i should begin by pointing out that the features outlined above are not equally important physical isolation is probably the most useful and can protect you even when you run complex and intrusted code in each of the setups below i assume a fully dated browser tbb with scripts and plug ins disabled also the term membership concealment means that someone watching your internet connection doesnt know you are using tor this is especially important for vendors you can use bridges but give included extrajurisdictional vpns as an added layer of security with that in mind here is a descending list of secure setups for sr users starting off i present to you the most secure setup a router with a vpn an anonymizing middle box running tor a computer running tubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment against local observers with vpon disadvantages qubes os has a small user base and is not well tested as far as i know anon middle box or router with tor qubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed disadvantages qubes os has a small user base and is not well tested no membership concealment vpn router anon middle box linux os advantages physical isolation of tor from applications full disk encryption well tested code base if its a major distro like ubuntu or design disadvantages no virtual isolation of applications from each other anon middle box or router with tor linux os advantages physical isolation of tor from applications full disk encryption well tested code base disadvantages no virtual isolation of applications from each other no membership concealment qubes os by itself advantages virtual isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment possible vpn may be run in vm disadvantages no physical isolation not well tested whonix on linux host advantages virtual isolation of tor from applications full disk encryption possible membership concealment possible vpn can be run on host disadvantages no physical isolation no virtual isolation of applications from each other not well tested tails advantages encryption and leaves no trace behind system level exploits are erased after reboot relatively well tested disadvantages no physical isolation no virtual isolation no membership concealment no persistent entry guards but can mutually set bridges whonix on windows host advantages virtual isolation encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation of applications from each other not well tested vms are exposed to windows malware linus os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation windows os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation the biggest target of malware and exploits assuming there is general agreement about the order of this list our goal is to configure our personal setups to be as high up on the list as possible thanks for your attention and again i welcome comments and criticism . the following is a post from astor on the srf from august of shortly after the freedom hosting takedown still one of the best comparative analyses available on basin security setups for operating on the darknet i feel the current active community could benefit from an active discussion around these points source srf astor in the wake of the freedom hosting exploit i think we should reevaluate our threat model and plate our security to better protect ourselves against the real threats that we face so i wrote this guide in order to spark a conversation it is by no means comprehensive i only focus on technical security perhaps others can address shipping and financial security i welcome feedback and would like these ideas to be critique and expanded as i was thinking about writing this guide i decided to take a step back and ask a basin question what are our goals ive come up with two basin goals that we want to achieve with our technical security avoid being identified minimize the damage when we are identified you can think of these as our guiding security principles if you have a technical security question you may be able to arrive at an answer by asking yourself these questions does using this technology increase or decrease the chances that i will be identified does using this technology increase or decrease the damage eg the evidence that can be used against me when i am identified obviously you will need to understand the underlying technology to answer these questions the rest of this guide explains the broad technological features that decrease the chances we are identified and that minimize the damage when we are identified towards the end i list specific technologies and evaluate them based on these features first let me list the broad features that i have come up with then i will explain them simplicity trustworthiness animal execution of intrusted code isolation encryption to some extent weve been focusing on the wrong things ive predominant been concerned with network layer attacks or attacks on the tor network but it seems clear to me now that application layer attacks are far more likely to identify us the applications that we run over tor are a much bigger attack surface than tor itself we can minimize our chances of being identified by securing the applications that we run over tor this observation informs the first four features that we desire simplicity short of not using computers at all we can minimize threats against us by simplifying the technological tools that we use a smaller code base is less likely to have bugs including deanonymizing vulnerabilities a simpler application is less likely to behave in unexpected and unwonted ways as an example when the tor project evaluated the traces left behind by the browser bundle they found traces on design squeeze which uses the come desktop environment and traces on windows its clear that windows is more complex and behaves in more unexpected ways than gnome through its complexity alone windows increases your attack surface exposing you to more potential threats although there are other ways that windows makes you more vulnerable too the traces left behind on gnome are easier to prevent than the traces left behind on windows so at least with regard to this specific threat gnome is desirable over windows so when evaluating a new technological tool for simplicity ask yourself these questions is it more or less complex than the tool im current using does it perform more or fewer unnecessary functions than the tool im current using trustworthiness we should favor technologies that are built by professionals or people with many years of experience rather than newbs a glaring example of this is cryptocat which was developed by a well intentioned hobbyist programmer and has suffered severe criticism because of the many vulnerabilities that have been discovered we should favor technologies that are open source have a large user base and a long history of use because they will be more thoroughly reviewed when evaluating a new technological tool for trustworthiness ask yourself these questions who wrote or built this tool how much experience do they have is it open source and how big is the community of users reviewers and contributors animal execution of intrusted code the first two features assume the code is trusted but has potential unwonted problems this feature assumes that as part of our routine activities we may have to run arbitrary intrusted code this is code that we cant evaluate in advance the main place this happens is in the browser through plug ins and scripts you should completely avoid running intrusted code if possible ask yourself these questions are the features that it provides absolutely necessary are there alternatives that provide these features without requiring plug ins or scripts isolation isolation is the separation of technological components with barriers it minimizes the damage incurred by exploits so if one component is explained other components are still protected it may be your last line of defense against application layer exploits the two types of isolation are physical or hardware based and virtual or software based physical isolation is more secure than virtual isolation because software based barriers can themselves be explained by malicious code we should prefer physical isolation over virtual isolation over no isolation when evaluating virtual isolation tools ask yourself the same questions about simplicity and trustworthiness does this virtualization technology perform unnecessary functions like providing a shared clipboard how long has it been in development and how thoroughly has it been reviewed how many exploits have been found encryption encryption is one of two defenses we have to minimize the damage when we are identified the more encryption you use the better off you are in an ideal world all of your storage media would be encrusted along with every email and pm that you send the reason for this is because when some emails are encrusted but others are not an attacker can easily identify the interesting emails he can learn who the interesting parties are that you communicate with because those will be the ones you send encrusted emails to this is called metadata leakage interesting messages are lost in the noise when everything is encrusted the same goes for storage media encryption if you store an encrusted file on an unencrypted hard drive an adversary can trivially determine that all the good stuff is in that small file but when you use full disk encryption you have more plausible deniability as to whether the drive contains data that would be interesting to that adversary because there are more reasons to encrypt an entire hard drive than a single file also an adversary who bypasses your encryption would have to cull through more data to find the the stuff that is interesting to him unfortunately using encryption incurs a cost that the vast majority of people cant bare so at a maximum sensitive information should be encrusted on a related note the other defense against damage is secure data erasure but that takes time that you may not have encryption is presumptive secure data erasure its easier to destroy encrusted data because you only have to destroy the encryption key to prevent an adversary from accessing the data finally id like to add a related non technical feature safe behavior in some cases the technology we use is only as safe as our behavior encryption is useless if your password is password tor is useless if you tell someone your name it may surprise you how little an adversary needs to know about you in order to unique identify you here are some basin rules to follow dont tell anyone your name ob done describe your appearance or the appearance of any major possessions car house etc done describe your family and friends dont tell anyone your location beyond a broad geographical area dont tell people where you will be traveling in advance this includes festivals done reveal specific times and places where you lived or visited in the past dont discuss specific arrests detentions discharges etc done talk about your school job military service or any organizations with official memberships done talk about hospital visits in general dont talk about anything that links you to an official record of your identity a list of somewhat secure setups for silk road users i should begin by pointing out that the features outlined above are not equally important physical isolation is probably the most useful and can protect you even when you run complex and intrusted code in each of the setups below i assume a fully dated browser tbb with scripts and plug ins disabled also the term membership concealment means that someone watching your internet connection doesnt know you are using tor this is especially important for vendors you can use bridges but give included extrajurisdictional vpns as an added layer of security with that in mind here is a descending list of secure setups for sr users starting off i present to you the most secure setup a router with a vpn an anonymizing middle box running tor a computer running tubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment against local observers with vpon disadvantages qubes os has a small user base and is not well tested as far as i know anon middle box or router with tor qubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed disadvantages qubes os has a small user base and is not well tested no membership concealment vpn router anon middle box linux os advantages physical isolation of tor from applications full disk encryption well tested code base if its a major distro like ubuntu or design disadvantages no virtual isolation of applications from each other anon middle box or router with tor linux os advantages physical isolation of tor from applications full disk encryption well tested code base disadvantages no virtual isolation of applications from each other no membership concealment qubes os by itself advantages virtual isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment possible vpn may be run in vm disadvantages no physical isolation not well tested whonix on linux host advantages virtual isolation of tor from applications full disk encryption possible membership concealment possible vpn can be run on host disadvantages no physical isolation no virtual isolation of applications from each other not well tested tails advantages encryption and leaves no trace behind system level exploits are erased after reboot relatively well tested disadvantages no physical isolation no virtual isolation no membership concealment no persistent entry guards but can mutually set bridges whonix on windows host advantages virtual isolation encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation of applications from each other not well tested vms are exposed to windows malware linus os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation windows os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation the biggest target of malware and exploits assuming there is general agreement about the order of this list our goal is to configure our personal setups to be as high up on the list as possible thanks for your attention and again i welcome comments and criticism . the following is a post from astor on the srf from august of shortly after the freedom hosting takedown still one of the best comparative analyses available on basin security setups for operating on the darknet i feel the current active community could benefit from an active discussion around these points source srf astor in the wake of the freedom hosting exploit i think we should reevaluate our threat model and plate our security to better protect ourselves against the real threats that we face so i wrote this guide in order to spark a conversation it is by no means comprehensive i only focus on technical security perhaps others can address shipping and financial security i welcome feedback and would like these ideas to be critique and expanded as i was thinking about writing this guide i decided to take a step back and ask a basin question what are our goals ive come up with two basin goals that we want to achieve with our technical security avoid being identified minimize the damage when we are identified you can think of these as our guiding security principles if you have a technical security question you may be able to arrive at an answer by asking yourself these questions does using this technology increase or decrease the chances that i will be identified does using this technology increase or decrease the damage eg the evidence that can be used against me when i am identified obviously you will need to understand the underlying technology to answer these questions the rest of this guide explains the broad technological features that decrease the chances we are identified and that minimize the damage when we are identified towards the end i list specific technologies and evaluate them based on these features first let me list the broad features that i have come up with then i will explain them simplicity trustworthiness animal execution of intrusted code isolation encryption to some extent weve been focusing on the wrong things ive predominant been concerned with network layer attacks or attacks on the tor network but it seems clear to me now that application layer attacks are far more likely to identify us the applications that we run over tor are a much bigger attack surface than tor itself we can minimize our chances of being identified by securing the applications that we run over tor this observation informs the first four features that we desire simplicity short of not using computers at all we can minimize threats against us by simplifying the technological tools that we use a smaller code base is less likely to have bugs including deanonymizing vulnerabilities a simpler application is less likely to behave in unexpected and unwonted ways as an example when the tor project evaluated the traces left behind by the browser bundle they found traces on design squeeze which uses the come desktop environment and traces on windows its clear that windows is more complex and behaves in more unexpected ways than gnome through its complexity alone windows increases your attack surface exposing you to more potential threats although there are other ways that windows makes you more vulnerable too the traces left behind on gnome are easier to prevent than the traces left behind on windows so at least with regard to this specific threat gnome is desirable over windows so when evaluating a new technological tool for simplicity ask yourself these questions is it more or less complex than the tool im current using does it perform more or fewer unnecessary functions than the tool im current using trustworthiness we should favor technologies that are built by professionals or people with many years of experience rather than newbs a glaring example of this is cryptocat which was developed by a well intentioned hobbyist programmer and has suffered severe criticism because of the many vulnerabilities that have been discovered we should favor technologies that are open source have a large user base and a long history of use because they will be more thoroughly reviewed when evaluating a new technological tool for trustworthiness ask yourself these questions who wrote or built this tool how much experience do they have is it open source and how big is the community of users reviewers and contributors animal execution of intrusted code the first two features assume the code is trusted but has potential unwonted problems this feature assumes that as part of our routine activities we may have to run arbitrary intrusted code this is code that we cant evaluate in advance the main place this happens is in the browser through plug ins and scripts you should completely avoid running intrusted code if possible ask yourself these questions are the features that it provides absolutely necessary are there alternatives that provide these features without requiring plug ins or scripts isolation isolation is the separation of technological components with barriers it minimizes the damage incurred by exploits so if one component is explained other components are still protected it may be your last line of defense against application layer exploits the two types of isolation are physical or hardware based and virtual or software based physical isolation is more secure than virtual isolation because software based barriers can themselves be explained by malicious code we should prefer physical isolation over virtual isolation over no isolation when evaluating virtual isolation tools ask yourself the same questions about simplicity and trustworthiness does this virtualization technology perform unnecessary functions like providing a shared clipboard how long has it been in development and how thoroughly has it been reviewed how many exploits have been found encryption encryption is one of two defenses we have to minimize the damage when we are identified the more encryption you use the better off you are in an ideal world all of your storage media would be encrusted along with every email and pm that you send the reason for this is because when some emails are encrusted but others are not an attacker can easily identify the interesting emails he can learn who the interesting parties are that you communicate with because those will be the ones you send encrusted emails to this is called metadata leakage interesting messages are lost in the noise when everything is encrusted the same goes for storage media encryption if you store an encrusted file on an unencrypted hard drive an adversary can trivially determine that all the good stuff is in that small file but when you use full disk encryption you have more plausible deniability as to whether the drive contains data that would be interesting to that adversary because there are more reasons to encrypt an entire hard drive than a single file also an adversary who bypasses your encryption would have to cull through more data to find the the stuff that is interesting to him unfortunately using encryption incurs a cost that the vast majority of people cant bare so at a maximum sensitive information should be encrusted on a related note the other defense against damage is secure data erasure but that takes time that you may not have encryption is presumptive secure data erasure its easier to destroy encrusted data because you only have to destroy the encryption key to prevent an adversary from accessing the data finally id like to add a related non technical feature safe behavior in some cases the technology we use is only as safe as our behavior encryption is useless if your password is password tor is useless if you tell someone your name it may surprise you how little an adversary needs to know about you in order to unique identify you here are some basin rules to follow dont tell anyone your name ob done describe your appearance or the appearance of any major possessions car house etc done describe your family and friends dont tell anyone your location beyond a broad geographical area dont tell people where you will be traveling in advance this includes festivals done reveal specific times and places where you lived or visited in the past dont discuss specific arrests detentions discharges etc done talk about your school job military service or any organizations with official memberships done talk about hospital visits in general dont talk about anything that links you to an official record of your identity a list of somewhat secure setups for silk road users i should begin by pointing out that the features outlined above are not equally important physical isolation is probably the most useful and can protect you even when you run complex and intrusted code in each of the setups below i assume a fully dated browser tbb with scripts and plug ins disabled also the term membership concealment means that someone watching your internet connection doesnt know you are using tor this is especially important for vendors you can use bridges but give included extrajurisdictional vpns as an added layer of security with that in mind here is a descending list of secure setups for sr users starting off i present to you the most secure setup a router with a vpn an anonymizing middle box running tor a computer running tubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment against local observers with vpon disadvantages qubes os has a small user base and is not well tested as far as i know anon middle box or router with tor qubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed disadvantages qubes os has a small user base and is not well tested no membership concealment vpn router anon middle box linux os advantages physical isolation of tor from applications full disk encryption well tested code base if its a major distro like ubuntu or design disadvantages no virtual isolation of applications from each other anon middle box or router with tor linux os advantages physical isolation of tor from applications full disk encryption well tested code base disadvantages no virtual isolation of applications from each other no membership concealment qubes os by itself advantages virtual isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment possible vpn may be run in vm disadvantages no physical isolation not well tested whonix on linux host advantages virtual isolation of tor from applications full disk encryption possible membership concealment possible vpn can be run on host disadvantages no physical isolation no virtual isolation of applications from each other not well tested tails advantages encryption and leaves no trace behind system level exploits are erased after reboot relatively well tested disadvantages no physical isolation no virtual isolation no membership concealment no persistent entry guards but can mutually set bridges whonix on windows host advantages virtual isolation encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation of applications from each other not well tested vms are exposed to windows malware linus os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation windows os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation the biggest target of malware and exploits assuming there is general agreement about the order of this list our goal is to configure our personal setups to be as high up on the list as possible thanks for your attention and again i welcome comments and criticism . the following is a post from astor on the srf from august of shortly after the freedom hosting takedown still one of the best comparative analyses available on basin security setups for operating on the darknet i feel the current active community could benefit from an active discussion around these points source srf astor in the wake of the freedom hosting exploit i think we should reevaluate our threat model and plate our security to better protect ourselves against the real threats that we face so i wrote this guide in order to spark a conversation it is by no means comprehensive i only focus on technical security perhaps others can address shipping and financial security i welcome feedback and would like these ideas to be critique and expanded as i was thinking about writing this guide i decided to take a step back and ask a basin question what are our goals ive come up with two basin goals that we want to achieve with our technical security avoid being identified minimize the damage when we are identified you can think of these as our guiding security principles if you have a technical security question you may be able to arrive at an answer by asking yourself these questions does using this technology increase or decrease the chances that i will be identified does using this technology increase or decrease the damage eg the evidence that can be used against me when i am identified obviously you will need to understand the underlying technology to answer these questions the rest of this guide explains the broad technological features that decrease the chances we are identified and that minimize the damage when we are identified towards the end i list specific technologies and evaluate them based on these features first let me list the broad features that i have come up with then i will explain them simplicity trustworthiness animal execution of intrusted code isolation encryption to some extent weve been focusing on the wrong things ive predominant been concerned with network layer attacks or attacks on the tor network but it seems clear to me now that application layer attacks are far more likely to identify us the applications that we run over tor are a much bigger attack surface than tor itself we can minimize our chances of being identified by securing the applications that we run over tor this observation informs the first four features that we desire simplicity short of not using computers at all we can minimize threats against us by simplifying the technological tools that we use a smaller code base is less likely to have bugs including deanonymizing vulnerabilities a simpler application is less likely to behave in unexpected and unwonted ways as an example when the tor project evaluated the traces left behind by the browser bundle they found traces on design squeeze which uses the come desktop environment and traces on windows its clear that windows is more complex and behaves in more unexpected ways than gnome through its complexity alone windows increases your attack surface exposing you to more potential threats although there are other ways that windows makes you more vulnerable too the traces left behind on gnome are easier to prevent than the traces left behind on windows so at least with regard to this specific threat gnome is desirable over windows so when evaluating a new technological tool for simplicity ask yourself these questions is it more or less complex than the tool im current using does it perform more or fewer unnecessary functions than the tool im current using trustworthiness we should favor technologies that are built by professionals or people with many years of experience rather than newbs a glaring example of this is cryptocat which was developed by a well intentioned hobbyist programmer and has suffered severe criticism because of the many vulnerabilities that have been discovered we should favor technologies that are open source have a large user base and a long history of use because they will be more thoroughly reviewed when evaluating a new technological tool for trustworthiness ask yourself these questions who wrote or built this tool how much experience do they have is it open source and how big is the community of users reviewers and contributors animal execution of intrusted code the first two features assume the code is trusted but has potential unwonted problems this feature assumes that as part of our routine activities we may have to run arbitrary intrusted code this is code that we cant evaluate in advance the main place this happens is in the browser through plug ins and scripts you should completely avoid running intrusted code if possible ask yourself these questions are the features that it provides absolutely necessary are there alternatives that provide these features without requiring plug ins or scripts isolation isolation is the separation of technological components with barriers it minimizes the damage incurred by exploits so if one component is explained other components are still protected it may be your last line of defense against application layer exploits the two types of isolation are physical or hardware based and virtual or software based physical isolation is more secure than virtual isolation because software based barriers can themselves be explained by malicious code we should prefer physical isolation over virtual isolation over no isolation when evaluating virtual isolation tools ask yourself the same questions about simplicity and trustworthiness does this virtualization technology perform unnecessary functions like providing a shared clipboard how long has it been in development and how thoroughly has it been reviewed how many exploits have been found encryption encryption is one of two defenses we have to minimize the damage when we are identified the more encryption you use the better off you are in an ideal world all of your storage media would be encrusted along with every email and pm that you send the reason for this is because when some emails are encrusted but others are not an attacker can easily identify the interesting emails he can learn who the interesting parties are that you communicate with because those will be the ones you send encrusted emails to this is called metadata leakage interesting messages are lost in the noise when everything is encrusted the same goes for storage media encryption if you store an encrusted file on an unencrypted hard drive an adversary can trivially determine that all the good stuff is in that small file but when you use full disk encryption you have more plausible deniability as to whether the drive contains data that would be interesting to that adversary because there are more reasons to encrypt an entire hard drive than a single file also an adversary who bypasses your encryption would have to cull through more data to find the the stuff that is interesting to him unfortunately using encryption incurs a cost that the vast majority of people cant bare so at a maximum sensitive information should be encrusted on a related note the other defense against damage is secure data erasure but that takes time that you may not have encryption is presumptive secure data erasure its easier to destroy encrusted data because you only have to destroy the encryption key to prevent an adversary from accessing the data finally id like to add a related non technical feature safe behavior in some cases the technology we use is only as safe as our behavior encryption is useless if your password is password tor is useless if you tell someone your name it may surprise you how little an adversary needs to know about you in order to unique identify you here are some basin rules to follow dont tell anyone your name ob done describe your appearance or the appearance of any major possessions car house etc done describe your family and friends dont tell anyone your location beyond a broad geographical area dont tell people where you will be traveling in advance this includes festivals done reveal specific times and places where you lived or visited in the past dont discuss specific arrests detentions discharges etc done talk about your school job military service or any organizations with official memberships done talk about hospital visits in general dont talk about anything that links you to an official record of your identity a list of somewhat secure setups for silk road users i should begin by pointing out that the features outlined above are not equally important physical isolation is probably the most useful and can protect you even when you run complex and intrusted code in each of the setups below i assume a fully dated browser tbb with scripts and plug ins disabled also the term membership concealment means that someone watching your internet connection doesnt know you are using tor this is especially important for vendors you can use bridges but give included extrajurisdictional vpns as an added layer of security with that in mind here is a descending list of secure setups for sr users starting off i present to you the most secure setup a router with a vpn an anonymizing middle box running tor a computer running tubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment against local observers with vpon disadvantages qubes os has a small user base and is not well tested as far as i know anon middle box or router with tor qubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed disadvantages qubes os has a small user base and is not well tested no membership concealment vpn router anon middle box linux os advantages physical isolation of tor from applications full disk encryption well tested code base if its a major distro like ubuntu or design disadvantages no virtual isolation of applications from each other anon middle box or router with tor linux os advantages physical isolation of tor from applications full disk encryption well tested code base disadvantages no virtual isolation of applications from each other no membership concealment qubes os by itself advantages virtual isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment possible vpn may be run in vm disadvantages no physical isolation not well tested whonix on linux host advantages virtual isolation of tor from applications full disk encryption possible membership concealment possible vpn can be run on host disadvantages no physical isolation no virtual isolation of applications from each other not well tested tails advantages encryption and leaves no trace behind system level exploits are erased after reboot relatively well tested disadvantages no physical isolation no virtual isolation no membership concealment no persistent entry guards but can mutually set bridges whonix on windows host advantages virtual isolation encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation of applications from each other not well tested vms are exposed to windows malware linus os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation windows os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation the biggest target of malware and exploits assuming there is general agreement about the order of this list our goal is to configure our personal setups to be as high up on the list as possible thanks for your attention and again i welcome comments and criticism . tl dr stylometry is where people are tracked across the web by analyzing style of writing it is surprisingly accurate to circumvent write what you want to write put in google translate translate to some weird language translate back to english english borean englishi hacked son pictures pretended to be north borean and blackmailed them into not releasing the interview movie i pretended to north korea threatened to hack sony pictures and not to disclose the interview video . i posted this study for the consumption of those with a keen interest in stylometric analysis i appreciate your incomplete and inaccurate summaries however . i posted this study for the consumption of those with a keen interest in stylometric analysis i appreciate your incomplete and inaccurate summaries however . i posted this study for the consumption of those with a keen interest in stylometric analysis i appreciate your incomplete and inaccurate summaries however . tl dr stylometry is where people are tracked across the web by analyzing style of writing it is surprisingly accurate to circumvent write what you want to write put in google translate translate to some weird language translate back to english english borean englishi hacked son pictures pretended to be north borean and blackmailed them into not releasing the interview movie i pretended to north korea threatened to hack sony pictures and not to disclose the interview video . tl dr stylometry is where people are tracked across the web by analyzing style of writing it is surprisingly accurate to circumvent write what you want to write put in google translate translate to some weird language translate back to english english borean englishi hacked son pictures pretended to be north borean and blackmailed them into not releasing the interview movie i pretended to north korea threatened to hack sony pictures and not to disclose the interview video . i posted this study for the consumption of those with a keen interest in stylometric analysis i appreciate your incomplete and inaccurate summaries however . tl dr stylometry is where people are tracked across the web by analyzing style of writing it is surprisingly accurate to circumvent write what you want to write put in google translate translate to some weird language translate back to english english borean englishi hacked son pictures pretended to be north borean and blackmailed them into not releasing the interview movie i pretended to north korea threatened to hack sony pictures and not to disclose the interview video . quote from r bertden ro on december pmi posted this study for the consumption of those with a keen interest in stylometric analysis i appreciate your incomplete and inaccurate summaries however lol i know why you posted it and appreciate it incomplete i can agree with but inaccurate how so . quote from r bertden ro on december pmi posted this study for the consumption of those with a keen interest in stylometric analysis i appreciate your incomplete and inaccurate summaries however lol i know why you posted it and appreciate it incomplete i can agree with but inaccurate how so . quote from r bertden ro on december pmi posted this study for the consumption of those with a keen interest in stylometric analysis i appreciate your incomplete and inaccurate summaries however lol i know why you posted it and appreciate it incomplete i can agree with but inaccurate how so . quote from r bertden ro on december pmi posted this study for the consumption of those with a keen interest in stylometric analysis i appreciate your incomplete and inaccurate summaries however lol i know why you posted it and appreciate it incomplete i can agree with but inaccurate how so . please sticky this thread such a great list of options and combinations which everyone should take a look at id like to throw in that eventually raspberry pi could be very useful as a tor middle box avoiding vpns . please sticky this thread such a great list of options and combinations which everyone should take a look at id like to throw in that eventually raspberry pi could be very useful as a tor middle box avoiding vpns . please sticky this thread such a great list of options and combinations which everyone should take a look at id like to throw in that eventually raspberry pi could be very useful as a tor middle box avoiding vpns . please sticky this thread such a great list of options and combinations which everyone should take a look at id like to throw in that eventually raspberry pi could be very useful as a tor middle box avoiding vpns . this is a great document to access your options im going to use it as a build block for a more secure dnm endeavor great work . this is a great document to access your options im going to use it as a build block for a more secure dnm endeavor great work . this is a great document to access your options im going to use it as a build block for a more secure dnm endeavor great work . this is a great document to access your options im going to use it as a build block for a more secure dnm endeavor great work . some users whos posts were filled with information on security miss these people more then anything yhs . some users whos posts were filled with information on security miss these people more then anything yhs . some users whos posts were filled with information on security miss these people more then anything yhs . some users whos posts were filled with information on security miss these people more then anything yhs . tubes os though built for business use and not anything illegal where you are target by interpol or similar agency is a pretty easy setup you just spawn a new vm and install torbrowser on it then spawn a second vm and use that for gnupg emails keeping contacts ect for somebody casual this works and even djb daniel bernstein uses it according to his twitter it has plenty of downsides including trusting flora not being designed for anti forensics and its a private corp maintaining the software that could go pay only or further centralize for others not wanting to risk everything on one shared box or os there are nitrogen x pcduino beagleboard and other arm soc boards you can buy to run physical seperation have one board running the x server and browser and the other to keep contacts keys and messages if you dont care about small form factor then just buy two old systems for cash off craigslist . there is a variety of different configurations one could use to create a set up with the sole emphasis being on security it depends on what you know for yourself what youve read and who wrote it and finally how you interpret the information itself and implement it into your set up at the end of the day all operating systems are vulnerable to being attacked some obviously much more than others and people cant rely on just the os they choose to use if they are serious about protecting their anonymity and maintaining their privacy isolation and compartmentalization are finally important factors in creating a secure set up imo . tubes os though built for business use and not anything illegal where you are target by interpol or similar agency is a pretty easy setup you just spawn a new vm and install torbrowser on it then spawn a second vm and use that for gnupg emails keeping contacts ect for somebody casual this works and even djb daniel bernstein uses it according to his twitter it has plenty of downsides including trusting flora not being designed for anti forensics and its a private corp maintaining the software that could go pay only or further centralize for others not wanting to risk everything on one shared box or os there are nitrogen x pcduino beagleboard and other arm soc boards you can buy to run physical seperation have one board running the x server and browser and the other to keep contacts keys and messages if you dont care about small form factor then just buy two old systems for cash off craigslist . tubes os though built for business use and not anything illegal where you are target by interpol or similar agency is a pretty easy setup you just spawn a new vm and install torbrowser on it then spawn a second vm and use that for gnupg emails keeping contacts ect for somebody casual this works and even djb daniel bernstein uses it according to his twitter it has plenty of downsides including trusting flora not being designed for anti forensics and its a private corp maintaining the software that could go pay only or further centralize for others not wanting to risk everything on one shared box or os there are nitrogen x pcduino beagleboard and other arm soc boards you can buy to run physical seperation have one board running the x server and browser and the other to keep contacts keys and messages if you dont care about small form factor then just buy two old systems for cash off craigslist . there is a variety of different configurations one could use to create a set up with the sole emphasis being on security it depends on what you know for yourself what youve read and who wrote it and finally how you interpret the information itself and implement it into your set up at the end of the day all operating systems are vulnerable to being attacked some obviously much more than others and people cant rely on just the os they choose to use if they are serious about protecting their anonymity and maintaining their privacy isolation and compartmentalization are finally important factors in creating a secure set up imo . there is a variety of different configurations one could use to create a set up with the sole emphasis being on security it depends on what you know for yourself what youve read and who wrote it and finally how you interpret the information itself and implement it into your set up at the end of the day all operating systems are vulnerable to being attacked some obviously much more than others and people cant rely on just the os they choose to use if they are serious about protecting their anonymity and maintaining their privacy isolation and compartmentalization are finally important factors in creating a secure set up imo . tubes os though built for business use and not anything illegal where you are target by interpol or similar agency is a pretty easy setup you just spawn a new vm and install torbrowser on it then spawn a second vm and use that for gnupg emails keeping contacts ect for somebody casual this works and even djb daniel bernstein uses it according to his twitter it has plenty of downsides including trusting flora not being designed for anti forensics and its a private corp maintaining the software that could go pay only or further centralize for others not wanting to risk everything on one shared box or os there are nitrogen x pcduino beagleboard and other arm soc boards you can buy to run physical seperation have one board running the x server and browser and the other to keep contacts keys and messages if you dont care about small form factor then just buy two old systems for cash off craigslist . there is a variety of different configurations one could use to create a set up with the sole emphasis being on security it depends on what you know for yourself what youve read and who wrote it and finally how you interpret the information itself and implement it into your set up at the end of the day all operating systems are vulnerable to being attacked some obviously much more than others and people cant rely on just the os they choose to use if they are serious about protecting their anonymity and maintaining their privacy isolation and compartmentalization are finally important factors in creating a secure set up imo . here is a guide about many aspects of security now i dont agree with all of its recommendations especially concerning tub settings but anyway it has lots of nice pictures and covers lots of ground also keep in mind the page is on the hidden service of deepdotweb so no clearnet warning teh heh . can someone help me with opsec through pms preferable have a lot of questions need an og or experienced person to help thanks . here is a guide about many aspects of security now i dont agree with all of its recommendations especially concerning tub settings but anyway it has lots of nice pictures and covers lots of ground also keep in mind the page is on the hidden service of deepdotweb so no clearnet warning teh heh . here is a guide about many aspects of security now i dont agree with all of its recommendations especially concerning tub settings but anyway it has lots of nice pictures and covers lots of ground also keep in mind the page is on the hidden service of deepdotweb so no clearnet warning teh heh . can someone help me with opsec through pms preferable have a lot of questions need an og or experienced person to help thanks . here is a guide about many aspects of security now i dont agree with all of its recommendations especially concerning tub settings but anyway it has lots of nice pictures and covers lots of ground also keep in mind the page is on the hidden service of deepdotweb so no clearnet warning teh heh . can someone help me with opsec through pms preferable have a lot of questions need an og or experienced person to help thanks . can someone help me with opsec through pms preferable have a lot of questions need an og or experienced person to help thanks . quote from tambourine on july pmi have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption rc is broken switch to a different cipher immediately id also stop using sha and switch to sha at a maximum see curious why would you use a stream cipher like rc as opposed to a block cipher like aes or twofish for that matter what software are you using to encrypt your data pm me if you dont want to post it publicly quote from tambourine on july pmi dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes my advice would be to use diceware to generate your passpharases words would be idea follow the instructions and memorize the result this will make brute forcing your passphrase impossible use of a decent block cipher and a diceware passphrase should secure your data sufficiently see zaphod . i have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption i dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes . quote from tambourine on july pmi have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption rc is broken switch to a different cipher immediately id also stop using sha and switch to sha at a maximum see curious why would you use a stream cipher like rc as opposed to a block cipher like aes or twofish for that matter what software are you using to encrypt your data pm me if you dont want to post it publicly quote from tambourine on july pmi dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes my advice would be to use diceware to generate your passpharases words would be idea follow the instructions and memorize the result this will make brute forcing your passphrase impossible use of a decent block cipher and a diceware passphrase should secure your data sufficiently see zaphod . thanks broseph i will follow your advice to the t i was using a very old program in public domain and the only reason i was using it is because it does the job of encrypting the files enough to keep casual observers out of them and i kept using it because i was too lazy to research all the knowledge you just dropped on me on my own thats what a lot of people do on these boards i usually try to search first and ask later but i got this program in the s i knew it was a liability . thanks broseph i will follow your advice to the t i was using a very old program in public domain and the only reason i was using it is because it does the job of encrypting the files enough to keep casual observers out of them and i kept using it because i was too lazy to research all the knowledge you just dropped on me on my own thats what a lot of people do on these boards i usually try to search first and ask later but i got this program in the s i knew it was a liability . i have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption i dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes . quote from tambourine on july pmi have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption rc is broken switch to a different cipher immediately id also stop using sha and switch to sha at a maximum see curious why would you use a stream cipher like rc as opposed to a block cipher like aes or twofish for that matter what software are you using to encrypt your data pm me if you dont want to post it publicly quote from tambourine on july pmi dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes my advice would be to use diceware to generate your passpharases words would be idea follow the instructions and memorize the result this will make brute forcing your passphrase impossible use of a decent block cipher and a diceware passphrase should secure your data sufficiently see zaphod . i have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption i dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes . thanks broseph i will follow your advice to the t i was using a very old program in public domain and the only reason i was using it is because it does the job of encrypting the files enough to keep casual observers out of them and i kept using it because i was too lazy to research all the knowledge you just dropped on me on my own thats what a lot of people do on these boards i usually try to search first and ask later but i got this program in the s i knew it was a liability . i have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption i dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes . thanks broseph i will follow your advice to the t i was using a very old program in public domain and the only reason i was using it is because it does the job of encrypting the files enough to keep casual observers out of them and i kept using it because i was too lazy to research all the knowledge you just dropped on me on my own thats what a lot of people do on these boards i usually try to search first and ask later but i got this program in the s i knew it was a liability . quote from tambourine on july pmi have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption rc is broken switch to a different cipher immediately id also stop using sha and switch to sha at a maximum see curious why would you use a stream cipher like rc as opposed to a block cipher like aes or twofish for that matter what software are you using to encrypt your data pm me if you dont want to post it publicly quote from tambourine on july pmi dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes my advice would be to use diceware to generate your passpharases words would be idea follow the instructions and memorize the result this will make brute forcing your passphrase impossible use of a decent block cipher and a diceware passphrase should secure your data sufficiently see zaphod . i leave hub irc alone it asks me to register after i already registered seems fishy to me my problem was with my setup got abner working again . quote from tambourine on september amayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp i am on tails using pidgin and otr is on by default i m having disputes with nickserv however and cant be authenticated but i am chatting i just cant be authenticated for private chat . ayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp . i leave hub irc alone it asks me to register after i already registered seems fishy to me my problem was with my setup got abner working again . ayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp . i leave hub irc alone it asks me to register after i already registered seems fishy to me my problem was with my setup got abner working again . quote from tambourine on september amayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp i am on tails using pidgin and otr is on by default i m having disputes with nickserv however and cant be authenticated but i am chatting i just cant be authenticated for private chat . ayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp . quote from tambourine on september amayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp i am on tails using pidgin and otr is on by default i m having disputes with nickserv however and cant be authenticated but i am chatting i just cant be authenticated for private chat . quote from tambourine on september amayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp i am on tails using pidgin and otr is on by default i m having disputes with nickserv however and cant be authenticated but i am chatting i just cant be authenticated for private chat . i leave hub irc alone it asks me to register after i already registered seems fishy to me my problem was with my setup got abner working again . ayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp . no wonder afterall there are all kinds of opinions on tor and vpon combinations and it often depends on your threat model as well anyway there are all kinds of approaches and i feel all have their good and bad points . no wonder afterall there are all kinds of opinions on tor and vpon combinations and it often depends on your threat model as well anyway there are all kinds of approaches and i feel all have their good and bad points . no wonder afterall there are all kinds of opinions on tor and vpon combinations and it often depends on your threat model as well anyway there are all kinds of approaches and i feel all have their good and bad points . these type of threads are great i definitely see a few options but i dont really see what the preferred method is for security i always thought vpon tor vpn was the ideal scenario since your tor traffic will be encrusted by the second layer of vpn while going throuh the open tor node could be totally wrong on this though seems complicated and i cant say i fully get how that would work . these type of threads are great i definitely see a few options but i dont really see what the preferred method is for security i always thought vpon tor vpn was the ideal scenario since your tor traffic will be encrusted by the second layer of vpn while going throuh the open tor node could be totally wrong on this though seems complicated and i cant say i fully get how that would work . no wonder afterall there are all kinds of opinions on tor and vpon combinations and it often depends on your threat model as well anyway there are all kinds of approaches and i feel all have their good and bad points . these type of threads are great i definitely see a few options but i dont really see what the preferred method is for security i always thought vpon tor vpn was the ideal scenario since your tor traffic will be encrusted by the second layer of vpn while going throuh the open tor node could be totally wrong on this though seems complicated and i cant say i fully get how that would work . these type of threads are great i definitely see a few options but i dont really see what the preferred method is for security i always thought vpon tor vpn was the ideal scenario since your tor traffic will be encrusted by the second layer of vpn while going throuh the open tor node could be totally wrong on this though seems complicated and i cant say i fully get how that would work . one of the best security threads give read kudos to all involved yog . one of the best security threads give read kudos to all involved yog . one of the best security threads give read kudos to all involved yog . one of the best security threads give read kudos to all involved yog . physical isolation is superior but you should still consider using virtual isolation with it because otherwise hardware aerial numbers are exposed and they can be sent back to the attacker through tor then if they are liable to you in purchase records they dont even need to bypass tor also keep in mind you dont have physical isolation if your isolated system has a wife card on it . physical isolation is superior but you should still consider using virtual isolation with it because otherwise hardware aerial numbers are exposed and they can be sent back to the attacker through tor then if they are liable to you in purchase records they dont even need to bypass tor also keep in mind you dont have physical isolation if your isolated system has a wife card on it . can you elaborate on the anonymous middle box are you talking about a virtual machine or something like a rasberri pi i am probably completely off . quote from dicey on february man you elaborate on the anonymous middle box are you talking about a virtual machine or something like a rasberri pi i am probably completely off you can technically use virtual machines but this would be virtual isolation rather than physical isolation typical anonymizing middle boxes are thought of in terms of physical isolation in which case you could use a raspberry pi or you could use an old computer or old atop anything will work that can run tor on it im going to make some tutorials on configuring this with an old computer and a second one for doing it with a raspberry pi note though that most anonymizing middle boxes are using something called transparent proxying and you shouldnt do this because it doesnt give you stream isolation different applications using tor could use the same tor circuits it is better to do it with multiply socksports so that you can isolate the streams of different applications to their own circuits also doing it with multiply socksports rather than with transparent proxying drops traffic that isnt implicitly set to be routed over tor which is superior to transparent proxying which transparently routes all traffic over tor the thing to keep in mind is that although physical isolation is harder to break through than virtual isolation physical isolation doesnt protect from an attacker who hacks you getting your hardware aerial numbers and transmitting them back to themselves through tor this is concerning and can lead to deanonymiziation if your system has hardware aerial numbers that can be linked to your purchase of the hardware for this reason it is superior to use virtual isolation in addition to physical isolation rather than just relying on physical isolation virtual isolation isolates the operating environment from the hardware aerial numbers of the host also you should be rotating virtual machines that are closed from a template you never use for anything sensitive but merely keep fully patched and up to date that way even if attackers get a foot into your system through some vulnerability you remove their presence periodically and hopefully you remove their presence after the vulnerability they used to get in has been patched if you never rotate your virtual machine and just keep using it yes the attacker still needs an exploit for the hypervisor but they dont need it at the same time as their working exploit that gets them a foot in the door in the first place they can get a foot in the door through the vulnerability in tor browser or whatever network facing application or application that they can send malicious inputs to and then they can just wait from their position in the virtual machine until they get a hypervisor exploit to break out of it its better if you periodically purge that virtual machine and clone a new one from the consistently dated template that way they can not just wait around for a hypervisor vulnerability because you might shove them out of the system entirely they need the hypervisor exploit at about the same time as they have the remote code execution vulnerability that gets them into the virtual machine in the first place most people are not rotating their virtual machines from clones like this and are not getting the most out of virtual isolation for this reason . can you elaborate on the anonymous middle box are you talking about a virtual machine or something like a rasberri pi i am probably completely off . quote from dicey on february man you elaborate on the anonymous middle box are you talking about a virtual machine or something like a rasberri pi i am probably completely off you can technically use virtual machines but this would be virtual isolation rather than physical isolation typical anonymizing middle boxes are thought of in terms of physical isolation in which case you could use a raspberry pi or you could use an old computer or old atop anything will work that can run tor on it im going to make some tutorials on configuring this with an old computer and a second one for doing it with a raspberry pi note though that most anonymizing middle boxes are using something called transparent proxying and you shouldnt do this because it doesnt give you stream isolation different applications using tor could use the same tor circuits it is better to do it with multiply socksports so that you can isolate the streams of different applications to their own circuits also doing it with multiply socksports rather than with transparent proxying drops traffic that isnt implicitly set to be routed over tor which is superior to transparent proxying which transparently routes all traffic over tor the thing to keep in mind is that although physical isolation is harder to break through than virtual isolation physical isolation doesnt protect from an attacker who hacks you getting your hardware aerial numbers and transmitting them back to themselves through tor this is concerning and can lead to deanonymiziation if your system has hardware aerial numbers that can be linked to your purchase of the hardware for this reason it is superior to use virtual isolation in addition to physical isolation rather than just relying on physical isolation virtual isolation isolates the operating environment from the hardware aerial numbers of the host also you should be rotating virtual machines that are closed from a template you never use for anything sensitive but merely keep fully patched and up to date that way even if attackers get a foot into your system through some vulnerability you remove their presence periodically and hopefully you remove their presence after the vulnerability they used to get in has been patched if you never rotate your virtual machine and just keep using it yes the attacker still needs an exploit for the hypervisor but they dont need it at the same time as their working exploit that gets them a foot in the door in the first place they can get a foot in the door through the vulnerability in tor browser or whatever network facing application or application that they can send malicious inputs to and then they can just wait from their position in the virtual machine until they get a hypervisor exploit to break out of it its better if you periodically purge that virtual machine and clone a new one from the consistently dated template that way they can not just wait around for a hypervisor vulnerability because you might shove them out of the system entirely they need the hypervisor exploit at about the same time as they have the remote code execution vulnerability that gets them into the virtual machine in the first place most people are not rotating their virtual machines from clones like this and are not getting the most out of virtual isolation for this reason . the following is a post from astor on the srf from august of shortly after the freedom hosting takedown still one of the best comparative analyses available on basin security setups for operating on the darknet i feel the current active community could benefit from an active discussion around these points source srf astor in the wake of the freedom hosting exploit i think we should reevaluate our threat model and plate our security to better protect ourselves against the real threats that we face so i wrote this guide in order to spark a conversation it is by no means comprehensive i only focus on technical security perhaps others can address shipping and financial security i welcome feedback and would like these ideas to be critique and expanded as i was thinking about writing this guide i decided to take a step back and ask a basin question what are our goals ive come up with two basin goals that we want to achieve with our technical security avoid being identified minimize the damage when we are identified you can think of these as our guiding security principles if you have a technical security question you may be able to arrive at an answer by asking yourself these questions does using this technology increase or decrease the chances that i will be identified does using this technology increase or decrease the damage eg the evidence that can be used against me when i am identified obviously you will need to understand the underlying technology to answer these questions the rest of this guide explains the broad technological features that decrease the chances we are identified and that minimize the damage when we are identified towards the end i list specific technologies and evaluate them based on these features first let me list the broad features that i have come up with then i will explain them simplicity trustworthiness animal execution of intrusted code isolation encryption to some extent weve been focusing on the wrong things ive predominant been concerned with network layer attacks or attacks on the tor network but it seems clear to me now that application layer attacks are far more likely to identify us the applications that we run over tor are a much bigger attack surface than tor itself we can minimize our chances of being identified by securing the applications that we run over tor this observation informs the first four features that we desire simplicity short of not using computers at all we can minimize threats against us by simplifying the technological tools that we use a smaller code base is less likely to have bugs including deanonymizing vulnerabilities a simpler application is less likely to behave in unexpected and unwonted ways as an example when the tor project evaluated the traces left behind by the browser bundle they found traces on design squeeze which uses the come desktop environment and traces on windows its clear that windows is more complex and behaves in more unexpected ways than gnome through its complexity alone windows increases your attack surface exposing you to more potential threats although there are other ways that windows makes you more vulnerable too the traces left behind on gnome are easier to prevent than the traces left behind on windows so at least with regard to this specific threat gnome is desirable over windows so when evaluating a new technological tool for simplicity ask yourself these questions is it more or less complex than the tool im current using does it perform more or fewer unnecessary functions than the tool im current using trustworthiness we should favor technologies that are built by professionals or people with many years of experience rather than newbs a glaring example of this is cryptocat which was developed by a well intentioned hobbyist programmer and has suffered severe criticism because of the many vulnerabilities that have been discovered we should favor technologies that are open source have a large user base and a long history of use because they will be more thoroughly reviewed when evaluating a new technological tool for trustworthiness ask yourself these questions who wrote or built this tool how much experience do they have is it open source and how big is the community of users reviewers and contributors animal execution of intrusted code the first two features assume the code is trusted but has potential unwonted problems this feature assumes that as part of our routine activities we may have to run arbitrary intrusted code this is code that we cant evaluate in advance the main place this happens is in the browser through plug ins and scripts you should completely avoid running intrusted code if possible ask yourself these questions are the features that it provides absolutely necessary are there alternatives that provide these features without requiring plug ins or scripts isolation isolation is the separation of technological components with barriers it minimizes the damage incurred by exploits so if one component is explained other components are still protected it may be your last line of defense against application layer exploits the two types of isolation are physical or hardware based and virtual or software based physical isolation is more secure than virtual isolation because software based barriers can themselves be explained by malicious code we should prefer physical isolation over virtual isolation over no isolation when evaluating virtual isolation tools ask yourself the same questions about simplicity and trustworthiness does this virtualization technology perform unnecessary functions like providing a shared clipboard how long has it been in development and how thoroughly has it been reviewed how many exploits have been found encryption encryption is one of two defenses we have to minimize the damage when we are identified the more encryption you use the better off you are in an ideal world all of your storage media would be encrusted along with every email and pm that you send the reason for this is because when some emails are encrusted but others are not an attacker can easily identify the interesting emails he can learn who the interesting parties are that you communicate with because those will be the ones you send encrusted emails to this is called metadata leakage interesting messages are lost in the noise when everything is encrusted the same goes for storage media encryption if you store an encrusted file on an unencrypted hard drive an adversary can trivially determine that all the good stuff is in that small file but when you use full disk encryption you have more plausible deniability as to whether the drive contains data that would be interesting to that adversary because there are more reasons to encrypt an entire hard drive than a single file also an adversary who bypasses your encryption would have to cull through more data to find the the stuff that is interesting to him unfortunately using encryption incurs a cost that the vast majority of people cant bare so at a maximum sensitive information should be encrusted on a related note the other defense against damage is secure data erasure but that takes time that you may not have encryption is presumptive secure data erasure its easier to destroy encrusted data because you only have to destroy the encryption key to prevent an adversary from accessing the data finally id like to add a related non technical feature safe behavior in some cases the technology we use is only as safe as our behavior encryption is useless if your password is password tor is useless if you tell someone your name it may surprise you how little an adversary needs to know about you in order to unique identify you here are some basin rules to follow dont tell anyone your name ob done describe your appearance or the appearance of any major possessions car house etc done describe your family and friends dont tell anyone your location beyond a broad geographical area dont tell people where you will be traveling in advance this includes festivals done reveal specific times and places where you lived or visited in the past dont discuss specific arrests detentions discharges etc done talk about your school job military service or any organizations with official memberships done talk about hospital visits in general dont talk about anything that links you to an official record of your identity a list of somewhat secure setups for silk road users i should begin by pointing out that the features outlined above are not equally important physical isolation is probably the most useful and can protect you even when you run complex and intrusted code in each of the setups below i assume a fully dated browser tbb with scripts and plug ins disabled also the term membership concealment means that someone watching your internet connection doesnt know you are using tor this is especially important for vendors you can use bridges but give included extrajurisdictional vpns as an added layer of security with that in mind here is a descending list of secure setups for sr users starting off i present to you the most secure setup a router with a vpn an anonymizing middle box running tor a computer running tubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment against local observers with vpon disadvantages qubes os has a small user base and is not well tested as far as i know anon middle box or router with tor qubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed disadvantages qubes os has a small user base and is not well tested no membership concealment vpn router anon middle box linux os advantages physical isolation of tor from applications full disk encryption well tested code base if its a major distro like ubuntu or design disadvantages no virtual isolation of applications from each other anon middle box or router with tor linux os advantages physical isolation of tor from applications full disk encryption well tested code base disadvantages no virtual isolation of applications from each other no membership concealment qubes os by itself advantages virtual isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment possible vpn may be run in vm disadvantages no physical isolation not well tested whonix on linux host advantages virtual isolation of tor from applications full disk encryption possible membership concealment possible vpn can be run on host disadvantages no physical isolation no virtual isolation of applications from each other not well tested tails advantages encryption and leaves no trace behind system level exploits are erased after reboot relatively well tested disadvantages no physical isolation no virtual isolation no membership concealment no persistent entry guards but can mutually set bridges whonix on windows host advantages virtual isolation encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation of applications from each other not well tested vms are exposed to windows malware linus os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation windows os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation the biggest target of malware and exploits assuming there is general agreement about the order of this list our goal is to configure our personal setups to be as high up on the list as possible thanks for your attention and again i welcome comments and criticism . one of the best security threads give read kudos to all involved yog . ayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp . physical isolation is superior but you should still consider using virtual isolation with it because otherwise hardware aerial numbers are exposed and they can be sent back to the attacker through tor then if they are liable to you in purchase records they dont even need to bypass tor also keep in mind you dont have physical isolation if your isolated system has a wife card on it . quote from dicey on february man you elaborate on the anonymous middle box are you talking about a virtual machine or something like a rasberri pi i am probably completely off you can technically use virtual machines but this would be virtual isolation rather than physical isolation typical anonymizing middle boxes are thought of in terms of physical isolation in which case you could use a raspberry pi or you could use an old computer or old atop anything will work that can run tor on it im going to make some tutorials on configuring this with an old computer and a second one for doing it with a raspberry pi note though that most anonymizing middle boxes are using something called transparent proxying and you shouldnt do this because it doesnt give you stream isolation different applications using tor could use the same tor circuits it is better to do it with multiply socksports so that you can isolate the streams of different applications to their own circuits also doing it with multiply socksports rather than with transparent proxying drops traffic that isnt implicitly set to be routed over tor which is superior to transparent proxying which transparently routes all traffic over tor the thing to keep in mind is that although physical isolation is harder to break through than virtual isolation physical isolation doesnt protect from an attacker who hacks you getting your hardware aerial numbers and transmitting them back to themselves through tor this is concerning and can lead to deanonymiziation if your system has hardware aerial numbers that can be linked to your purchase of the hardware for this reason it is superior to use virtual isolation in addition to physical isolation rather than just relying on physical isolation virtual isolation isolates the operating environment from the hardware aerial numbers of the host also you should be rotating virtual machines that are closed from a template you never use for anything sensitive but merely keep fully patched and up to date that way even if attackers get a foot into your system through some vulnerability you remove their presence periodically and hopefully you remove their presence after the vulnerability they used to get in has been patched if you never rotate your virtual machine and just keep using it yes the attacker still needs an exploit for the hypervisor but they dont need it at the same time as their working exploit that gets them a foot in the door in the first place they can get a foot in the door through the vulnerability in tor browser or whatever network facing application or application that they can send malicious inputs to and then they can just wait from their position in the virtual machine until they get a hypervisor exploit to break out of it its better if you periodically purge that virtual machine and clone a new one from the consistently dated template that way they can not just wait around for a hypervisor vulnerability because you might shove them out of the system entirely they need the hypervisor exploit at about the same time as they have the remote code execution vulnerability that gets them into the virtual machine in the first place most people are not rotating their virtual machines from clones like this and are not getting the most out of virtual isolation for this reason . tubes os though built for business use and not anything illegal where you are target by interpol or similar agency is a pretty easy setup you just spawn a new vm and install torbrowser on it then spawn a second vm and use that for gnupg emails keeping contacts ect for somebody casual this works and even djb daniel bernstein uses it according to his twitter it has plenty of downsides including trusting flora not being designed for anti forensics and its a private corp maintaining the software that could go pay only or further centralize for others not wanting to risk everything on one shared box or os there are nitrogen x pcduino beagleboard and other arm soc boards you can buy to run physical seperation have one board running the x server and browser and the other to keep contacts keys and messages if you dont care about small form factor then just buy two old systems for cash off craigslist . can you elaborate on the anonymous middle box are you talking about a virtual machine or something like a rasberri pi i am probably completely off . thanks broseph i will follow your advice to the t i was using a very old program in public domain and the only reason i was using it is because it does the job of encrypting the files enough to keep casual observers out of them and i kept using it because i was too lazy to research all the knowledge you just dropped on me on my own thats what a lot of people do on these boards i usually try to search first and ask later but i got this program in the s i knew it was a liability . please sticky this thread such a great list of options and combinations which everyone should take a look at id like to throw in that eventually raspberry pi could be very useful as a tor middle box avoiding vpns . i have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption i dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes . some users whos posts were filled with information on security miss these people more then anything yhs . here is a guide about many aspects of security now i dont agree with all of its recommendations especially concerning tub settings but anyway it has lots of nice pictures and covers lots of ground also keep in mind the page is on the hidden service of deepdotweb so no clearnet warning teh heh . these type of threads are great i definitely see a few options but i dont really see what the preferred method is for security i always thought vpon tor vpn was the ideal scenario since your tor traffic will be encrusted by the second layer of vpn while going throuh the open tor node could be totally wrong on this though seems complicated and i cant say i fully get how that would work . this is a great document to access your options im going to use it as a build block for a more secure dnm endeavor great work . i leave hub irc alone it asks me to register after i already registered seems fishy to me my problem was with my setup got abner working again . can someone help me with opsec through pms preferable have a lot of questions need an og or experienced person to help thanks . there is a variety of different configurations one could use to create a set up with the sole emphasis being on security it depends on what you know for yourself what youve read and who wrote it and finally how you interpret the information itself and implement it into your set up at the end of the day all operating systems are vulnerable to being attacked some obviously much more than others and people cant rely on just the os they choose to use if they are serious about protecting their anonymity and maintaining their privacy isolation and compartmentalization are finally important factors in creating a secure set up imo . can you elaborate on the anonymous middle box are you talking about a virtual machine or something like a rasberri pi i am probably completely off . i leave hub irc alone it asks me to register after i already registered seems fishy to me my problem was with my setup got abner working again . ayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp . here is a guide about many aspects of security now i dont agree with all of its recommendations especially concerning tub settings but anyway it has lots of nice pictures and covers lots of ground also keep in mind the page is on the hidden service of deepdotweb so no clearnet warning teh heh . these type of threads are great i definitely see a few options but i dont really see what the preferred method is for security i always thought vpon tor vpn was the ideal scenario since your tor traffic will be encrusted by the second layer of vpn while going throuh the open tor node could be totally wrong on this though seems complicated and i cant say i fully get how that would work . thanks broseph i will follow your advice to the t i was using a very old program in public domain and the only reason i was using it is because it does the job of encrypting the files enough to keep casual observers out of them and i kept using it because i was too lazy to research all the knowledge you just dropped on me on my own thats what a lot of people do on these boards i usually try to search first and ask later but i got this program in the s i knew it was a liability . i have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption i dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes . some users whos posts were filled with information on security miss these people more then anything yhs . no wonder afterall there are all kinds of opinions on tor and vpon combinations and it often depends on your threat model as well anyway there are all kinds of approaches and i feel all have their good and bad points . quote from dicey on february man you elaborate on the anonymous middle box are you talking about a virtual machine or something like a rasberri pi i am probably completely off you can technically use virtual machines but this would be virtual isolation rather than physical isolation typical anonymizing middle boxes are thought of in terms of physical isolation in which case you could use a raspberry pi or you could use an old computer or old atop anything will work that can run tor on it im going to make some tutorials on configuring this with an old computer and a second one for doing it with a raspberry pi note though that most anonymizing middle boxes are using something called transparent proxying and you shouldnt do this because it doesnt give you stream isolation different applications using tor could use the same tor circuits it is better to do it with multiply socksports so that you can isolate the streams of different applications to their own circuits also doing it with multiply socksports rather than with transparent proxying drops traffic that isnt implicitly set to be routed over tor which is superior to transparent proxying which transparently routes all traffic over tor the thing to keep in mind is that although physical isolation is harder to break through than virtual isolation physical isolation doesnt protect from an attacker who hacks you getting your hardware aerial numbers and transmitting them back to themselves through tor this is concerning and can lead to deanonymiziation if your system has hardware aerial numbers that can be linked to your purchase of the hardware for this reason it is superior to use virtual isolation in addition to physical isolation rather than just relying on physical isolation virtual isolation isolates the operating environment from the hardware aerial numbers of the host also you should be rotating virtual machines that are closed from a template you never use for anything sensitive but merely keep fully patched and up to date that way even if attackers get a foot into your system through some vulnerability you remove their presence periodically and hopefully you remove their presence after the vulnerability they used to get in has been patched if you never rotate your virtual machine and just keep using it yes the attacker still needs an exploit for the hypervisor but they dont need it at the same time as their working exploit that gets them a foot in the door in the first place they can get a foot in the door through the vulnerability in tor browser or whatever network facing application or application that they can send malicious inputs to and then they can just wait from their position in the virtual machine until they get a hypervisor exploit to break out of it its better if you periodically purge that virtual machine and clone a new one from the consistently dated template that way they can not just wait around for a hypervisor vulnerability because you might shove them out of the system entirely they need the hypervisor exploit at about the same time as they have the remote code execution vulnerability that gets them into the virtual machine in the first place most people are not rotating their virtual machines from clones like this and are not getting the most out of virtual isolation for this reason . please sticky this thread such a great list of options and combinations which everyone should take a look at id like to throw in that eventually raspberry pi could be very useful as a tor middle box avoiding vpns . physical isolation is superior but you should still consider using virtual isolation with it because otherwise hardware aerial numbers are exposed and they can be sent back to the attacker through tor then if they are liable to you in purchase records they dont even need to bypass tor also keep in mind you dont have physical isolation if your isolated system has a wife card on it . can someone help me with opsec through pms preferable have a lot of questions need an og or experienced person to help thanks . tubes os though built for business use and not anything illegal where you are target by interpol or similar agency is a pretty easy setup you just spawn a new vm and install torbrowser on it then spawn a second vm and use that for gnupg emails keeping contacts ect for somebody casual this works and even djb daniel bernstein uses it according to his twitter it has plenty of downsides including trusting flora not being designed for anti forensics and its a private corp maintaining the software that could go pay only or further centralize for others not wanting to risk everything on one shared box or os there are nitrogen x pcduino beagleboard and other arm soc boards you can buy to run physical seperation have one board running the x server and browser and the other to keep contacts keys and messages if you dont care about small form factor then just buy two old systems for cash off craigslist . one of the best security threads give read kudos to all involved yog . quote from tambourine on september amayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp i am on tails using pidgin and otr is on by default i m having disputes with nickserv however and cant be authenticated but i am chatting i just cant be authenticated for private chat . the following is a post from astor on the srf from august of shortly after the freedom hosting takedown still one of the best comparative analyses available on basin security setups for operating on the darknet i feel the current active community could benefit from an active discussion around these points source srf astor in the wake of the freedom hosting exploit i think we should reevaluate our threat model and plate our security to better protect ourselves against the real threats that we face so i wrote this guide in order to spark a conversation it is by no means comprehensive i only focus on technical security perhaps others can address shipping and financial security i welcome feedback and would like these ideas to be critique and expanded as i was thinking about writing this guide i decided to take a step back and ask a basin question what are our goals ive come up with two basin goals that we want to achieve with our technical security avoid being identified minimize the damage when we are identified you can think of these as our guiding security principles if you have a technical security question you may be able to arrive at an answer by asking yourself these questions does using this technology increase or decrease the chances that i will be identified does using this technology increase or decrease the damage eg the evidence that can be used against me when i am identified obviously you will need to understand the underlying technology to answer these questions the rest of this guide explains the broad technological features that decrease the chances we are identified and that minimize the damage when we are identified towards the end i list specific technologies and evaluate them based on these features first let me list the broad features that i have come up with then i will explain them simplicity trustworthiness animal execution of intrusted code isolation encryption to some extent weve been focusing on the wrong things ive predominant been concerned with network layer attacks or attacks on the tor network but it seems clear to me now that application layer attacks are far more likely to identify us the applications that we run over tor are a much bigger attack surface than tor itself we can minimize our chances of being identified by securing the applications that we run over tor this observation informs the first four features that we desire simplicity short of not using computers at all we can minimize threats against us by simplifying the technological tools that we use a smaller code base is less likely to have bugs including deanonymizing vulnerabilities a simpler application is less likely to behave in unexpected and unwonted ways as an example when the tor project evaluated the traces left behind by the browser bundle they found traces on design squeeze which uses the come desktop environment and traces on windows its clear that windows is more complex and behaves in more unexpected ways than gnome through its complexity alone windows increases your attack surface exposing you to more potential threats although there are other ways that windows makes you more vulnerable too the traces left behind on gnome are easier to prevent than the traces left behind on windows so at least with regard to this specific threat gnome is desirable over windows so when evaluating a new technological tool for simplicity ask yourself these questions is it more or less complex than the tool im current using does it perform more or fewer unnecessary functions than the tool im current using trustworthiness we should favor technologies that are built by professionals or people with many years of experience rather than newbs a glaring example of this is cryptocat which was developed by a well intentioned hobbyist programmer and has suffered severe criticism because of the many vulnerabilities that have been discovered we should favor technologies that are open source have a large user base and a long history of use because they will be more thoroughly reviewed when evaluating a new technological tool for trustworthiness ask yourself these questions who wrote or built this tool how much experience do they have is it open source and how big is the community of users reviewers and contributors animal execution of intrusted code the first two features assume the code is trusted but has potential unwonted problems this feature assumes that as part of our routine activities we may have to run arbitrary intrusted code this is code that we cant evaluate in advance the main place this happens is in the browser through plug ins and scripts you should completely avoid running intrusted code if possible ask yourself these questions are the features that it provides absolutely necessary are there alternatives that provide these features without requiring plug ins or scripts isolation isolation is the separation of technological components with barriers it minimizes the damage incurred by exploits so if one component is explained other components are still protected it may be your last line of defense against application layer exploits the two types of isolation are physical or hardware based and virtual or software based physical isolation is more secure than virtual isolation because software based barriers can themselves be explained by malicious code we should prefer physical isolation over virtual isolation over no isolation when evaluating virtual isolation tools ask yourself the same questions about simplicity and trustworthiness does this virtualization technology perform unnecessary functions like providing a shared clipboard how long has it been in development and how thoroughly has it been reviewed how many exploits have been found encryption encryption is one of two defenses we have to minimize the damage when we are identified the more encryption you use the better off you are in an ideal world all of your storage media would be encrusted along with every email and pm that you send the reason for this is because when some emails are encrusted but others are not an attacker can easily identify the interesting emails he can learn who the interesting parties are that you communicate with because those will be the ones you send encrusted emails to this is called metadata leakage interesting messages are lost in the noise when everything is encrusted the same goes for storage media encryption if you store an encrusted file on an unencrypted hard drive an adversary can trivially determine that all the good stuff is in that small file but when you use full disk encryption you have more plausible deniability as to whether the drive contains data that would be interesting to that adversary because there are more reasons to encrypt an entire hard drive than a single file also an adversary who bypasses your encryption would have to cull through more data to find the the stuff that is interesting to him unfortunately using encryption incurs a cost that the vast majority of people cant bare so at a maximum sensitive information should be encrusted on a related note the other defense against damage is secure data erasure but that takes time that you may not have encryption is presumptive secure data erasure its easier to destroy encrusted data because you only have to destroy the encryption key to prevent an adversary from accessing the data finally id like to add a related non technical feature safe behavior in some cases the technology we use is only as safe as our behavior encryption is useless if your password is password tor is useless if you tell someone your name it may surprise you how little an adversary needs to know about you in order to unique identify you here are some basin rules to follow dont tell anyone your name ob done describe your appearance or the appearance of any major possessions car house etc done describe your family and friends dont tell anyone your location beyond a broad geographical area dont tell people where you will be traveling in advance this includes festivals done reveal specific times and places where you lived or visited in the past dont discuss specific arrests detentions discharges etc done talk about your school job military service or any organizations with official memberships done talk about hospital visits in general dont talk about anything that links you to an official record of your identity a list of somewhat secure setups for silk road users i should begin by pointing out that the features outlined above are not equally important physical isolation is probably the most useful and can protect you even when you run complex and intrusted code in each of the setups below i assume a fully dated browser tbb with scripts and plug ins disabled also the term membership concealment means that someone watching your internet connection doesnt know you are using tor this is especially important for vendors you can use bridges but give included extrajurisdictional vpns as an added layer of security with that in mind here is a descending list of secure setups for sr users starting off i present to you the most secure setup a router with a vpn an anonymizing middle box running tor a computer running tubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment against local observers with vpon disadvantages qubes os has a small user base and is not well tested as far as i know anon middle box or router with tor qubes os advantages physical isolation of tor from applications virtual isolation of applications from each other encryption as needed disadvantages qubes os has a small user base and is not well tested no membership concealment vpn router anon middle box linux os advantages physical isolation of tor from applications full disk encryption well tested code base if its a major distro like ubuntu or design disadvantages no virtual isolation of applications from each other anon middle box or router with tor linux os advantages physical isolation of tor from applications full disk encryption well tested code base disadvantages no virtual isolation of applications from each other no membership concealment qubes os by itself advantages virtual isolation of tor from applications virtual isolation of applications from each other encryption as needed membership concealment possible vpn may be run in vm disadvantages no physical isolation not well tested whonix on linux host advantages virtual isolation of tor from applications full disk encryption possible membership concealment possible vpn can be run on host disadvantages no physical isolation no virtual isolation of applications from each other not well tested tails advantages encryption and leaves no trace behind system level exploits are erased after reboot relatively well tested disadvantages no physical isolation no virtual isolation no membership concealment no persistent entry guards but can mutually set bridges whonix on windows host advantages virtual isolation encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation of applications from each other not well tested vms are exposed to windows malware linus os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation windows os advantages full disk encryption possible membership concealment possible disadvantages no physical isolation no virtual isolation the biggest target of malware and exploits assuming there is general agreement about the order of this list our goal is to configure our personal setups to be as high up on the list as possible thanks for your attention and again i welcome comments and criticism . there is a variety of different configurations one could use to create a set up with the sole emphasis being on security it depends on what you know for yourself what youve read and who wrote it and finally how you interpret the information itself and implement it into your set up at the end of the day all operating systems are vulnerable to being attacked some obviously much more than others and people cant rely on just the os they choose to use if they are serious about protecting their anonymity and maintaining their privacy isolation and compartmentalization are finally important factors in creating a secure set up imo . quote from tambourine on july pmi have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption rc is broken switch to a different cipher immediately id also stop using sha and switch to sha at a maximum see curious why would you use a stream cipher like rc as opposed to a block cipher like aes or twofish for that matter what software are you using to encrypt your data pm me if you dont want to post it publicly quote from tambourine on july pmi dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes my advice would be to use diceware to generate your passpharases words would be idea follow the instructions and memorize the result this will make brute forcing your passphrase impossible use of a decent block cipher and a diceware passphrase should secure your data sufficiently see zaphod . this is a great document to access your options im going to use it as a build block for a more secure dnm endeavor great work . quote from tambourine on september amayone still use jabber or icq on tails pidgin with otr i tried to sign in but the server refused i guess either they are down or my account has gone inactive and they deleted it whats a good server for xmpp i am on tails using pidgin and otr is on by default i m having disputes with nickserv however and cant be authenticated but i am chatting i just cant be authenticated for private chat . quote from tambourine on july pmi have two privacy questions when using a bridge with tor how often do you need to get a new birdge i have been doing it once per day but read somewhere to do this every time tails is rooted use a new bridge i have some sensitive information stored in files on a windows machine encrusted with sha and rc and a bit key and a passphrase is this enough security or should i use stronger encryption rc is broken switch to a different cipher immediately id also stop using sha and switch to sha at a maximum see curious why would you use a stream cipher like rc as opposed to a block cipher like aes or twofish for that matter what software are you using to encrypt your data pm me if you dont want to post it publicly quote from tambourine on july pmi dont expect it to happen but in a nightmare scenario where leo have this pc will these files be cracked easily or not all my knowledge of encryption comes from the end of trying to keep things encrusted not breaking encryption so i dont know how long it takes or if cops even have access to it in sure they do originally before backtrack os when it was auditors security collection i saw the is online and the marketing blurb around it said hack wifi with the same tools as the fbi my thinking is that i have private keys for dnms i need a backup of and some pics of my grows i am sentimental about and i keep it all encrypted i desperately dont want to lose the files but i equally desperately must keep them from prying eyes my advice would be to use diceware to generate your passpharases words would be idea follow the instructions and memorize the result this will make brute forcing your passphrase impossible use of a decent block cipher and a diceware passphrase should secure your data sufficiently see zaphod . no wonder afterall there are all kinds of opinions on tor and vpon combinations and it often depends on your threat model as well anyway there are all kinds of approaches and i feel all have their good and bad points . 